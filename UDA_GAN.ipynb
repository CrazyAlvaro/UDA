{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "UOmUNvGYgPRN",
      "metadata": {
        "id": "UOmUNvGYgPRN"
      },
      "source": [
        "# Unsupervised Domain Adaptation Project\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ExU_wmMAgVsR",
      "metadata": {
        "id": "ExU_wmMAgVsR"
      },
      "source": [
        "## Part-1: Data download\n",
        "Load data to project from Google Drive. Copy a subset of classes of images to the path:\n",
        "- `adaptiope_small/product_images`\n",
        "- `adaptiope_small/real_life` \n",
        "\n",
        "two directories. They represent images from two different domain **product** and **real_life**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "4134f6cf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4134f6cf",
        "outputId": "ece23d55-a71f-4c37-c413-b313178c76eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from os import makedirs, listdir\n",
        "from tqdm import tqdm\n",
        "from google.colab import drive\n",
        "from os.path import join\n",
        "from shutil import copytree\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "!mkdir dataset\n",
        "!cp \"gdrive/My Drive/Colab Notebooks/data/Adaptiope.zip\" dataset/\n",
        "# !ls dataset\n",
        "\n",
        "!unzip -qq dataset/Adaptiope.zip   # unzip file\n",
        "\n",
        "!rm -rf dataset/Adaptiope.zip \n",
        "!rm -rf adaptiope_small"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "0a6cac67",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0a6cac67",
        "outputId": "9e07abd7-aff7-4cd6-fe5c-f99b40929e8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['bottle', 'file cabinet', 'brachiosaurus', 'binoculars', 'stethoscope', 'smartphone', 'mixing console', 'axe', 'purse', 'hard-wired fixed phone', 'golf club', 'pipe wrench', 'puncher', 'computer mouse', 'speakers', 'fighter jet', 'diving fins', 'knife', 'ladder', 'projector', 'glasses', 'laptop', 'tape dispenser', 'spatula', 'game controller', 'electric shaver', 'tank', 'ring binder', 'trash can', 'crown', 'magic lamp', 'printer', 'shower head', 'toothbrush', 'hair dryer', 'stapler', 'chainsaw', 'sleeping bag', 'motorbike helmet', 'wristwatch', 'vr goggles', 'phonograph', 'drum set', 'wheelchair', 'sword', 'bicycle', 'letter tray', 'network switch', 'ice cube tray', 'nail clipper', 'handcuffs', 'umbrella', 'coat hanger', 'hoverboard', 'tent', 'computer', 'flat iron', 'rubber boat', 'snow shovel', 'rifle', 'telescope', 'dart', 'acoustic guitar', 'corkscrew', 'ruler', 'scissors', 'watering can', 'scooter', 'fan', 'hot glue gun', 'hourglass', 'razor', 'calculator', 'wallet', 'cordless fixed phone', 'webcam', 'monitor', 'car jack', 'cellphone', 'syringe', 'usb stick', 'vacuum cleaner', 'power drill', 'sewing machine', 'mug', 'ice skates', 'hand mixer', 'lawn mower', 'over-ear headphones', 'office chair', 'pikachu', 'grill', 'stroller', 'tyrannosaurus', 'microwave', 'skeleton', 'hat', 'keyboard', 'baseball bat', 'helicopter', 'toilet brush', 'bookcase', 'screwdriver', 'quadcopter', 'skateboard', 'comb', 'notepad', 'pogo stick', 'rc car', 'roller skates', 'in-ear headphones', 'stand mixer', 'desk lamp', 'pen', 'handgun', 'smoking pipe', 'backpack', 'compass', 'electric guitar', 'fire extinguisher', 'boxing gloves', 'power strip', 'bicycle helmet']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20/20 [00:02<00:00,  6.82it/s]\n",
            "100%|██████████| 20/20 [00:04<00:00,  4.57it/s]\n"
          ]
        }
      ],
      "source": [
        "!mkdir adaptiope_small\n",
        "classes = listdir(\"Adaptiope/product_images\")\n",
        "print(classes)\n",
        "classes = [\"backpack\", \"bookcase\", \"car jack\", \"comb\", \"crown\", \"file cabinet\", \"flat iron\", \"game controller\", \"glasses\",\n",
        "           \"helicopter\", \"ice skates\", \"letter tray\", \"monitor\", \"mug\", \"network switch\", \"over-ear headphones\", \"pen\",\n",
        "           \"purse\", \"stand mixer\", \"stroller\"]\n",
        "domain_classes = [\"product_images\", \"real_life\"]\n",
        "for d, td in zip([\"Adaptiope/product_images\", \"Adaptiope/real_life\"], [\"adaptiope_small/product_images\", \"adaptiope_small/real_life\"]):\n",
        "  makedirs(td)\n",
        "  for c in tqdm(classes):\n",
        "    c_path = join(d, c)\n",
        "    c_target = join(td, c)\n",
        "    copytree(c_path, c_target)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "uUHNozN6ggRr",
      "metadata": {
        "id": "uUHNozN6ggRr"
      },
      "source": [
        "## Part-2: Image Classification Neural Network\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fwiJOBXWpeS0",
      "metadata": {
        "id": "fwiJOBXWpeS0"
      },
      "source": [
        "### Part-2.0: Data Loading\n",
        "\n",
        "First we load the data and preprocessing them"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "hmth8mlu1oov",
      "metadata": {
        "id": "hmth8mlu1oov"
      },
      "outputs": [],
      "source": [
        "product_path = 'adaptiope_small/product_images'\n",
        "real_life_path = 'adaptiope_small/real_life'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "ht9bXOHU6zTQ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ht9bXOHU6zTQ",
        "outputId": "c3699783-3d0c-49e8-b6cd-bae47e6513e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Adaptiope  adaptiope_small  dataset  gdrive  sample_data\n"
          ]
        }
      ],
      "source": [
        "!pwd\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "_GZxbLlT6O8m",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_GZxbLlT6O8m",
        "outputId": "76d26bc6-b6ee-489d-8cb3-2250da14678e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image size:  (679, 679)\n"
          ]
        }
      ],
      "source": [
        "from PIL import Image\n",
        "from os.path import join\n",
        "\n",
        "img = Image.open(join(product_path, 'backpack', 'backpack_003.jpg'))\n",
        "print('Image size: ', img.size)\n",
        "#img"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "RE2WUwy9BORV",
      "metadata": {
        "id": "RE2WUwy9BORV"
      },
      "source": [
        "import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "Vei6SzEeggzU",
      "metadata": {
        "id": "Vei6SzEeggzU"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision.models import vgg11, alexnet \n",
        "from torch.utils.data import DataLoader, random_split"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "M5rt9x7nBKiT",
      "metadata": {
        "id": "M5rt9x7nBKiT"
      },
      "source": [
        "configuration constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "id": "JXznFSNkAzn3",
      "metadata": {
        "id": "JXznFSNkAzn3"
      },
      "outputs": [],
      "source": [
        "img_size = 256\n",
        "# mean, std used by pre-trained models from PyTorch\n",
        "mean, std = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n",
        "config = dict(epochs=5, batch_size=64,lr=0.001, wd=0.001, momentum=0.9, domain_regression_weight=0.3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_Rz4CI9spEkN",
      "metadata": {
        "id": "_Rz4CI9spEkN"
      },
      "source": [
        "Configue GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "nHv2o65FpDpn",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nHv2o65FpDpn",
        "outputId": "9686d1e2-f95d-43bd-ed33-7b34a17bc0e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "GF3YyTBAhJk8",
      "metadata": {
        "id": "GF3YyTBAhJk8"
      },
      "outputs": [],
      "source": [
        "from torchvision.transforms.transforms import ToTensor\n",
        "\n",
        "def get_dataset(root_path):\n",
        "  '''\n",
        "    Get dataset from specific data path\n",
        "\n",
        "    # parameters:\n",
        "        root_path: path to image folder\n",
        "\n",
        "    # return: train_loader, test_loader\n",
        "  '''\n",
        "  # Construct image transform\n",
        "  image_transform = transforms.Compose([\n",
        "    transforms.Resize(img_size),\n",
        "    transforms.CenterCrop(img_size),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std)\n",
        "  ])\n",
        "\n",
        "  # Load data from filesystem\n",
        "  image_dataset = ImageFolder(root_path, transform=image_transform)\n",
        "\n",
        "  return image_dataset\n",
        "\n",
        "def get_dataloader(dataset, batch_size, shuffle_train=True, shuffle_test=False):\n",
        "  '''\n",
        "    Get DataLoader from specific data path\n",
        "\n",
        "    # parameters:\n",
        "        dataset: ImageFolder instance\n",
        "        batch_size: batch_size for DataLoader\n",
        "        shuffle_train: whether to shuffle training data\n",
        "        shuffle_test: whether to shuffle test data\n",
        "  '''\n",
        "  # Get train, test number\n",
        "  num_total = len(dataset)\n",
        "  num_train = int(num_total * 0.8 + 1)\n",
        "  num_test  = num_total - num_train\n",
        "\n",
        "  # random split dataset\n",
        "  data_train, data_test = random_split(dataset, [num_train, num_test])\n",
        "\n",
        "  # initialize dataloaders\n",
        "  loader_train = DataLoader(data_train, batch_size=batch_size, shuffle=shuffle_train)\n",
        "  loader_test  = DataLoader(data_test, batch_size=batch_size, shuffle=shuffle_test)\n",
        "\n",
        "  return loader_train, loader_test"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-J4MSc3spcYU",
      "metadata": {
        "id": "-J4MSc3spcYU"
      },
      "source": [
        "### Part-2.1 Pretrain Network\n",
        "\n",
        "Here we use a pretrain Neural Network to start with, then we fine tune it with the data set we have from **Adaptiope** in one domain, and test it on the target domain. Compare the two result, and set the benchmark for later UDA enriched method. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "ijDPD_jrvlz_",
      "metadata": {
        "id": "ijDPD_jrvlz_"
      },
      "outputs": [],
      "source": [
        "# pd_dataset = get_dataset(product_path)\n",
        "# len(pd_dataset.classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ZjWy3ak98L0F",
      "metadata": {
        "id": "ZjWy3ak98L0F"
      },
      "source": [
        "### Part-2.2 Define the Model with Feature Extractor, Classifier, and Domain Regressor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "DIIhUIqITlfi",
      "metadata": {
        "id": "DIIhUIqITlfi"
      },
      "outputs": [],
      "source": [
        "# class DAAN(torch.nn.Module):\n",
        "#   def __init__(self, num_classes):\n",
        "#     super(DAAN, self).__init__()\n",
        "\n",
        "#     # Feature Extractor with Pretained VGG16\n",
        "#     # self.feature_extractor = vgg11(pretrained=True)\n",
        "\n",
        "#     # Feature Extractor with AlexNet\n",
        "#     self.feature_extractor = alexnet(pretrained=True)\n",
        "#     feature_out_size = self.feature_extractor.classifier[-1].in_features\n",
        "#     self.feature_extractor.classifier[-1] = torch.nn.Identity()\n",
        "\n",
        "#     # Classifier\n",
        "#     self.label_classifier = torch.nn.Linear(feature_out_size, num_classes)\n",
        "\n",
        "#     # Domain Regressor\n",
        "#     self.domain_regressor  = torch.nn.Linear(feature_out_size, 1)\n",
        "#     self.domain_prediction = torch.nn.Sigmoid()\n",
        "\n",
        "#   def forward(self, X):\n",
        "#     # Feature Extractor\n",
        "#     feature_representation = self.feature_extractor(X)\n",
        "\n",
        "#     # Class Prediction\n",
        "#     label_pred = self.label_classifier(feature_representation)\n",
        "\n",
        "#     # Domain Regression Prediction\n",
        "#     domain_rgrs = self.domain_regressor(feature_representation)\n",
        "#     domain_pred = self.domain_prediction(domain_rgrs) \n",
        "\n",
        "#     # Concatenates label_prediction and domain_regression\n",
        "#     # output = torch.cat((label_pred, domain_pred), dim=0)\n",
        "\n",
        "#     return label_pred, domain_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "95ff2db4",
      "metadata": {
        "id": "95ff2db4"
      },
      "outputs": [],
      "source": [
        "class FeatureExtractor(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super(FeatureExtractor, self).__init__()\n",
        "\n",
        "    # Feature Extractor with AlexNet\n",
        "    self.feature_extractor = alexnet(pretrained=True)\n",
        "    self.feature_dim = self.feature_extractor.classifier[-1].in_features\n",
        "\n",
        "    # make the last layer identity\n",
        "    self.feature_extractor.classifier[-1] = torch.nn.Identity()\n",
        "\n",
        "  def forward(self, x):\n",
        "    # Feature Extractor\n",
        "    return self.feature_extractor(x)\n",
        "  \n",
        "  def output_dim(self):\n",
        "    return self.feature_dim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "517118c3",
      "metadata": {
        "id": "517118c3"
      },
      "outputs": [],
      "source": [
        "class Classifier(torch.nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(Classifier, self).__init__()\n",
        "        self.classifier = torch.nn.Linear(input_dim, output_dim)\n",
        "    \n",
        "    def forward(self, X):\n",
        "        return self.classifier(X) "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.autograd import Function\n",
        "\n",
        "class ReverseLayerF(Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, x, alpha):\n",
        "        ctx.alpha = alpha\n",
        "        return x.view_as(x)\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        output = grad_output.neg() * ctx.alpha\n",
        "        return output, None"
      ],
      "metadata": {
        "id": "RES6EY4PO7KF"
      },
      "id": "RES6EY4PO7KF",
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Discriminator(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=256):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.discriminator = torch.nn.Linear(input_dim, hidden_dim)\n",
        "        self.discrim_out   = torch.nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        intermed_out = F.relu(self.discriminator(x))\n",
        "        output = torch.sigmoid(self.discrim_out(intermed_out))\n",
        "        return output"
      ],
      "metadata": {
        "id": "K9syaPhFxOFd"
      },
      "id": "K9syaPhFxOFd",
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DANN(torch.nn.Module):\n",
        "  # def __init__(self, num_classes, adversarial=True):\n",
        "  def __init__(self, num_classes):\n",
        "    super(DANN, self).__init__()\n",
        "    self.output_dim = num_classes\n",
        "\n",
        "    # define inner network component\n",
        "    self.feature_extractor = FeatureExtractor()\n",
        "    self.classifier = Classifier(self.feature_extractor.output_dim(), num_classes)\n",
        "    self.discriminator = Discriminator(self.feature_extractor.output_dim())  \n",
        "  \n",
        "  def forward(self, x):\n",
        "    feature_output = self.feature_extractor(x)\n",
        "\n",
        "    class_pred = self.classifier(feature_output)\n",
        "\n",
        "    # Add a ReverseLayer here for negative gradient computation\n",
        "    reverse_feature = ReverseLayerF.apply(feature_output, 1)\n",
        "    discr_pred = self.discriminator(reverse_feature)\n",
        "\n",
        "    return class_pred, discr_pred"
      ],
      "metadata": {
        "id": "BsbwoZkZwjjl"
      },
      "id": "BsbwoZkZwjjl",
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "rOiatGA9xVdu"
      },
      "id": "rOiatGA9xVdu",
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "SbvVK4NO68Yp",
      "metadata": {
        "id": "SbvVK4NO68Yp"
      },
      "source": [
        "### Part-2.2.1 Define the Feature Extractor by using a pretrain model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "JfloSw5b2jVe",
      "metadata": {
        "id": "JfloSw5b2jVe"
      },
      "outputs": [],
      "source": [
        "# def feature_extractor():\n",
        "#   '''\n",
        "#   Using VGG16 pretrain model\n",
        "#   And set the final classifer as an Identity Mapping(Use the model as feature extractor)\n",
        "#   '''\n",
        "#   model = vgg16(pretrained=True)\n",
        "#   out_feature = model.classifier[-1].in_features\n",
        "#   model.classifier[-1] = torch.nn.Identity()\n",
        "\n",
        "#   return model, out_feature"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "B5xVtgIO8BnL",
      "metadata": {
        "id": "B5xVtgIO8BnL"
      },
      "source": [
        "### Part-2.2.2 Define the Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "p8JGJfyF8CHB",
      "metadata": {
        "id": "p8JGJfyF8CHB"
      },
      "outputs": [],
      "source": [
        "# def classifier(input_size, output_size):\n",
        "#   return torch.nn.Linear(input_size, output_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "E8fnXyvS8CgD",
      "metadata": {
        "id": "E8fnXyvS8CgD"
      },
      "source": [
        "### Part-2.2.3 Define the Domain Regressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "cGHGLjez8C9g",
      "metadata": {
        "id": "cGHGLjez8C9g"
      },
      "outputs": [],
      "source": [
        "# def domain_regressor(input_size, output_size=2):\n",
        "#   return torch.nn.Linear(input_size, output_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "IhnsX7lq5saz",
      "metadata": {
        "id": "IhnsX7lq5saz"
      },
      "source": [
        "### Part-2.3 Cost function\n",
        "\n",
        "Divide parameters intro two groups, in which the last fully conneted layer with learning_rate, the other layers with 0.1 * learning_rate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "id": "q6BkiCH_5sOr",
      "metadata": {
        "id": "q6BkiCH_5sOr"
      },
      "outputs": [],
      "source": [
        "def get_class_loss_func():\n",
        "  return torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# def get_domain_loss_func():\n",
        "  # return torch.nn.BCELoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "B-y0dTPj5zrR",
      "metadata": {
        "id": "B-y0dTPj5zrR"
      },
      "source": [
        "### Part-2.4 Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "zKRb56iY5sGt",
      "metadata": {
        "id": "zKRb56iY5sGt"
      },
      "outputs": [],
      "source": [
        "# def get_optimizer(model, learning_rate, weight_decay, momentum):\n",
        "#   '''\n",
        "#   Config Optimizer\n",
        "#   '''\n",
        "#   # classification layer name: label_classifier\n",
        "#   classifier_name = \"label_classifier\"\n",
        "#   domain_regressor_name = \"domain_regressor\"\n",
        "\n",
        "#   pre_trained_weights = []\n",
        "#   domain_regressor_weights = []\n",
        "#   classifier_weights = []\n",
        "\n",
        "#   # get all the parameters required gradient updates\n",
        "#   for name, param in model.named_parameters():\n",
        "#     if param.requires_grad == True:\n",
        "#       if name.startswith(classifier_name):\n",
        "#         classifier_weights.append(param)\n",
        "#       elif name.startswith(domain_regressor_name):\n",
        "#         domain_regressor_weights.append(param)\n",
        "#       else:\n",
        "#         pre_trained_weights.append(param)\n",
        "\n",
        "#   # assign parameters to parameters\n",
        "#   optimizer = torch.optim.SGD([\n",
        "#     {'params': pre_trained_weights},\n",
        "#     {'params': classifier_weights, 'lr': learning_rate}\n",
        "#   ], lr= learning_rate/10, weight_decay=weight_decay, momentum=momentum)\n",
        "  \n",
        "#   return optimizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_optimizer(model, config, adversarial=True):\n",
        "  '''\n",
        "  Config Optimizer\n",
        "  '''\n",
        "  learning_rate = config['lr']\n",
        "  weight_decay  = config['wd']\n",
        "  momentum      = config['momentum']\n",
        "\n",
        "  feature_ext   = model.get_submodule(\"feature_extractor\")\n",
        "  classifier    = model.get_submodule(\"classifier\")\n",
        "  discriminator = model.get_submodule(\"discriminator\")\n",
        "\n",
        "  # classification layer name: label_classifier\n",
        "  classifier_name = \"label_classifier\"\n",
        "  domain_regressor_name = \"domain_regressor\"\n",
        "  pre_trained_weights = feature_ext.parameters()\n",
        "\n",
        "  if adversarial:\n",
        "    other_weights = list(classifier.parameters()) + list(discriminator.parameters())\n",
        "  else:\n",
        "    other_weights = list(classifier.parameters())\n",
        "\n",
        "  # assign parameters to parameters\n",
        "  optimizer = torch.optim.SGD([\n",
        "    {'params': pre_trained_weights},\n",
        "    {'params': other_weights, 'lr': learning_rate}\n",
        "  ], lr= learning_rate/10, weight_decay=weight_decay, momentum=momentum)\n",
        "  \n",
        "  return optimizer"
      ],
      "metadata": {
        "id": "ZsoNnQCq2aEt"
      },
      "id": "ZsoNnQCq2aEt",
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "3wL74sCV0Ycm"
      },
      "id": "3wL74sCV0Ycm",
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "Atdanl9REs3F",
      "metadata": {
        "id": "Atdanl9REs3F"
      },
      "source": [
        "### Part-2.5 Training and Testing Step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "id": "ORDqPkiT5r1s",
      "metadata": {
        "id": "ORDqPkiT5r1s"
      },
      "outputs": [],
      "source": [
        "def train_loop(dataloader, model, optimizer, device):\n",
        "  size = len(dataloader.dataset)\n",
        "  loss_fn = get_class_loss_func()\n",
        "\n",
        "  for batch, (X, y) in enumerate(dataloader):\n",
        "    X, y = X.to(device), y.to(device)\n",
        "    \n",
        "    # compute prediction and loss\n",
        "    class_pred, _ = model(X)\n",
        "\n",
        "    # classification loss\n",
        "    loss = loss_fn(class_pred, y)\n",
        "\n",
        "    # backpropagation\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "      loss, current = loss.item(), batch * len(X)\n",
        "      print(f\"loss: {loss:>7f} [{current:>5d}/{size:>5d}]\")\n",
        "    \n",
        "    del loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "id": "LlvQLNUDLGJF",
      "metadata": {
        "id": "LlvQLNUDLGJF"
      },
      "outputs": [],
      "source": [
        "def test_loop(dataloader, model, device):\n",
        "  test_loss, correct = 0, 0\n",
        "  loss_fn = get_class_loss_func()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for X, y in dataloader:\n",
        "      X, y = X.to(device), y.to(device)\n",
        "      class_pred, _ = model(X)\n",
        "\n",
        "      test_loss += loss_fn(class_pred, y).item()\n",
        "      correct += (class_pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "  size = len(dataloader.dataset)\n",
        "  num_batches = len(dataloader)\n",
        "\n",
        "  test_loss /= num_batches\n",
        "  correct /= size\n",
        "  print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "\n",
        "  return test_loss, correct"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "z5uhItnwhj8S",
      "metadata": {
        "id": "z5uhItnwhj8S"
      },
      "source": [
        "### Part-2.6 Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "nM76Syqahknd",
      "metadata": {
        "id": "nM76Syqahknd"
      },
      "outputs": [],
      "source": [
        "def training(model, train_dataloader, test_dataloader, config, device):\n",
        "  epochs = config['epochs']\n",
        "  print(f\"Learning_rate {config['lr']}, weight_decay {config['wd']}\")\n",
        "  loss_fn = get_class_loss_func()\n",
        "\n",
        "  optimizer = get_optimizer(model, config, adversarial=False)\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    print(f\"Epoch {epoch+1}\\n------------------\")\n",
        "    train_loop(train_dataloader, model, loss_fn, optimizer, device)\n",
        "    test_loop(test_dataloader, model, loss_fn, device)\n",
        "\n",
        "  print(\"Done\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "id": "V93UE4rXi3C9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V93UE4rXi3C9",
        "outputId": "f5be9ed8-6f2a-423d-831d-072e4cd381aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Learning_rate 0.001, weight_decay 0.001\n",
            "get_optimizer\n",
            "FeatureExtractor(\n",
            "  (feature_extractor): AlexNet(\n",
            "    (features): Sequential(\n",
            "      (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
            "      (1): ReLU(inplace=True)\n",
            "      (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "      (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "      (4): ReLU(inplace=True)\n",
            "      (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "      (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (7): ReLU(inplace=True)\n",
            "      (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (9): ReLU(inplace=True)\n",
            "      (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (11): ReLU(inplace=True)\n",
            "      (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "    (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
            "    (classifier): Sequential(\n",
            "      (0): Dropout(p=0.5, inplace=False)\n",
            "      (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
            "      (2): ReLU(inplace=True)\n",
            "      (3): Dropout(p=0.5, inplace=False)\n",
            "      (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
            "      (5): ReLU(inplace=True)\n",
            "      (6): Identity()\n",
            "    )\n",
            "  )\n",
            ")\n",
            "Epoch 1\n",
            "------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1944: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 3.349539 [    0/ 1601]\n",
            "Test Error: \n",
            " Accuracy: 88.7%, Avg loss: 0.335081 \n",
            "\n",
            "Epoch 2\n",
            "------------------\n",
            "loss: 0.159048 [    0/ 1601]\n",
            "Test Error: \n",
            " Accuracy: 91.7%, Avg loss: 0.263871 \n",
            "\n",
            "Epoch 3\n",
            "------------------\n",
            "loss: 0.211627 [    0/ 1601]\n",
            "Test Error: \n",
            " Accuracy: 93.0%, Avg loss: 0.245775 \n",
            "\n",
            "Epoch 4\n",
            "------------------\n",
            "loss: 0.240699 [    0/ 1601]\n",
            "Test Error: \n",
            " Accuracy: 92.2%, Avg loss: 0.219472 \n",
            "\n",
            "Epoch 5\n",
            "------------------\n",
            "loss: 0.052230 [    0/ 1601]\n",
            "Test Error: \n",
            " Accuracy: 94.0%, Avg loss: 0.204437 \n",
            "\n",
            "Done\n"
          ]
        }
      ],
      "source": [
        "# Get train_dataloader, test_dataloader\n",
        "source_dataset = get_dataset(product_path)\n",
        "train_dataloader, test_dataloader = get_dataloader(source_dataset, 50)\n",
        "\n",
        "model = DANN(len(source_dataset.classes), False).to(device)\n",
        "\n",
        "# Training\n",
        "training(model, train_dataloader, test_dataloader, config, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "05P25psLS27W",
      "metadata": {
        "id": "05P25psLS27W"
      },
      "source": [
        "### Part-2.7 Testing on Target Domain\n",
        "#### Apply the model trained on the source domain directly to the target domain. This result will be used for comparison with the results obtained after domain adaptation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "id": "E54Vm3jmTRcc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E54Vm3jmTRcc",
        "outputId": "6fc3bad8-b07e-4646-da86-d6a181b8eb0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1944: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Error: \n",
            " Accuracy: 63.7%, Avg loss: 1.203011 \n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1.2030107021331786, 0.6375)"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ],
      "source": [
        "target_dataset = get_dataset(real_life_path)\n",
        "loader_target_dataset = DataLoader(target_dataset, batch_size=100, shuffle=False)\n",
        "\n",
        "# model.load_state_dict(torch.load('model_state.pt', map_location='cpu'))\n",
        "test_loop(loader_target_dataset, model, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "id": "Cw4GP_z-_iy_",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cw4GP_z-_iy_",
        "outputId": "c7770acf-a8cc-4189-80e0-438860c8e98d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "940750848\n"
          ]
        }
      ],
      "source": [
        "del model\n",
        "print(torch.cuda.memory_allocated())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "id": "rgdhQQwIAzDO",
      "metadata": {
        "id": "rgdhQQwIAzDO"
      },
      "outputs": [],
      "source": [
        "del train_dataloader, test_dataloader, loader_target_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "id": "Sb5mqdPMBHli",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sb5mqdPMBHli",
        "outputId": "42ba8eab-26d4-46fe-9d54-d76e8a418193"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "478516736\n"
          ]
        }
      ],
      "source": [
        "print(torch.cuda.memory_allocated())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "kTUMJfmMw-tQ",
      "metadata": {
        "id": "kTUMJfmMw-tQ"
      },
      "source": [
        "## TODO\n",
        "\n",
        "TODO: Dataset unzip Google Drive, Copy to folder\n",
        "\n",
        "TODO: Batch progress number error\n",
        "\n",
        "TODO: Early stop or dropout, when accuracy doesn't improve much\n",
        "\n",
        "Otherwise Continue UDA"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7q7jwnwwhRz2",
      "metadata": {
        "id": "7q7jwnwwhRz2"
      },
      "source": [
        "## 3: UDA \n",
        "\n",
        "Here we use Contrastive Domain Adaptation method proposed [here](https://openaccess.thecvf.com/content_CVPR_2019/papers/Kang_Contrastive_Adaptation_Network_for_Unsupervised_Domain_Adaptation_CVPR_2019_paper.pdf).\n",
        "We train the previous network and run the test on both Source Domain and Target Domain. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "425ysNsFjUy-",
      "metadata": {
        "id": "425ysNsFjUy-"
      },
      "source": [
        "### 3.1 Adversarial Discriminator "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "psgAJw-YhZGJ",
      "metadata": {
        "id": "psgAJw-YhZGJ"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b380a14e",
      "metadata": {
        "id": "b380a14e"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba549cb0",
      "metadata": {
        "id": "ba549cb0"
      },
      "source": [
        "Discriminator Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "id": "718d5319",
      "metadata": {
        "id": "718d5319"
      },
      "outputs": [],
      "source": [
        "# def discriminator_loss(source, target, input_dim=256, hidden_dim=512):\n",
        "#     domain_loss = torch.nn.BCELoss()\n",
        "\n",
        "#     discriminator = Discriminator(input_dim, hidden_dim).cuda()\n",
        "#     domain_source = torch.ones(len(source)).cuda()\n",
        "#     domain_target = torch.zeros(len(target)).cuda()\n",
        "#     domain_source, domain_target = domain_source.view(domain_source.shape[0],1), domain_target.view(domain_target.shape[0],1)\n",
        "\n",
        "#     reverse_source = ReverseLayerF.apply(source, 1)\n",
        "#     reverse_target = ReverseLayerF.apply(target, 1)\n",
        "\n",
        "#     pred_source = discriminator(reverse_source)\n",
        "#     pred_target = discriminator(reverse_target)\n",
        "\n",
        "#     loss_source, loss_target = domain_loss(pred_source, domain_source), domain_loss(pred_target, domain_target)\n",
        "#     return loss_source + loss_target"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_discriminator_loss(source_pred, target_pred):\n",
        "  domain_loss = torch.nn.BCELoss()\n",
        "\n",
        "  domain_source = torch.ones(len(source_pred)).cuda()\n",
        "  domain_target = torch.zeros(len(target_pred)).cuda()\n",
        "  # TODO What's this?\n",
        "  domain_source, domain_target = domain_source.view(domain_source.shape[0],1), domain_target.view(domain_target.shape[0],1)\n",
        "\n",
        "  loss_source, loss_target = domain_loss(source_pred, domain_source), domain_loss(target_pred, domain_target)\n",
        "  return loss_source + loss_target\n"
      ],
      "metadata": {
        "id": "RQRM7ULpKHQa"
      },
      "id": "RQRM7ULpKHQa",
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "c8a0b727",
      "metadata": {
        "id": "c8a0b727"
      },
      "source": [
        "Classification Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "id": "ee11a8d7",
      "metadata": {
        "id": "ee11a8d7"
      },
      "outputs": [],
      "source": [
        "# def classification_loss(source, )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "BBaNX5GgjK7F",
      "metadata": {
        "id": "BBaNX5GgjK7F"
      },
      "source": [
        "### 3.2 Adversarial optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "id": "zKscrnYohp7k",
      "metadata": {
        "id": "zKscrnYohp7k"
      },
      "outputs": [],
      "source": [
        "# def get_adversarial_optimizer(model, config):\n",
        "#   '''\n",
        "#   Get Adversarial Optimizers\n",
        "#   '''\n",
        "#   lr, wd, momtm = config['lr'], config['wd'], config['momentum']\n",
        "  \n",
        "#   # classification layer name: label_classifier\n",
        "#   classifier_name = \"label_classifier\"\n",
        "#   domain_regressor_name = \"domain_regressor\"\n",
        "\n",
        "#   pre_trained_weights = []\n",
        "#   domain_regressor_weights = []\n",
        "#   classifier_weights = []\n",
        "\n",
        "#   # get all the parameters required gradient updates\n",
        "#   for name, param in model.named_parameters():\n",
        "#     if param.requires_grad == True:\n",
        "#       if name.startswith(classifier_name):\n",
        "#         classifier_weights.append(param)\n",
        "#       elif name.startswith(domain_regressor_name):\n",
        "#         domain_regressor_weights.append(param)\n",
        "#       else:\n",
        "#         pre_trained_weights.append(param)\n",
        "\n",
        "#   feature_optim       = torch.optim.SGD([{'params': pre_trained_weights}],     lr=lr/10, weight_decay=wd, momentum=momtm)\n",
        "#   classifier_optim    = torch.optim.SGD([{'params': classifier_weights}],      lr=lr,    weight_decay=wd, momentum=momtm)\n",
        "#   domain_regres_optim = torch.optim.SGD([{'params': domain_regressor_weights}],lr=lr,    weight_decay=wd, momentum=momtm)\n",
        "  \n",
        "#   return feature_optim, classifier_optim, domain_regres_optim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "id": "9b1bff4a",
      "metadata": {
        "id": "9b1bff4a"
      },
      "outputs": [],
      "source": [
        "# def get_adversarial_optimizer(model, config):\n",
        "#   '''\n",
        "#   Get Adversarial Optimizers\n",
        "#   '''\n",
        "#   lr, wd, momtm = config['lr'], config['wd'], config['momentum']\n",
        "  \n",
        "#   # classification layer name: label_classifier\n",
        "#   classifier_name = \"label_classifier\"\n",
        "#   domain_regressor_name = \"domain_regressor\"\n",
        "\n",
        "#   pre_trained_weights = []\n",
        "#   domain_regressor_weights = []\n",
        "#   classifier_weights = []\n",
        "\n",
        "#   # get all the parameters required gradient updates\n",
        "#   for name, param in model.named_parameters():\n",
        "#     if param.requires_grad == True:\n",
        "#       if name.startswith(classifier_name):\n",
        "#         classifier_weights.append(param)\n",
        "#       elif name.startswith(domain_regressor_name):\n",
        "#         domain_regressor_weights.append(param)\n",
        "#       else:\n",
        "#         pre_trained_weights.append(param)\n",
        "\n",
        "#   optim = torch.optim.SGD([{'params': pre_trained_weights, 'lr': lr/10}, \n",
        "#                            {'params': classifier_weights}, \n",
        "#                            {'params': domain_regressor_weights}], lr=lr, weight_decay=wd, momentum=momtm)\n",
        "  \n",
        "#   return optim "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ttOqMBEqufWO",
      "metadata": {
        "id": "ttOqMBEqufWO"
      },
      "source": [
        "### 3.3 Domain Regression Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "id": "xmO1Zk5WufvR",
      "metadata": {
        "id": "xmO1Zk5WufvR"
      },
      "outputs": [],
      "source": [
        "# def compute_domain_regression_loss(source, target, source_pred, target_pred):\n",
        "#   # Use the binomial cross-entropy loss\n",
        "\n",
        "#   # source_loss = F.log_softmax(1-source_pred, dim=1).sum(-1).mean() \n",
        "#   source_loss = torch.log(1-source_pred).sum(-1).mean()\n",
        "#   target_loss = torch.log(target_pred).sum(-1).mean()\n",
        "#   return source_loss + target_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hRbDuyfU929u",
      "metadata": {
        "id": "hRbDuyfU929u"
      },
      "source": [
        "### 3.4 Adversarial Train Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "id": "1SQHSVPE93hi",
      "metadata": {
        "id": "1SQHSVPE93hi"
      },
      "outputs": [],
      "source": [
        "# def adversarial_train_loop(source_loader, target_loader, model, config, device):\n",
        "#   size = len(source_loader.dataset)\n",
        "#   domain_weight = config['domain_regression_weight']\n",
        "  \n",
        "#   # cross entropy loss\n",
        "#   classification_loss = get_cost_function()\n",
        "\n",
        "#   # Get three optimizer\n",
        "#   feature_optim, class_optim, domain_optim = get_adversarial_optimizer(model, config)\n",
        "\n",
        "#   # Target data loader iterator\n",
        "#   iter_target = iter(target_loader)\n",
        "\n",
        "#   for batch, (X_source, y_source) in enumerate(source_loader):\n",
        "#     try:\n",
        "#       X_target, _ = next(iter_target)\n",
        "#     except:\n",
        "#       iter_target = iter(target_loader)\n",
        "#       X_target, _ = next(iter_target)\n",
        "    \n",
        "#     X_source, y_source, X_target = X_source.to(device), y_source.to(device), X_target.to(device)\n",
        "\n",
        "#     class_pred_source, domain_pred_source = model(X_source)\n",
        "#     _,                 domain_pred_target = model(X_target)\n",
        "\n",
        "#     if batch % 2 == 0:\n",
        "#       # classification loss\n",
        "#       class_loss = classification_loss(class_pred_source, y_source)\n",
        "#       class_optim.zero_grad()\n",
        "#       class_loss.backward(retain_graph=True)\n",
        "#       class_optim.step()\n",
        "\n",
        "#       # domain regression loss, negative here to minimize the \"negative\", which is \n",
        "#       # maximize the orignal positive one\n",
        "#       domain_loss = -compute_domain_regression_loss(domain_pred_source, domain_pred_target)\n",
        "#       domain_optim.zero_grad()\n",
        "#       domain_loss.backward()\n",
        "#       domain_optim.step()\n",
        "\n",
        "#       if batch % 10 == 0:\n",
        "#         class_loss, domain_loss, current = class_loss.item(), domain_loss.item(), batch * len(X_source)\n",
        "#         print(f\"classification loss: {class_loss:>7f} negative domain loss: {domain_loss:>7f}[{current:>5d}/{size:>5d}]\")\n",
        "#     else: \n",
        "#       # feature extractor loss\n",
        "#       class_loss  = classification_loss(class_pred_source, y_source)\n",
        "#       # Domain Loss positive here, to add regularization\n",
        "#       domain_loss = compute_domain_regression_loss(domain_pred_source, domain_pred_target)\n",
        "#       feature_extractor_loss = class_loss + domain_weight * domain_loss \n",
        "\n",
        "#       feature_optim.zero_grad()\n",
        "#       feature_extractor_loss.backward()\n",
        "#       feature_optim.step()\n",
        "\n",
        "#       if batch % 11 ==0:\n",
        "#         class_loss, domain_loss, current = class_loss.item(), domain_loss.item(), batch * len(X_source)\n",
        "#         print(f\"classification loss: {class_loss:>7f} domain loss: {domain_loss:>7f}[{current:>5d}/{size:>5d}]\")\n",
        "#         feature_loss, current = feature_extractor_loss.item(), batch * len(X_source)\n",
        "#         print(f\"feature extraction loss: {feature_loss:>7f} [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "#       del feature_extractor_loss \n",
        "\n",
        "#     del class_loss, domain_loss \n",
        "#     del X_source, y_source, X_target, class_pred_source, domain_pred_source, domain_pred_target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "rRvIegSNvF4V"
      },
      "outputs": [],
      "source": [
        "def adversarial_train_loop(source_loader, target_loader, model, config, device):\n",
        "  size = len(source_loader.dataset)\n",
        "  domain_weight = config['domain_regression_weight']\n",
        "   \n",
        "  # cross entropy loss\n",
        "  class_loss_func  = get_class_loss_func()\n",
        "\n",
        "  optim = get_optimizer(model, config, adversarial=True)\n",
        "  \n",
        "  # Target data loader iterator\n",
        "  iter_target = iter(target_loader)\n",
        "\n",
        "  for batch, (X_source, y_source) in enumerate(source_loader):\n",
        "    try:\n",
        "      X_target, _ = next(iter_target)\n",
        "    except:\n",
        "      iter_target = iter(target_loader)\n",
        "      X_target, _ = next(iter_target)\n",
        "    \n",
        "    X_source, y_source, X_target = X_source.to(device), y_source.to(device), X_target.to(device)\n",
        "\n",
        "    class_pred_source, domain_pred_source = model(X_source)\n",
        "    _,                 domain_pred_target = model(X_target)\n",
        "\n",
        "    class_loss = class_loss_func(class_pred_source, y_source)\n",
        "    discm_loss = get_discriminator_loss(domain_pred_source, domain_pred_target)\n",
        "    print('class_loss {}'.format(class_loss))\n",
        "    print('discr_loss {}'.format(discm_loss))\n",
        "\n",
        "    total_loss = class_loss + domain_weight * discm_loss \n",
        "\n",
        "    optim.zero_grad()\n",
        "    total_loss.backward()\n",
        "    optim.step()\n",
        "\n",
        "    del X_source, y_source, X_target, class_pred_source, domain_pred_source, domain_pred_target"
      ],
      "id": "rRvIegSNvF4V"
    },
    {
      "cell_type": "markdown",
      "id": "1jHD6IJ6H8vF",
      "metadata": {
        "id": "1jHD6IJ6H8vF"
      },
      "source": [
        "### 3.5 Adversarial Test Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "id": "KyhdZbs3H9Ue",
      "metadata": {
        "id": "KyhdZbs3H9Ue"
      },
      "outputs": [],
      "source": [
        "def adversarial_test_loop(dataloader, model, device, name=\"\"):\n",
        "  test_loss, correct = 0, 0\n",
        "\n",
        "  class_loss_func = get_class_loss_func()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for X, y in dataloader:\n",
        "      X, y = X.to(device), y.to(device)\n",
        "      class_pred, _ = model(X)\n",
        "\n",
        "      test_loss += class_loss_func(class_pred, y).item()\n",
        "      correct += (class_pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "  size = len(dataloader.dataset)\n",
        "  num_batches = len(dataloader)\n",
        "\n",
        "  test_loss /= num_batches\n",
        "  correct /= size\n",
        "  print(f\"{name} Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "\n",
        "  return test_loss, correct"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tyopBjWr-FAs",
      "metadata": {
        "id": "tyopBjWr-FAs"
      },
      "source": [
        "### 3.6 Adversarial training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "id": "dBCrfNPj-Bxa",
      "metadata": {
        "id": "dBCrfNPj-Bxa"
      },
      "outputs": [],
      "source": [
        "def adversarial_training(model, source_loader, source_test_loader, target_loader, config, device):\n",
        "  print(f\"Learning_rate {config['lr']}, weight_decay {config['wd']}\")\n",
        "\n",
        "  for epoch in range(config['epochs']):\n",
        "    print(f\"Epoch {epoch+1}\\n------------------\")\n",
        "    adversarial_train_loop(source_loader, target_loader, model, config, device)\n",
        "    adversarial_test_loop(source_test_loader, model, device, \"Source Test\")\n",
        "    adversarial_test_loop(target_loader, model, device, \"Target Train\")\n",
        "\n",
        "  print(\"Done\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "kLZMbwjdlDTh",
      "metadata": {
        "id": "kLZMbwjdlDTh"
      },
      "source": [
        "### 3.7 Adversarial Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "id": "wt-IOOThNShC",
      "metadata": {
        "id": "wt-IOOThNShC"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "id": "rcLFqgOelDsW",
      "metadata": {
        "id": "rcLFqgOelDsW"
      },
      "outputs": [],
      "source": [
        "# Get train_dataloader, test_dataloader\n",
        "source_dataset = get_dataset(product_path)\n",
        "train_dataloader, train_test_dataloader = get_dataloader(source_dataset, 128)\n",
        "\n",
        "adv_model = DANN(len(source_dataset.classes)).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZgN_WIqqPtjk",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZgN_WIqqPtjk",
        "outputId": "32cfdab1-a477-4655-c854-943fe7545d4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Learning_rate 0.001, weight_decay 0.001\n",
            "Epoch 1\n",
            "------------------\n",
            "class_loss 3.3811676502227783\n",
            "discr_loss 1.3925280570983887\n",
            "class_loss 3.2137138843536377\n",
            "discr_loss 1.3868913650512695\n",
            "class_loss 3.006359815597534\n",
            "discr_loss 1.3629651069641113\n",
            "class_loss 2.9859161376953125\n",
            "discr_loss 1.3387115001678467\n",
            "class_loss 2.7200682163238525\n",
            "discr_loss 1.3783187866210938\n",
            "class_loss 2.5018374919891357\n",
            "discr_loss 1.3717260360717773\n",
            "class_loss 2.3374557495117188\n",
            "discr_loss 1.3300957679748535\n",
            "class_loss 2.0043320655822754\n",
            "discr_loss 1.341005563735962\n",
            "class_loss 1.658748984336853\n",
            "discr_loss 1.3256382942199707\n",
            "class_loss 1.676161527633667\n",
            "discr_loss 1.3070709705352783\n",
            "class_loss 1.5400370359420776\n",
            "discr_loss 1.2954800128936768\n",
            "class_loss 1.2905508279800415\n",
            "discr_loss 1.2944834232330322\n",
            "class_loss 1.0999844074249268\n",
            "discr_loss 1.3124091625213623\n",
            "Source Test Test Error: \n",
            " Accuracy: 75.7%, Avg loss: 1.167455 \n",
            "\n",
            "Target Train Test Error: \n",
            " Accuracy: 48.6%, Avg loss: 1.933682 \n",
            "\n",
            "Epoch 2\n",
            "------------------\n"
          ]
        }
      ],
      "source": [
        "target_dataset = get_dataset(real_life_path)\n",
        "# target_dataloader = DataLoader(target_dataset, batch_size=128, shuffle=False)\n",
        "target_dataloader, target_test_dataloader = get_dataloader(target_dataset, 128)\n",
        "\n",
        "# Training\n",
        "# torch.autograd.set_detect_anomaly(True)\n",
        "adversarial_training(adv_model, train_dataloader, train_test_dataloader, target_dataloader, config, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "nSh7gaC8JaVC",
      "metadata": {
        "id": "nSh7gaC8JaVC"
      },
      "source": [
        "### 3.8 Testing on Target Domain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "id": "oUUo7mSiJasc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oUUo7mSiJasc",
        "outputId": "43ee8ad6-d6b7-4b34-85e8-d8fd0e5cb817"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1944: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Error: \n",
            " Accuracy: 59.0%, Avg loss: 1.435003 \n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1.4350030928850175, 0.59)"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ],
      "source": [
        "target_dataset = get_dataset(real_life_path)\n",
        "loader_target_dataset = DataLoader(target_dataset, batch_size=100, shuffle=False)\n",
        "\n",
        "# adversarial_test_loop(loader_target_dataset, adv_model, device)\n",
        "test_loop(loader_target_dataset, adv_model, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bFtADV4AtN1Z",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "id": "bFtADV4AtN1Z",
        "outputId": "eb74e9ba-fa12-415c-8e9f-5ee5338b20b2"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-177-5d261b590725>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mdel\u001b[0m \u001b[0msource_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloader_target_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0madv_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'source_dataset' is not defined"
          ]
        }
      ],
      "source": [
        "del source_dataset, train_dataloader, test_dataloader, target_dataset, loader_target_dataset\n",
        "del adv_model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "CKsMcxgihquu",
      "metadata": {
        "id": "CKsMcxgihquu"
      },
      "source": [
        "## Part-4: Comparison & Discussion\n",
        "Here we compare the test result from the direct method and the UDA method. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6C0fg_GZh4ou",
      "metadata": {
        "id": "6C0fg_GZh4ou"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lcDs1PTfh9G9",
      "metadata": {
        "id": "lcDs1PTfh9G9"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "u3VyXbyth9sO",
      "metadata": {
        "id": "u3VyXbyth9sO"
      },
      "source": [
        "## Part-5: Conclusion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xgSBA7eMh-Nk",
      "metadata": {
        "id": "xgSBA7eMh-Nk"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "UDA_0_1_.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}