{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "UOmUNvGYgPRN",
      "metadata": {
        "id": "UOmUNvGYgPRN"
      },
      "source": [
        "# Unsupervised Domain Adaptation Project\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ExU_wmMAgVsR",
      "metadata": {
        "id": "ExU_wmMAgVsR"
      },
      "source": [
        "## 1: Data download\n",
        "Load data to project from Google Drive. Copy a subset of classes of images to the path:\n",
        "- `adaptiope_small/product_images`\n",
        "- `adaptiope_small/real_life` \n",
        "\n",
        "two directories. They represent images from two different domain **product** and **real_life**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "4134f6cf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4134f6cf",
        "outputId": "2d5ebc1e-d81d-4a07-b6a7-05356c6afa72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "mkdir: cannot create directory ‘dataset’: File exists\n",
            "replace Adaptiope/product_images/watering can/watering can_080.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n"
          ]
        }
      ],
      "source": [
        "from os import makedirs, listdir\n",
        "from tqdm import tqdm\n",
        "from google.colab import drive\n",
        "from os.path import join\n",
        "from shutil import copytree\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "!mkdir dataset\n",
        "!cp \"gdrive/My Drive/Colab Notebooks/data/Adaptiope.zip\" dataset/\n",
        "# !ls dataset\n",
        "\n",
        "!unzip -qq dataset/Adaptiope.zip   # unzip file\n",
        "\n",
        "!rm -rf dataset/Adaptiope.zip \n",
        "!rm -rf adaptiope_small"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "0a6cac67",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0a6cac67",
        "outputId": "39d775e8-c51e-4267-a8a1-3548ea50ba7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ice skates', 'chainsaw', 'hot glue gun', 'grill', 'tank', 'computer', 'coat hanger', 'speakers', 'sleeping bag', 'razor', 'skeleton', 'syringe', 'bookcase', 'car jack', 'boxing gloves', 'microwave', 'smartphone', 'toothbrush', 'stapler', 'smoking pipe', 'wheelchair', 'ring binder', 'rc car', 'projector', 'in-ear headphones', 'hourglass', 'bicycle', 'golf club', 'compass', 'sword', 'hoverboard', 'fighter jet', 'corkscrew', 'acoustic guitar', 'flat iron', 'spatula', 'mixing console', 'cellphone', 'comb', 'lawn mower', 'hat', 'motorbike helmet', 'ice cube tray', 'helicopter', 'binoculars', 'pen', 'watering can', 'vr goggles', 'computer mouse', 'letter tray', 'hard-wired fixed phone', 'puncher', 'scissors', 'power strip', 'mug', 'pogo stick', 'roller skates', 'network switch', 'bottle', 'stand mixer', 'webcam', 'phonograph', 'brachiosaurus', 'keyboard', 'knife', 'fan', 'office chair', 'rubber boat', 'stroller', 'trash can', 'monitor', 'bicycle helmet', 'scooter', 'diving fins', 'ruler', 'pikachu', 'toilet brush', 'crown', 'file cabinet', 'baseball bat', 'snow shovel', 'sewing machine', 'electric guitar', 'wallet', 'hand mixer', 'screwdriver', 'notepad', 'quadcopter', 'skateboard', 'backpack', 'game controller', 'telescope', 'calculator', 'drum set', 'usb stick', 'tape dispenser', 'nail clipper', 'rifle', 'cordless fixed phone', 'laptop', 'tent', 'stethoscope', 'fire extinguisher', 'handgun', 'dart', 'hair dryer', 'handcuffs', 'ladder', 'pipe wrench', 'tyrannosaurus', 'printer', 'desk lamp', 'electric shaver', 'wristwatch', 'power drill', 'shower head', 'purse', 'glasses', 'vacuum cleaner', 'axe', 'umbrella', 'over-ear headphones', 'magic lamp']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20/20 [00:02<00:00,  8.70it/s]\n",
            "100%|██████████| 20/20 [00:05<00:00,  3.92it/s]\n"
          ]
        }
      ],
      "source": [
        "!mkdir adaptiope_small\n",
        "classes = listdir(\"Adaptiope/product_images\")\n",
        "print(classes)\n",
        "classes = [\"backpack\", \"bookcase\", \"car jack\", \"comb\", \"crown\", \"file cabinet\", \"flat iron\", \"game controller\", \"glasses\",\n",
        "           \"helicopter\", \"ice skates\", \"letter tray\", \"monitor\", \"mug\", \"network switch\", \"over-ear headphones\", \"pen\",\n",
        "           \"purse\", \"stand mixer\", \"stroller\"]\n",
        "domain_classes = [\"product_images\", \"real_life\"]\n",
        "for d, td in zip([\"Adaptiope/product_images\", \"Adaptiope/real_life\"], [\"adaptiope_small/product_images\", \"adaptiope_small/real_life\"]):\n",
        "  makedirs(td)\n",
        "  for c in tqdm(classes):\n",
        "    c_path = join(d, c)\n",
        "    c_target = join(td, c)\n",
        "    copytree(c_path, c_target)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "product_path = 'adaptiope_small/product_images'\n",
        "real_life_path = 'adaptiope_small/real_life'"
      ],
      "metadata": {
        "id": "S8yOthKskBmC"
      },
      "id": "S8yOthKskBmC",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "uUHNozN6ggRr",
      "metadata": {
        "id": "uUHNozN6ggRr"
      },
      "source": [
        "## 2: Domain-Adversarial training of Neural Network\n",
        "We implement DANN UDA method [DANN](https://arxiv.org/pdf/1505.07818.pdf)\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fwiJOBXWpeS0",
      "metadata": {
        "id": "fwiJOBXWpeS0"
      },
      "source": [
        "### 2.0: Import Libraries and Data Loading\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "_GZxbLlT6O8m",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_GZxbLlT6O8m",
        "outputId": "a5e875ca-82c4-499c-9e90-d79e85f04ab7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image size:  (679, 679)\n"
          ]
        }
      ],
      "source": [
        "from PIL import Image\n",
        "from os.path import join\n",
        "import math\n",
        "\n",
        "img = Image.open(join(product_path, 'backpack', 'backpack_003.jpg'))\n",
        "print('Image size: ', img.size)\n",
        "#img"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "RE2WUwy9BORV",
      "metadata": {
        "id": "RE2WUwy9BORV"
      },
      "source": [
        "import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "Vei6SzEeggzU",
      "metadata": {
        "id": "Vei6SzEeggzU"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import softmax\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision.models import vgg11, alexnet \n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision.transforms.transforms import ToTensor"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "M5rt9x7nBKiT",
      "metadata": {
        "id": "M5rt9x7nBKiT"
      },
      "source": [
        "configuration constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "JXznFSNkAzn3",
      "metadata": {
        "id": "JXznFSNkAzn3"
      },
      "outputs": [],
      "source": [
        "img_size = 256\n",
        "# mean, std used by pre-trained models from PyTorch\n",
        "mean, std = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n",
        "config = dict(epochs=10, batch_size=64,lr=0.01, wd=0.001, momentum=0.9, alpha=10, beta=0.75, gamma=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_Rz4CI9spEkN",
      "metadata": {
        "id": "_Rz4CI9spEkN"
      },
      "source": [
        "Configue GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "nHv2o65FpDpn",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nHv2o65FpDpn",
        "outputId": "626a9560-55de-490e-f2aa-fbe4aae9b662"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "GF3YyTBAhJk8",
      "metadata": {
        "id": "GF3YyTBAhJk8"
      },
      "outputs": [],
      "source": [
        "def get_dataset(root_path):\n",
        "  '''\n",
        "    Get dataset from specific data path\n",
        "\n",
        "    # parameters:\n",
        "        root_path: path to image folder\n",
        "\n",
        "    # return: train_loader, test_loader\n",
        "  '''\n",
        "  # Construct image transform\n",
        "  image_transform = transforms.Compose([\n",
        "    transforms.Resize(img_size),\n",
        "    transforms.CenterCrop(img_size),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std)\n",
        "  ])\n",
        "\n",
        "  # Load data from filesystem\n",
        "  image_dataset = ImageFolder(root_path, transform=image_transform)\n",
        "\n",
        "  return image_dataset\n",
        "\n",
        "def get_dataloader(dataset, batch_size, shuffle_train=True, shuffle_test=False):\n",
        "  '''\n",
        "    Get DataLoader from specific data path\n",
        "\n",
        "    # parameters:\n",
        "        dataset: ImageFolder instance\n",
        "        batch_size: batch_size for DataLoader\n",
        "        shuffle_train: whether to shuffle training data\n",
        "        shuffle_test: whether to shuffle test data\n",
        "  '''\n",
        "  # Get train, test number\n",
        "  num_total = len(dataset)\n",
        "  num_train = int(num_total * 0.8 + 1)\n",
        "  num_test  = num_total - num_train\n",
        "\n",
        "  # random split dataset\n",
        "  data_train, data_test = random_split(dataset, [num_train, num_test])\n",
        "\n",
        "  # initialize dataloaders\n",
        "  loader_train = DataLoader(data_train, batch_size=batch_size, shuffle=shuffle_train)\n",
        "  loader_test  = DataLoader(data_test, batch_size=batch_size, shuffle=shuffle_test)\n",
        "\n",
        "  return loader_train, loader_test"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-J4MSc3spcYU",
      "metadata": {
        "id": "-J4MSc3spcYU"
      },
      "source": [
        "### 2.1 Define Feature Extractor with Pretrain Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "95ff2db4",
      "metadata": {
        "id": "95ff2db4"
      },
      "outputs": [],
      "source": [
        "class FeatureExtractor(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(FeatureExtractor, self).__init__()\n",
        "\n",
        "    # Feature Extractor with AlexNet\n",
        "    self.feature_extractor = alexnet(pretrained=True)\n",
        "    self.feature_dim = self.feature_extractor.classifier[-1].in_features\n",
        "\n",
        "    # make the last layer identity\n",
        "    self.feature_extractor.classifier[-1] = nn.Identity()\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.feature_extractor(x)\n",
        "  \n",
        "  def output_dim(self):\n",
        "    return self.feature_dim"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ZjWy3ak98L0F",
      "metadata": {
        "id": "ZjWy3ak98L0F"
      },
      "source": [
        "### 2.2 Define Classifier, Discriminator with RevereLayerF for training the Feature Extractor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "517118c3",
      "metadata": {
        "id": "517118c3"
      },
      "outputs": [],
      "source": [
        "class Classifier(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(Classifier, self).__init__()\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(input_dim, 1024),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(512, output_dim)\n",
        "        )\n",
        "    \n",
        "    def forward(self, X):\n",
        "        return self.classifier(X) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "RES6EY4PO7KF",
      "metadata": {
        "id": "RES6EY4PO7KF"
      },
      "outputs": [],
      "source": [
        "from torch.autograd import Function\n",
        "\n",
        "class ReverseLayerF(Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, tensor):\n",
        "        return tensor.view_as(tensor)\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        return grad_output.neg(), None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "K9syaPhFxOFd",
      "metadata": {
        "id": "K9syaPhFxOFd"
      },
      "outputs": [],
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.discriminator =  nn.Sequential(\n",
        "            nn.Linear(int(input_dim), 1024),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(1024,1),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        validity = self.discriminator(x)\n",
        "        return validity "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "BsbwoZkZwjjl",
      "metadata": {
        "id": "BsbwoZkZwjjl"
      },
      "outputs": [],
      "source": [
        "class DANN(nn.Module):\n",
        "  # def __init__(self, num_classes, adversarial=True):\n",
        "  def __init__(self, num_classes):\n",
        "    super(DANN, self).__init__()\n",
        "    self.output_dim = num_classes\n",
        "\n",
        "    # define inner network component\n",
        "    self.feature_extractor = FeatureExtractor()\n",
        "    self.classifier = Classifier(self.feature_extractor.output_dim(), num_classes)\n",
        "    self.discriminator = Discriminator(self.feature_extractor.output_dim())  \n",
        "  \n",
        "  def forward(self, x):\n",
        "    feature_output = self.feature_extractor(x)\n",
        "\n",
        "    class_pred = self.classifier(feature_output)\n",
        "\n",
        "    # Add a ReverseLayer here for negative gradient computation\n",
        "    reverse_feature = ReverseLayerF.apply(feature_output)\n",
        "    domain_pred = self.discriminator(reverse_feature)\n",
        "\n",
        "    return class_pred, domain_pred "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "IhnsX7lq5saz",
      "metadata": {
        "id": "IhnsX7lq5saz"
      },
      "source": [
        "### 2.3 Cost function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "q6BkiCH_5sOr",
      "metadata": {
        "id": "q6BkiCH_5sOr"
      },
      "outputs": [],
      "source": [
        "def get_class_loss_func():\n",
        "  return nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "B-y0dTPj5zrR",
      "metadata": {
        "id": "B-y0dTPj5zrR"
      },
      "source": [
        "### 2.4 Optimizer\n",
        "\n",
        "Setting the **learning rate** according to the original [paper](https://arxiv.org/pdf/1505.07818.pdf) section 5.2.2\n",
        "\n",
        "$$ \\mu_p =  \\frac{\\mu_0}{(1+\\alpha \\cdot p)^\\beta}$$\n",
        "\n",
        "where p is the training progress linearly changing from 0 to 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "ZsoNnQCq2aEt",
      "metadata": {
        "id": "ZsoNnQCq2aEt"
      },
      "outputs": [],
      "source": [
        "def get_optimizer(model, config, progress, adversarial=True):\n",
        "  '''\n",
        "  Config Optimizer\n",
        "  '''\n",
        "  learning_rate = config['lr']\n",
        "  learning_rate = learning_rate / ((1 + config['alpha']*progress)**config['beta'])\n",
        "\n",
        "  weight_decay  = config['wd']\n",
        "  momentum      = config['momentum']\n",
        "\n",
        "  feature_ext   = model.get_submodule(\"feature_extractor\")\n",
        "  classifier    = model.get_submodule(\"classifier\")\n",
        "  discriminator = model.get_submodule(\"discriminator\")\n",
        "\n",
        "  pre_trained_weights = feature_ext.parameters()\n",
        "\n",
        "  if adversarial:\n",
        "    other_weights = list(classifier.parameters()) + list(discriminator.parameters())\n",
        "  else:\n",
        "    other_weights = list(classifier.parameters())\n",
        "\n",
        "  # assign parameters to parameters\n",
        "  optimizer = torch.optim.SGD([\n",
        "    {'params': pre_trained_weights},\n",
        "    {'params': other_weights, 'lr': learning_rate}\n",
        "  ], lr= learning_rate/10, weight_decay=weight_decay, momentum=momentum)\n",
        "  \n",
        "  return optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Atdanl9REs3F",
      "metadata": {
        "id": "Atdanl9REs3F"
      },
      "source": [
        "### 2.5 Training Loop and Testing Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "ORDqPkiT5r1s",
      "metadata": {
        "id": "ORDqPkiT5r1s"
      },
      "outputs": [],
      "source": [
        "def train_loop(dataloader, model, device, progress):\n",
        "  \"\"\"\n",
        "    Return:\n",
        "      @best_state: best performance model state parameters\n",
        "      @best_loss: best performance loss\n",
        "  \"\"\"\n",
        "  size = len(dataloader.dataset)\n",
        "  loss_fn = get_class_loss_func()\n",
        "\n",
        "  optimizer = get_optimizer(model, config, progress, adversarial=False)\n",
        "\n",
        "  best_loss  = float('inf')\n",
        "  best_state = None\n",
        "\n",
        "  for batch, (X, y) in enumerate(dataloader):\n",
        "    X, y = X.to(device), y.to(device)\n",
        "    \n",
        "    # compute prediction and loss\n",
        "    class_pred, _ = model(X)\n",
        "\n",
        "    # classification loss\n",
        "    loss = loss_fn(class_pred, y)\n",
        "    \n",
        "    # store best state\n",
        "    curr_loss = loss.item()\n",
        "    if curr_loss < best_loss:\n",
        "      best_loss  = curr_loss\n",
        "      best_state = model.state_dict()\n",
        "      print(f\"## Update ## best_state with loss: {curr_loss:>7f}\")\n",
        "    \n",
        "    # backpropagation\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if batch % 10 == 0:\n",
        "      current = batch * len(X)\n",
        "      print(f\"## Meter  ## current loss: {curr_loss:>7f} [{current:>5d}/{size:>5d}]\")\n",
        "    \n",
        "  return best_state, best_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "LlvQLNUDLGJF",
      "metadata": {
        "id": "LlvQLNUDLGJF"
      },
      "outputs": [],
      "source": [
        "def test_loop(dataloader, model, device):\n",
        "  test_loss, correct = 0, 0\n",
        "  loss_fn = get_class_loss_func()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for X, y in dataloader:\n",
        "      X, y = X.to(device), y.to(device)\n",
        "      class_pred, _ = model(X)\n",
        "\n",
        "      test_loss += loss_fn(class_pred, y).item()\n",
        "      correct += (class_pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "  size = len(dataloader.dataset)\n",
        "  num_batches = len(dataloader)\n",
        "\n",
        "  test_loss /= num_batches\n",
        "  correct /= size\n",
        "  print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "\n",
        "  return test_loss, correct"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "z5uhItnwhj8S",
      "metadata": {
        "id": "z5uhItnwhj8S"
      },
      "source": [
        "### 2.6 Training Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "nM76Syqahknd",
      "metadata": {
        "id": "nM76Syqahknd"
      },
      "outputs": [],
      "source": [
        "def training(model, train_dataloader, test_dataloader, config, device):\n",
        "  epochs = config['epochs']\n",
        "  # print(f\"Learning_rate {config['lr']}, weight_decay {config['wd']}\")\n",
        "\n",
        "  best_state, best_loss = None, float('inf')\n",
        "  no_improve_count = 0\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    print(f\"Epoch {epoch+1}\\n------------------\")\n",
        "    progress = epoch/epochs\n",
        "\n",
        "    curr_state, _ = train_loop(train_dataloader, model, device, progress)\n",
        "\n",
        "    # Test with curr_state\n",
        "    model.load_state_dict(curr_state)\n",
        "\n",
        "    test_loss, _ = test_loop(test_dataloader, model, device)\n",
        "\n",
        "    # store the best performed state parameters\n",
        "    if test_loss < best_loss:\n",
        "      no_improve_count = 0\n",
        "\n",
        "      best_state = curr_state\n",
        "      best_loss  = test_loss\n",
        "    else:\n",
        "      no_improve_count += 1\n",
        "    \n",
        "    if no_improve_count > 2:\n",
        "      print(f\"## No Improvement on Test Set, Stopping ##\")\n",
        "      break\n",
        "\n",
        "  model.load_state_dict(best_state)\n",
        "  print(\"Done\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "PIkYlRhi4tkK",
      "metadata": {
        "id": "PIkYlRhi4tkK"
      },
      "source": [
        "## 3 Training without using Domain Adaptation techniques"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " ### 3.1 Product Domain -> Real Life"
      ],
      "metadata": {
        "id": "chQgbFmYorL-"
      },
      "id": "chQgbFmYorL-"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "OCWnRXkc79a4",
      "metadata": {
        "id": "OCWnRXkc79a4"
      },
      "outputs": [],
      "source": [
        "# Get dataloader\n",
        "product_dataset   = get_dataset(product_path)\n",
        "real_life_dataset = get_dataset(real_life_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.1.1 Training on Source Domain"
      ],
      "metadata": {
        "id": "raxew0mKm-gM"
      },
      "id": "raxew0mKm-gM"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "V93UE4rXi3C9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V93UE4rXi3C9",
        "outputId": "cf9d9147-511a-4a0f-bbdb-9429d5aaa2b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
            "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "------------------\n",
            "## Update ## best_state with loss: 3.000999\n",
            "## Meter  ## current loss: 3.000999 [    0/ 1601]\n",
            "## Update ## best_state with loss: 2.959620\n",
            "## Update ## best_state with loss: 2.926028\n",
            "## Update ## best_state with loss: 2.904498\n",
            "## Update ## best_state with loss: 2.808029\n",
            "## Update ## best_state with loss: 2.693931\n",
            "## Update ## best_state with loss: 2.628657\n",
            "## Update ## best_state with loss: 2.576085\n",
            "## Update ## best_state with loss: 2.549546\n",
            "## Update ## best_state with loss: 2.382282\n",
            "## Meter  ## current loss: 2.382282 [  640/ 1601]\n",
            "## Update ## best_state with loss: 2.195917\n",
            "## Update ## best_state with loss: 1.951670\n",
            "## Update ## best_state with loss: 1.903493\n",
            "## Update ## best_state with loss: 1.634728\n",
            "## Update ## best_state with loss: 1.402812\n",
            "## Update ## best_state with loss: 1.268612\n",
            "## Update ## best_state with loss: 0.888332\n",
            "## Update ## best_state with loss: 0.813600\n",
            "## Update ## best_state with loss: 0.652161\n",
            "## Meter  ## current loss: 0.733628 [ 1280/ 1601]\n",
            "## Update ## best_state with loss: 0.457398\n",
            "## Update ## best_state with loss: 0.368967\n",
            "## Update ## best_state with loss: 0.019534\n",
            "Test Error: \n",
            " Accuracy: 82.0%, Avg loss: 0.587528 \n",
            "\n",
            "Epoch 2\n",
            "------------------\n",
            "## Update ## best_state with loss: 0.517409\n",
            "## Meter  ## current loss: 0.517409 [    0/ 1601]\n",
            "## Update ## best_state with loss: 0.407533\n",
            "## Update ## best_state with loss: 0.394233\n",
            "## Update ## best_state with loss: 0.232539\n",
            "## Meter  ## current loss: 0.409878 [  640/ 1601]\n",
            "## Meter  ## current loss: 0.414157 [ 1280/ 1601]\n",
            "## Update ## best_state with loss: 0.013341\n",
            "Test Error: \n",
            " Accuracy: 87.7%, Avg loss: 0.413433 \n",
            "\n",
            "Epoch 3\n",
            "------------------\n",
            "## Update ## best_state with loss: 0.365270\n",
            "## Meter  ## current loss: 0.365270 [    0/ 1601]\n",
            "## Update ## best_state with loss: 0.261358\n",
            "## Update ## best_state with loss: 0.225717\n",
            "## Update ## best_state with loss: 0.217061\n",
            "## Update ## best_state with loss: 0.171490\n",
            "## Update ## best_state with loss: 0.167619\n",
            "## Meter  ## current loss: 0.210425 [  640/ 1601]\n",
            "## Update ## best_state with loss: 0.117581\n",
            "## Meter  ## current loss: 0.445273 [ 1280/ 1601]\n",
            "Test Error: \n",
            " Accuracy: 92.0%, Avg loss: 0.298010 \n",
            "\n",
            "Epoch 4\n",
            "------------------\n",
            "## Update ## best_state with loss: 0.153248\n",
            "## Meter  ## current loss: 0.153248 [    0/ 1601]\n",
            "## Update ## best_state with loss: 0.085810\n",
            "## Update ## best_state with loss: 0.029881\n",
            "## Meter  ## current loss: 0.148481 [  640/ 1601]\n",
            "## Meter  ## current loss: 0.104019 [ 1280/ 1601]\n",
            "## Update ## best_state with loss: 0.006604\n",
            "Test Error: \n",
            " Accuracy: 91.7%, Avg loss: 0.233351 \n",
            "\n",
            "Epoch 5\n",
            "------------------\n",
            "## Update ## best_state with loss: 0.061013\n",
            "## Meter  ## current loss: 0.061013 [    0/ 1601]\n",
            "## Update ## best_state with loss: 0.035778\n",
            "## Meter  ## current loss: 0.046406 [  640/ 1601]\n",
            "## Update ## best_state with loss: 0.028982\n",
            "## Meter  ## current loss: 0.052620 [ 1280/ 1601]\n",
            "Test Error: \n",
            " Accuracy: 93.5%, Avg loss: 0.221824 \n",
            "\n",
            "Epoch 6\n",
            "------------------\n",
            "## Update ## best_state with loss: 0.080619\n",
            "## Meter  ## current loss: 0.080619 [    0/ 1601]\n",
            "## Update ## best_state with loss: 0.057997\n",
            "## Update ## best_state with loss: 0.049269\n",
            "## Update ## best_state with loss: 0.048589\n",
            "## Meter  ## current loss: 0.084289 [  640/ 1601]\n",
            "## Meter  ## current loss: 0.088758 [ 1280/ 1601]\n",
            "## Update ## best_state with loss: 0.035096\n",
            "Test Error: \n",
            " Accuracy: 93.2%, Avg loss: 0.248399 \n",
            "\n",
            "Epoch 7\n",
            "------------------\n",
            "## Update ## best_state with loss: 0.068417\n",
            "## Meter  ## current loss: 0.068417 [    0/ 1601]\n",
            "## Update ## best_state with loss: 0.058779\n",
            "## Update ## best_state with loss: 0.052224\n",
            "## Update ## best_state with loss: 0.044173\n",
            "## Update ## best_state with loss: 0.040838\n",
            "## Meter  ## current loss: 0.053610 [  640/ 1601]\n",
            "## Update ## best_state with loss: 0.015105\n",
            "## Meter  ## current loss: 0.050381 [ 1280/ 1601]\n",
            "## Update ## best_state with loss: 0.000006\n",
            "Test Error: \n",
            " Accuracy: 93.2%, Avg loss: 0.211774 \n",
            "\n",
            "Epoch 8\n",
            "------------------\n",
            "## Update ## best_state with loss: 0.035490\n",
            "## Meter  ## current loss: 0.035490 [    0/ 1601]\n",
            "## Update ## best_state with loss: 0.030474\n",
            "## Meter  ## current loss: 0.074523 [  640/ 1601]\n",
            "## Update ## best_state with loss: 0.014118\n",
            "## Meter  ## current loss: 0.062682 [ 1280/ 1601]\n",
            "## Update ## best_state with loss: 0.000061\n",
            "Test Error: \n",
            " Accuracy: 92.5%, Avg loss: 0.217580 \n",
            "\n",
            "Epoch 9\n",
            "------------------\n",
            "## Update ## best_state with loss: 0.075012\n",
            "## Meter  ## current loss: 0.075012 [    0/ 1601]\n",
            "## Update ## best_state with loss: 0.025435\n",
            "## Update ## best_state with loss: 0.018918\n",
            "## Meter  ## current loss: 0.082604 [  640/ 1601]\n",
            "## Update ## best_state with loss: 0.016765\n",
            "## Meter  ## current loss: 0.052447 [ 1280/ 1601]\n",
            "## Update ## best_state with loss: 0.001535\n",
            "Test Error: \n",
            " Accuracy: 94.0%, Avg loss: 0.194300 \n",
            "\n",
            "Epoch 10\n",
            "------------------\n",
            "## Update ## best_state with loss: 0.017796\n",
            "## Meter  ## current loss: 0.017796 [    0/ 1601]\n",
            "## Meter  ## current loss: 0.027518 [  640/ 1601]\n",
            "## Meter  ## current loss: 0.031322 [ 1280/ 1601]\n",
            "## Update ## best_state with loss: 0.001934\n",
            "Test Error: \n",
            " Accuracy: 91.2%, Avg loss: 0.242345 \n",
            "\n",
            "Done\n"
          ]
        }
      ],
      "source": [
        "train_dataloader, test_dataloader = get_dataloader(product_dataset, config['batch_size'])\n",
        "\n",
        "model = DANN(len(product_dataset.classes)).to(device)\n",
        "\n",
        "# Training\n",
        "training(model, train_dataloader, test_dataloader, config, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "__d71xhd5SVx",
      "metadata": {
        "id": "__d71xhd5SVx"
      },
      "source": [
        "#### 3.1.2 Test on Real Life"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "E54Vm3jmTRcc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E54Vm3jmTRcc",
        "outputId": "2cb0cbd5-8865-429c-be31-04dead26ae12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Error: \n",
            " Accuracy: 64.6%, Avg loss: 1.221270 \n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1.2212704503908753, 0.646)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "loader_target_dataset = DataLoader(real_life_dataset, batch_size=config['batch_size'], shuffle=False)\n",
        "\n",
        "# model.load_state_dict(torch.load('model_state.pt', map_location='cpu'))\n",
        "test_loop(loader_target_dataset, model, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "Cw4GP_z-_iy_",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cw4GP_z-_iy_",
        "outputId": "ea0d484b-f337-4d97-859c-b0abc68c244c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "513480192\n"
          ]
        }
      ],
      "source": [
        "del train_dataloader, test_dataloader, loader_target_dataset\n",
        "del model\n",
        "print(torch.cuda.memory_allocated())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ITPgzgfR5BQB",
      "metadata": {
        "id": "ITPgzgfR5BQB"
      },
      "source": [
        "### 3.2 Real Life -> Product\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.2.1 Training on Real Life"
      ],
      "metadata": {
        "id": "42FQtZSXoRSI"
      },
      "id": "42FQtZSXoRSI"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "Sb5mqdPMBHli",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sb5mqdPMBHli",
        "outputId": "ab0390af-5249-4042-81e0-675f0e020b2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
            "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "------------------\n",
            "## Update ## best_state with loss: 3.019980\n",
            "## Meter  ## current loss: 3.019980 [    0/ 1601]\n",
            "## Update ## best_state with loss: 2.985003\n",
            "## Update ## best_state with loss: 2.971178\n",
            "## Update ## best_state with loss: 2.941204\n",
            "## Update ## best_state with loss: 2.902153\n",
            "## Update ## best_state with loss: 2.869621\n",
            "## Update ## best_state with loss: 2.761132\n",
            "## Update ## best_state with loss: 2.707194\n",
            "## Meter  ## current loss: 2.707194 [  640/ 1601]\n",
            "## Update ## best_state with loss: 2.656637\n",
            "## Update ## best_state with loss: 2.635004\n",
            "## Update ## best_state with loss: 2.562607\n",
            "## Update ## best_state with loss: 2.362159\n",
            "## Update ## best_state with loss: 2.349334\n",
            "## Update ## best_state with loss: 2.225987\n",
            "## Update ## best_state with loss: 1.874920\n",
            "## Update ## best_state with loss: 1.710216\n",
            "## Meter  ## current loss: 1.710216 [ 1280/ 1601]\n",
            "## Update ## best_state with loss: 1.480511\n",
            "## Update ## best_state with loss: 1.214254\n",
            "Test Error: \n",
            " Accuracy: 33.3%, Avg loss: 2.447212 \n",
            "\n",
            "Epoch 2\n",
            "------------------\n",
            "## Update ## best_state with loss: 2.539697\n",
            "## Meter  ## current loss: 2.539697 [    0/ 1601]\n",
            "## Update ## best_state with loss: 1.344823\n",
            "## Update ## best_state with loss: 1.111352\n",
            "## Update ## best_state with loss: 0.895433\n",
            "## Meter  ## current loss: 1.782368 [  640/ 1601]\n",
            "## Update ## best_state with loss: 0.774896\n",
            "## Update ## best_state with loss: 0.768483\n",
            "## Meter  ## current loss: 0.768483 [ 1280/ 1601]\n",
            "## Update ## best_state with loss: 0.178619\n",
            "Test Error: \n",
            " Accuracy: 76.2%, Avg loss: 0.791465 \n",
            "\n",
            "Epoch 3\n",
            "------------------\n",
            "## Update ## best_state with loss: 0.643601\n",
            "## Meter  ## current loss: 0.643601 [    0/ 1601]\n",
            "## Update ## best_state with loss: 0.498355\n",
            "## Update ## best_state with loss: 0.453736\n",
            "## Update ## best_state with loss: 0.393418\n",
            "## Meter  ## current loss: 0.393418 [  640/ 1601]\n",
            "## Meter  ## current loss: 0.467840 [ 1280/ 1601]\n",
            "Test Error: \n",
            " Accuracy: 67.4%, Avg loss: 1.293746 \n",
            "\n",
            "Epoch 4\n",
            "------------------\n",
            "## Update ## best_state with loss: 0.758187\n",
            "## Meter  ## current loss: 0.758187 [    0/ 1601]\n",
            "## Update ## best_state with loss: 0.699559\n",
            "## Update ## best_state with loss: 0.355910\n",
            "## Update ## best_state with loss: 0.323983\n",
            "## Meter  ## current loss: 0.441697 [  640/ 1601]\n",
            "## Update ## best_state with loss: 0.321285\n",
            "## Update ## best_state with loss: 0.286489\n",
            "## Update ## best_state with loss: 0.235801\n",
            "## Meter  ## current loss: 0.371154 [ 1280/ 1601]\n",
            "## Update ## best_state with loss: 0.014948\n",
            "Test Error: \n",
            " Accuracy: 79.7%, Avg loss: 0.625379 \n",
            "\n",
            "Epoch 5\n",
            "------------------\n",
            "## Update ## best_state with loss: 0.204655\n",
            "## Meter  ## current loss: 0.204655 [    0/ 1601]\n",
            "## Update ## best_state with loss: 0.198079\n",
            "## Meter  ## current loss: 0.395960 [  640/ 1601]\n",
            "## Update ## best_state with loss: 0.197299\n",
            "## Update ## best_state with loss: 0.177785\n",
            "## Meter  ## current loss: 0.177785 [ 1280/ 1601]\n",
            "## Update ## best_state with loss: 0.166628\n",
            "Test Error: \n",
            " Accuracy: 80.2%, Avg loss: 0.642035 \n",
            "\n",
            "Epoch 6\n",
            "------------------\n",
            "## Update ## best_state with loss: 0.347944\n",
            "## Meter  ## current loss: 0.347944 [    0/ 1601]\n",
            "## Update ## best_state with loss: 0.258135\n",
            "## Update ## best_state with loss: 0.227186\n",
            "## Update ## best_state with loss: 0.182706\n",
            "## Update ## best_state with loss: 0.149975\n",
            "## Update ## best_state with loss: 0.132730\n",
            "## Update ## best_state with loss: 0.131657\n",
            "## Meter  ## current loss: 0.131657 [  640/ 1601]\n",
            "## Meter  ## current loss: 0.268609 [ 1280/ 1601]\n",
            "Test Error: \n",
            " Accuracy: 75.7%, Avg loss: 0.879574 \n",
            "\n",
            "Epoch 7\n",
            "------------------\n",
            "## Update ## best_state with loss: 0.342057\n",
            "## Meter  ## current loss: 0.342057 [    0/ 1601]\n",
            "## Update ## best_state with loss: 0.256357\n",
            "## Update ## best_state with loss: 0.238749\n",
            "## Update ## best_state with loss: 0.189555\n",
            "## Update ## best_state with loss: 0.108050\n",
            "## Meter  ## current loss: 0.546722 [  640/ 1601]\n",
            "## Meter  ## current loss: 0.225430 [ 1280/ 1601]\n",
            "Test Error: \n",
            " Accuracy: 80.5%, Avg loss: 0.729693 \n",
            "\n",
            "## No Improvement on Test Set, Stopping ##\n",
            "Done\n"
          ]
        }
      ],
      "source": [
        "train_dataloader, test_dataloader = get_dataloader(real_life_dataset, config['batch_size'])\n",
        "\n",
        "model = DANN(len(real_life_dataset.classes)).to(device)\n",
        "\n",
        "# Training\n",
        "training(model, train_dataloader, test_dataloader, config, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7HMoVx_85eur",
      "metadata": {
        "id": "7HMoVx_85eur"
      },
      "source": [
        "#### 3.2.2 Testing on Product\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "shPERUI05drr",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "shPERUI05drr",
        "outputId": "6c8d2245-9c6b-4590-8e43-02f50fd11939"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Error: \n",
            " Accuracy: 79.2%, Avg loss: 0.701823 \n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.701822874834761, 0.792)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "loader_target_dataset = DataLoader(product_dataset, batch_size=config['batch_size'], shuffle=False)\n",
        "\n",
        "# model.load_state_dict(torch.load('model_state.pt', map_location='cpu'))\n",
        "test_loop(loader_target_dataset, model, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "6aT-zlOq862D",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6aT-zlOq862D",
        "outputId": "a46f0ee3-21ed-4d44-e264-8bdff2e19fb7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "513021440\n"
          ]
        }
      ],
      "source": [
        "del train_dataloader, test_dataloader, loader_target_dataset\n",
        "del model\n",
        "print(torch.cuda.memory_allocated())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7q7jwnwwhRz2",
      "metadata": {
        "id": "7q7jwnwwhRz2"
      },
      "source": [
        "## 4: Define UDA functions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "425ysNsFjUy-",
      "metadata": {
        "id": "425ysNsFjUy-"
      },
      "source": [
        "### 4.1 Adversarial Discriminator Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "81ccf625",
      "metadata": {
        "id": "81ccf625"
      },
      "outputs": [],
      "source": [
        "def get_discriminator_loss(source_pred, target_pred): \n",
        "    domain_pred = torch.cat((source_pred, target_pred),dim=0).cuda()\n",
        "    #print(domain_pred.shape) # [128,1024]\n",
        "    source_truth = torch.zeros(len(source_pred))\n",
        "    target_truth = torch.ones(len(target_pred))\n",
        "    domain_truth = torch.cat((source_truth, target_truth),dim=0).cuda()\n",
        "    #print(domain_truth.shape) # [128]\n",
        "\n",
        "    domain_loss = domain_truth*torch.log(1/domain_pred)+(1-domain_truth)*torch.log(1/(1-domain_pred))\n",
        "    domain_loss = domain_loss.mean()\n",
        "\n",
        "    return domain_loss "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "BBaNX5GgjK7F",
      "metadata": {
        "id": "BBaNX5GgjK7F"
      },
      "source": [
        "### 4.2 Adversarial optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "zKscrnYohp7k",
      "metadata": {
        "id": "zKscrnYohp7k"
      },
      "outputs": [],
      "source": [
        "def get_adversarial_optimizer(model, config, progress, adversarial=True):\n",
        "  '''\n",
        "  Get Adversarial Optimizers\n",
        "  '''\n",
        "  lr, wd, momtm = config['lr'], config['wd'], config['momentum']\n",
        "  lr = lr / ((1 + config['alpha']*progress)**config['beta'])\n",
        "\n",
        "  feature_ext   = model.get_submodule(\"feature_extractor\")\n",
        "  classifier    = model.get_submodule(\"classifier\")\n",
        "  discriminator = model.get_submodule(\"discriminator\")\n",
        "\n",
        "  pre_trained_weights   = feature_ext.parameters()\n",
        "  classifier_weights    = classifier.parameters()\n",
        "  discriminator_weights = discriminator.parameters()\n",
        "\n",
        "  feature_optim       = torch.optim.SGD([{'params': pre_trained_weights}],     lr=lr/10, weight_decay=wd, momentum=momtm)\n",
        "  classifier_optim    = torch.optim.SGD([{'params': classifier_weights}],      lr=lr,    weight_decay=wd, momentum=momtm)\n",
        "  discriminator_optim = torch.optim.SGD([{'params': discriminator_weights}],   lr=lr,    weight_decay=wd, momentum=momtm)\n",
        "  \n",
        "  return feature_optim, classifier_optim, discriminator_optim "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hRbDuyfU929u",
      "metadata": {
        "id": "hRbDuyfU929u"
      },
      "source": [
        "### 4.3 Adversarial Train Loop\n",
        "\n",
        "Setting the **domain adaptation parameter** according to the original [paper](https://arxiv.org/pdf/1505.07818.pdf) section 5.2.2\n",
        "\n",
        "$$ \\lambda_p = \\frac{2}{1 + exp(-\\gamma \\cdot p)} - 1 $$\n",
        "\n",
        "where p is the training progress linearly changing from 0 to 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "1SQHSVPE93hi",
      "metadata": {
        "id": "1SQHSVPE93hi"
      },
      "outputs": [],
      "source": [
        "def adversarial_train_loop(source_loader, target_loader, model, config, progress, device):\n",
        "  \"\"\"\n",
        "  return:\n",
        "    @best_state\n",
        "    @best_loss\n",
        "  \"\"\"\n",
        "  size = len(source_loader.dataset)\n",
        "  \n",
        "  # cross entropy loss\n",
        "  classification_loss = get_class_loss_func()\n",
        "\n",
        "  # Get three optimizer\n",
        "  feature_optim, class_optim, discriminator_optim = get_adversarial_optimizer(model, config, progress)\n",
        "\n",
        "  # Target data loader iterator\n",
        "  iter_target = iter(target_loader)\n",
        "\n",
        "  domain_adapt = 2 / (1 + math.exp(-config['gamma']*progress)) - 1\n",
        "\n",
        "  best_loss, best_state = float('inf'), None\n",
        "\n",
        "  for batch, (X_source, y_source) in enumerate(source_loader):\n",
        "    try:\n",
        "      X_target, _ = next(iter_target)\n",
        "    except:\n",
        "      iter_target = iter(target_loader)\n",
        "      X_target, _ = next(iter_target)  \n",
        "\n",
        "    # Some internal bug return nested tesnor with size 1\n",
        "    if len(X_source) < 64:\n",
        "      continue\n",
        "\n",
        "    X_source, y_source, X_target = X_source.to(device), y_source.to(device), X_target.to(device)\n",
        "\n",
        "    class_pred_source, domain_pred_source = model(X_source)\n",
        "    _,                 domain_pred_target = model(X_target)\n",
        "\n",
        "    class_loss   = classification_loss(class_pred_source, y_source)\n",
        "    discrim_loss = get_discriminator_loss(domain_pred_source, domain_pred_target)\n",
        "\n",
        "    feature_optim.zero_grad()\n",
        "\n",
        "    # Update discriminator\n",
        "    discriminator_optim.zero_grad()\n",
        "    discrim_loss.backward(retain_graph=True)\n",
        "    discriminator_optim.step()\n",
        "\n",
        "    # Update classifier\n",
        "    class_optim.zero_grad()\n",
        "    class_loss.backward(retain_graph=True)\n",
        "    class_optim.step()\n",
        "\n",
        "    # Update feature extractor\n",
        "    feature_optim.step()  \n",
        "\n",
        "    # Total loss\n",
        "    total_loss = class_loss - domain_adapt * discrim_loss \n",
        "\n",
        "    if total_loss < best_loss:\n",
        "      best_loss = total_loss\n",
        "      best_state = model.state_dict()\n",
        "      print(f\"## Update ## best_state updated with loss: {total_loss:>7f}\")\n",
        "\n",
        "    if batch % 10 == 0:\n",
        "      class_loss, discrim_loss, current = class_loss.item(), discrim_loss.item(), batch * len(X_source)\n",
        "      total_loss = total_loss.item()\n",
        "      print(f\"## Meter  ## [{current:>5d}/{size:>5d}]\")\n",
        "      # print(f\"## Meter  ## classification loss: {class_loss:>7f} discrim loss: {discrim_loss:>7f} total loss: {total_loss:>7f}[{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "    del class_loss, discrim_loss \n",
        "    del X_source, y_source, X_target, class_pred_source, domain_pred_source, domain_pred_target\n",
        "  \n",
        "  return best_state, best_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1jHD6IJ6H8vF",
      "metadata": {
        "id": "1jHD6IJ6H8vF"
      },
      "source": [
        "### 4.4 Adversarial Test Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "KyhdZbs3H9Ue",
      "metadata": {
        "id": "KyhdZbs3H9Ue"
      },
      "outputs": [],
      "source": [
        "def adversarial_test_loop(dataloader, model, device, name=\"\"):\n",
        "  test_loss, correct = 0, 0\n",
        "\n",
        "  class_loss_func = get_class_loss_func()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for X, y in dataloader:\n",
        "      X, y = X.to(device), y.to(device)\n",
        "      class_pred, _ = model(X)\n",
        "\n",
        "      test_loss += class_loss_func(class_pred, y).item()\n",
        "      correct += (class_pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "  size = len(dataloader.dataset)\n",
        "  num_batches = len(dataloader)\n",
        "\n",
        "  test_loss /= num_batches\n",
        "  correct /= size\n",
        "  print(f\"{name} Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "\n",
        "  return test_loss, correct"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tyopBjWr-FAs",
      "metadata": {
        "id": "tyopBjWr-FAs"
      },
      "source": [
        "### 4.5 Adversarial Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "dBCrfNPj-Bxa",
      "metadata": {
        "id": "dBCrfNPj-Bxa"
      },
      "outputs": [],
      "source": [
        "def adversarial_training(model, source_loader, source_test_loader, target_loader, config, device):\n",
        "  # print(f\"Learning_rate {config['lr']}, weight_decay {config['wd']}\")\n",
        "  best_loss, best_state = float('inf'), None\n",
        "  no_improve_count = 0\n",
        "\n",
        "  for epoch in range(config['epochs']):\n",
        "    print(f\"Epoch {epoch+1}\\n------------------\")\n",
        "    progress = epoch/config['epochs']\n",
        "\n",
        "    curr_state, _ = adversarial_train_loop(source_loader, target_loader, model, config, progress, device)\n",
        "\n",
        "    # Load the best state\n",
        "    model.load_state_dict(curr_state)\n",
        "\n",
        "    source_loss, _ = adversarial_test_loop(source_test_loader, model, device, \"Source Test\")\n",
        "    target_loss, _ = adversarial_test_loop(target_loader, model, device, \"Target Train\")\n",
        "\n",
        "    if target_loss < best_loss:\n",
        "      no_improve_count = 0\n",
        "\n",
        "      best_loss = target_loss\n",
        "      best_state = curr_state\n",
        "    else:\n",
        "      no_improve_count += 1\n",
        "\n",
        "    if no_improve_count > 2:\n",
        "      print(f\"## No Improvement on Target Set, Stopping ##\")\n",
        "      break\n",
        "\n",
        "  model.load_state_dict(best_state)\n",
        "  print(\"Done\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "x4ePILfZ4jOA",
      "metadata": {
        "id": "x4ePILfZ4jOA"
      },
      "source": [
        "## 5 Training with UDA Techniques"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "wt-IOOThNShC",
      "metadata": {
        "id": "wt-IOOThNShC"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "rcLFqgOelDsW",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rcLFqgOelDsW",
        "outputId": "dddac16d-387f-4e61-dcf0-2b8a7e9480c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
            "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ],
      "source": [
        "adv_model = DANN(len(product_dataset.classes)).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.1 Product -> Real Life"
      ],
      "metadata": {
        "id": "H7QPRLosqltl"
      },
      "id": "H7QPRLosqltl"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5.1.1 Training on Product"
      ],
      "metadata": {
        "id": "VKR4440ZqxJV"
      },
      "id": "VKR4440ZqxJV"
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "IcUMoe8x_qrC",
      "metadata": {
        "id": "IcUMoe8x_qrC"
      },
      "outputs": [],
      "source": [
        "train_dataloader, train_test_dataloader = get_dataloader(product_dataset, config['batch_size'])\n",
        "target_dataloader, target_test_dataloader = get_dataloader(real_life_dataset, config['batch_size'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "ZgN_WIqqPtjk",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZgN_WIqqPtjk",
        "outputId": "b1debfa8-1764-4022-f3a3-eda013044dc6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "------------------\n",
            "## Update ## best_state updated with loss: 3.002315\n",
            "## Meter  ## [    0/ 1601]\n",
            "## Update ## best_state updated with loss: 2.955360\n",
            "## Update ## best_state updated with loss: 2.949759\n",
            "## Update ## best_state updated with loss: 2.913088\n",
            "## Update ## best_state updated with loss: 2.848958\n",
            "## Update ## best_state updated with loss: 2.832256\n",
            "## Update ## best_state updated with loss: 2.697971\n",
            "## Update ## best_state updated with loss: 2.648842\n",
            "## Update ## best_state updated with loss: 2.480603\n",
            "## Update ## best_state updated with loss: 2.402117\n",
            "## Update ## best_state updated with loss: 2.282860\n",
            "## Meter  ## [  640/ 1601]\n",
            "## Update ## best_state updated with loss: 2.073895\n",
            "## Update ## best_state updated with loss: 1.776182\n",
            "## Update ## best_state updated with loss: 1.662053\n",
            "## Update ## best_state updated with loss: 1.512952\n",
            "## Update ## best_state updated with loss: 1.194102\n",
            "## Update ## best_state updated with loss: 1.004736\n",
            "## Update ## best_state updated with loss: 0.836270\n",
            "## Update ## best_state updated with loss: 0.605750\n",
            "## Update ## best_state updated with loss: 0.527074\n",
            "## Update ## best_state updated with loss: 0.386209\n",
            "## Meter  ## [ 1280/ 1601]\n",
            "Source Test Test Error: \n",
            " Accuracy: 79.4%, Avg loss: 1.235822 \n",
            "\n",
            "Target Train Test Error: \n",
            " Accuracy: 55.3%, Avg loss: 1.720054 \n",
            "\n",
            "Epoch 2\n",
            "------------------\n",
            "## Update ## best_state updated with loss: 0.552132\n",
            "## Meter  ## [    0/ 1601]\n",
            "## Update ## best_state updated with loss: 0.011415\n",
            "## Update ## best_state updated with loss: -0.031075\n",
            "## Meter  ## [  640/ 1601]\n",
            "## Update ## best_state updated with loss: -0.082262\n",
            "## Update ## best_state updated with loss: -0.155779\n",
            "## Meter  ## [ 1280/ 1601]\n",
            "Source Test Test Error: \n",
            " Accuracy: 90.0%, Avg loss: 0.383568 \n",
            "\n",
            "Target Train Test Error: \n",
            " Accuracy: 61.6%, Avg loss: 1.440411 \n",
            "\n",
            "Epoch 3\n",
            "------------------\n",
            "## Update ## best_state updated with loss: -0.294047\n",
            "## Meter  ## [    0/ 1601]\n",
            "## Update ## best_state updated with loss: -0.430490\n",
            "## Meter  ## [  640/ 1601]\n",
            "## Meter  ## [ 1280/ 1601]\n",
            "Source Test Test Error: \n",
            " Accuracy: 89.0%, Avg loss: 0.347641 \n",
            "\n",
            "Target Train Test Error: \n",
            " Accuracy: 62.3%, Avg loss: 1.322451 \n",
            "\n",
            "Epoch 4\n",
            "------------------\n",
            "## Update ## best_state updated with loss: -0.519333\n",
            "## Meter  ## [    0/ 1601]\n",
            "## Update ## best_state updated with loss: -0.544083\n",
            "## Update ## best_state updated with loss: -0.560671\n",
            "## Update ## best_state updated with loss: -0.600769\n",
            "## Meter  ## [  640/ 1601]\n",
            "## Meter  ## [ 1280/ 1601]\n",
            "Source Test Test Error: \n",
            " Accuracy: 90.5%, Avg loss: 0.292231 \n",
            "\n",
            "Target Train Test Error: \n",
            " Accuracy: 61.9%, Avg loss: 1.246843 \n",
            "\n",
            "Epoch 5\n",
            "------------------\n",
            "## Update ## best_state updated with loss: -0.539928\n",
            "## Meter  ## [    0/ 1601]\n",
            "## Update ## best_state updated with loss: -0.615555\n",
            "## Update ## best_state updated with loss: -0.630202\n",
            "## Meter  ## [  640/ 1601]\n",
            "## Meter  ## [ 1280/ 1601]\n",
            "Source Test Test Error: \n",
            " Accuracy: 92.2%, Avg loss: 0.278961 \n",
            "\n",
            "Target Train Test Error: \n",
            " Accuracy: 62.4%, Avg loss: 1.318595 \n",
            "\n",
            "Epoch 6\n",
            "------------------\n",
            "## Update ## best_state updated with loss: -0.633465\n",
            "## Meter  ## [    0/ 1601]\n",
            "## Update ## best_state updated with loss: -0.636357\n",
            "## Update ## best_state updated with loss: -0.641429\n",
            "## Update ## best_state updated with loss: -0.657104\n",
            "## Meter  ## [  640/ 1601]\n",
            "## Meter  ## [ 1280/ 1601]\n",
            "Source Test Test Error: \n",
            " Accuracy: 90.7%, Avg loss: 0.280563 \n",
            "\n",
            "Target Train Test Error: \n",
            " Accuracy: 63.8%, Avg loss: 1.211971 \n",
            "\n",
            "Epoch 7\n",
            "------------------\n",
            "## Update ## best_state updated with loss: -0.661455\n",
            "## Meter  ## [    0/ 1601]\n",
            "## Update ## best_state updated with loss: -0.667902\n",
            "## Update ## best_state updated with loss: -0.673126\n",
            "## Update ## best_state updated with loss: -0.677583\n",
            "## Meter  ## [  640/ 1601]\n",
            "## Meter  ## [ 1280/ 1601]\n",
            "Source Test Test Error: \n",
            " Accuracy: 91.0%, Avg loss: 0.274498 \n",
            "\n",
            "Target Train Test Error: \n",
            " Accuracy: 62.7%, Avg loss: 1.202927 \n",
            "\n",
            "Epoch 8\n",
            "------------------\n",
            "## Update ## best_state updated with loss: -0.591984\n",
            "## Meter  ## [    0/ 1601]\n",
            "## Update ## best_state updated with loss: -0.620525\n",
            "## Update ## best_state updated with loss: -0.653444\n",
            "## Update ## best_state updated with loss: -0.663386\n",
            "## Update ## best_state updated with loss: -0.673358\n",
            "## Update ## best_state updated with loss: -0.678938\n",
            "## Meter  ## [  640/ 1601]\n",
            "## Update ## best_state updated with loss: -0.680828\n",
            "## Meter  ## [ 1280/ 1601]\n",
            "Source Test Test Error: \n",
            " Accuracy: 91.0%, Avg loss: 0.276695 \n",
            "\n",
            "Target Train Test Error: \n",
            " Accuracy: 62.7%, Avg loss: 1.198557 \n",
            "\n",
            "Epoch 9\n",
            "------------------\n",
            "## Update ## best_state updated with loss: -0.572211\n",
            "## Meter  ## [    0/ 1601]\n",
            "## Update ## best_state updated with loss: -0.658365\n",
            "## Update ## best_state updated with loss: -0.671915\n",
            "## Update ## best_state updated with loss: -0.684280\n",
            "## Meter  ## [  640/ 1601]\n",
            "## Meter  ## [ 1280/ 1601]\n",
            "Source Test Test Error: \n",
            " Accuracy: 89.2%, Avg loss: 0.378557 \n",
            "\n",
            "Target Train Test Error: \n",
            " Accuracy: 62.6%, Avg loss: 1.181171 \n",
            "\n",
            "Epoch 10\n",
            "------------------\n",
            "## Update ## best_state updated with loss: -0.644570\n",
            "## Meter  ## [    0/ 1601]\n",
            "## Update ## best_state updated with loss: -0.648931\n",
            "## Update ## best_state updated with loss: -0.656694\n",
            "## Update ## best_state updated with loss: -0.666282\n",
            "## Update ## best_state updated with loss: -0.675295\n",
            "## Meter  ## [  640/ 1601]\n",
            "## Update ## best_state updated with loss: -0.675571\n",
            "## Update ## best_state updated with loss: -0.677077\n",
            "## Update ## best_state updated with loss: -0.683164\n",
            "## Meter  ## [ 1280/ 1601]\n",
            "Source Test Test Error: \n",
            " Accuracy: 91.0%, Avg loss: 0.305980 \n",
            "\n",
            "Target Train Test Error: \n",
            " Accuracy: 63.1%, Avg loss: 1.197999 \n",
            "\n",
            "Done\n"
          ]
        }
      ],
      "source": [
        "torch.autograd.set_detect_anomaly(True)\n",
        "adversarial_training(adv_model, train_dataloader, train_test_dataloader, target_dataloader, config, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5.1.2 Testing on Real Life"
      ],
      "metadata": {
        "id": "1s0Z_xXXq5gO"
      },
      "id": "1s0Z_xXXq5gO"
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "oUUo7mSiJasc",
      "metadata": {
        "id": "oUUo7mSiJasc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96496ef4-72f2-4536-a317-c2696425cdc1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Error: \n",
            " Accuracy: 64.6%, Avg loss: 1.243085 \n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1.2430853825062513, 0.646)"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "loader_target_dataset = DataLoader(real_life_dataset, batch_size=config['batch_size'], shuffle=False)\n",
        "\n",
        "test_loop(loader_target_dataset, adv_model, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "FNofveL_-8gX",
      "metadata": {
        "id": "FNofveL_-8gX"
      },
      "source": [
        "### 5.2 Real Life -> Product"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5.2.1 Training on Real Life"
      ],
      "metadata": {
        "id": "6YxXx0Lzresh"
      },
      "id": "6YxXx0Lzresh"
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "enMKc_Em_tWq",
      "metadata": {
        "id": "enMKc_Em_tWq"
      },
      "outputs": [],
      "source": [
        "train_dataloader, train_test_dataloader = get_dataloader(real_life_dataset, config['batch_size'])\n",
        "target_dataloader, target_test_dataloader = get_dataloader(product_dataset, config['batch_size'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "Msifip2P-7UB",
      "metadata": {
        "id": "Msifip2P-7UB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88c778e7-5731-4728-825c-433ebfcd285e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "------------------\n",
            "## Update ## best_state updated with loss: 1.547083\n",
            "## Meter  ## [    0/ 1601]\n",
            "## Update ## best_state updated with loss: 1.282004\n",
            "## Update ## best_state updated with loss: 1.178914\n",
            "## Update ## best_state updated with loss: 0.928878\n",
            "## Meter  ## [  640/ 1601]\n",
            "## Update ## best_state updated with loss: 0.660247\n",
            "## Meter  ## [ 1280/ 1601]\n",
            "## Update ## best_state updated with loss: 0.587790\n",
            "Source Test Test Error: \n",
            " Accuracy: 74.9%, Avg loss: 0.872657 \n",
            "\n",
            "Target Train Test Error: \n",
            " Accuracy: 91.0%, Avg loss: 0.274368 \n",
            "\n",
            "Epoch 2\n",
            "------------------\n",
            "## Update ## best_state updated with loss: 0.276027\n",
            "## Meter  ## [    0/ 1601]\n",
            "## Update ## best_state updated with loss: -0.007978\n",
            "## Meter  ## [  640/ 1601]\n",
            "## Update ## best_state updated with loss: -0.037597\n",
            "## Meter  ## [ 1280/ 1601]\n",
            "Source Test Test Error: \n",
            " Accuracy: 76.7%, Avg loss: 0.834816 \n",
            "\n",
            "Target Train Test Error: \n",
            " Accuracy: 90.9%, Avg loss: 0.280146 \n",
            "\n",
            "Epoch 3\n",
            "------------------\n",
            "## Update ## best_state updated with loss: -0.260266\n",
            "## Meter  ## [    0/ 1601]\n",
            "## Update ## best_state updated with loss: -0.313592\n",
            "## Update ## best_state updated with loss: -0.333087\n",
            "## Update ## best_state updated with loss: -0.359026\n",
            "## Meter  ## [  640/ 1601]\n",
            "## Meter  ## [ 1280/ 1601]\n",
            "Source Test Test Error: \n",
            " Accuracy: 76.7%, Avg loss: 0.811308 \n",
            "\n",
            "Target Train Test Error: \n",
            " Accuracy: 92.0%, Avg loss: 0.270222 \n",
            "\n",
            "Epoch 4\n",
            "------------------\n",
            "## Update ## best_state updated with loss: -0.454041\n",
            "## Meter  ## [    0/ 1601]\n",
            "## Update ## best_state updated with loss: -0.524551\n",
            "## Meter  ## [  640/ 1601]\n",
            "## Meter  ## [ 1280/ 1601]\n",
            "Source Test Test Error: \n",
            " Accuracy: 77.4%, Avg loss: 0.822078 \n",
            "\n",
            "Target Train Test Error: \n",
            " Accuracy: 91.9%, Avg loss: 0.274843 \n",
            "\n",
            "Epoch 5\n",
            "------------------\n",
            "## Update ## best_state updated with loss: -0.569319\n",
            "## Meter  ## [    0/ 1601]\n",
            "## Update ## best_state updated with loss: -0.583558\n",
            "## Update ## best_state updated with loss: -0.608510\n",
            "## Meter  ## [  640/ 1601]\n",
            "## Meter  ## [ 1280/ 1601]\n",
            "Source Test Test Error: \n",
            " Accuracy: 78.2%, Avg loss: 0.846608 \n",
            "\n",
            "Target Train Test Error: \n",
            " Accuracy: 91.0%, Avg loss: 0.325148 \n",
            "\n",
            "Epoch 6\n",
            "------------------\n",
            "## Update ## best_state updated with loss: -0.593676\n",
            "## Meter  ## [    0/ 1601]\n",
            "## Update ## best_state updated with loss: -0.605756\n",
            "## Update ## best_state updated with loss: -0.608573\n",
            "## Meter  ## [  640/ 1601]\n",
            "## Update ## best_state updated with loss: -0.614502\n",
            "## Meter  ## [ 1280/ 1601]\n",
            "Source Test Test Error: \n",
            " Accuracy: 77.7%, Avg loss: 0.820517 \n",
            "\n",
            "Target Train Test Error: \n",
            " Accuracy: 91.8%, Avg loss: 0.351770 \n",
            "\n",
            "## No Improvement on Target Set, Stopping ##\n",
            "Done\n"
          ]
        }
      ],
      "source": [
        "# Training\n",
        "adversarial_training(adv_model, train_dataloader, train_test_dataloader, target_dataloader, config, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "nSh7gaC8JaVC",
      "metadata": {
        "id": "nSh7gaC8JaVC"
      },
      "source": [
        "#### 5.2.2 Testing on Product"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "DfSRjr84-8DF",
      "metadata": {
        "id": "DfSRjr84-8DF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0eb9ad5-ff01-46db-ef1e-005451a34462"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Error: \n",
            " Accuracy: 91.9%, Avg loss: 0.313109 \n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.31310947152087465, 0.919)"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "loader_target_dataset = DataLoader(product_dataset, batch_size=config['batch_size'], shuffle=False)\n",
        "\n",
        "test_loop(loader_target_dataset, adv_model, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "bFtADV4AtN1Z",
      "metadata": {
        "id": "bFtADV4AtN1Z"
      },
      "outputs": [],
      "source": [
        "# del source_dataset, train_dataloader, test_dataloader, target_dataset, loader_target_dataset\n",
        "del adv_model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6 Summary"
      ],
      "metadata": {
        "id": "YOKgE6LQuqHC"
      },
      "id": "YOKgE6LQuqHC"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.1 Product -> Real Life\n"
      ],
      "metadata": {
        "id": "S41pMBv6uy9b"
      },
      "id": "S41pMBv6uy9b"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6.1.1 Purely training on Source Domain-Product\n",
        "\n",
        "Without using any domain adaptation techniques, within 10 epochs training, the classifier network achieves  64.6% accuracy on the target domain."
      ],
      "metadata": {
        "id": "bkflD8rau94r"
      },
      "id": "bkflD8rau94r"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6.1.2 Training on both Source and Target Domains\n",
        "\n",
        "Using DANN doamin adaptation technique, the classifier with feature extractor trained on both source and target domain achives 64.6% accuracy, which is the same as the one trained solely on the source domain, with no improvement on accuracy."
      ],
      "metadata": {
        "id": "vxoGKmCJvHqD"
      },
      "id": "vxoGKmCJvHqD"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.2 Real Life -> Product"
      ],
      "metadata": {
        "id": "QTVdi9zvuysa"
      },
      "id": "QTVdi9zvuysa"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6.2.1 Purely Training on Source Domain-Real Life\n",
        "\n",
        "The classifier trained solely on the source domain achives 79.2% accuracy on the target domain."
      ],
      "metadata": {
        "id": "_rV09wirvMcM"
      },
      "id": "_rV09wirvMcM"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6.2.2 Training on both Source and Target Domains\n",
        "\n",
        "With feature extractor trained on both source and target domain, the classifer achives 91.9% accuracy on the target domain, which is about 12% improvement over feature extractor purely trained on source domain."
      ],
      "metadata": {
        "id": "VShk9r-GvM2U"
      },
      "id": "VShk9r-GvM2U"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "UDA_GAN.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}