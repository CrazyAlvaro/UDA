{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "UOmUNvGYgPRN",
      "metadata": {
        "id": "UOmUNvGYgPRN"
      },
      "source": [
        "# Unsupervised Domain Adaptation Project\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ExU_wmMAgVsR",
      "metadata": {
        "id": "ExU_wmMAgVsR"
      },
      "source": [
        "## 1: Data download\n",
        "Load data to project from Google Drive. Copy a subset of classes of images to the path:\n",
        "- `adaptiope_small/product_images`\n",
        "- `adaptiope_small/real_life` \n",
        "\n",
        "two directories. They represent images from two different domain **product** and **real_life**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "4134f6cf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4134f6cf",
        "outputId": "a16e5fda-1438-4d04-b720-19a46f9db869"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from os import makedirs, listdir\n",
        "from tqdm import tqdm\n",
        "from google.colab import drive\n",
        "from os.path import join\n",
        "from shutil import copytree\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "!mkdir dataset\n",
        "!cp \"gdrive/My Drive/Colab Notebooks/data/Adaptiope.zip\" dataset/\n",
        "# !ls dataset\n",
        "\n",
        "!unzip -qq dataset/Adaptiope.zip   # unzip file\n",
        "\n",
        "!rm -rf dataset/Adaptiope.zip \n",
        "!rm -rf adaptiope_small"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "0a6cac67",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0a6cac67",
        "outputId": "e84f795e-0407-4d15-f233-c0bfeab8ee51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['tape dispenser', 'comb', 'car jack', 'toothbrush', 'handcuffs', 'magic lamp', 'usb stick', 'glasses', 'tank', 'computer', 'rubber boat', 'shower head', 'telescope', 'quadcopter', 'scooter', 'tyrannosaurus', 'rc car', 'syringe', 'cordless fixed phone', 'bottle', 'vr goggles', 'razor', 'helicopter', 'hat', 'dart', 'compass', 'hoverboard', 'corkscrew', 'projector', 'file cabinet', 'smoking pipe', 'rifle', 'mug', 'fan', 'sewing machine', 'pen', 'keyboard', 'knife', 'trash can', 'tent', 'drum set', 'nail clipper', 'phonograph', 'monitor', 'toilet brush', 'skateboard', 'electric guitar', 'screwdriver', 'coat hanger', 'speakers', 'boxing gloves', 'roller skates', 'computer mouse', 'ladder', 'motorbike helmet', 'scissors', 'handgun', 'power strip', 'ruler', 'microwave', 'golf club', 'stapler', 'watering can', 'over-ear headphones', 'umbrella', 'pipe wrench', 'vacuum cleaner', 'purse', 'in-ear headphones', 'webcam', 'pikachu', 'letter tray', 'chainsaw', 'ice cube tray', 'fighter jet', 'grill', 'power drill', 'sleeping bag', 'crown', 'game controller', 'calculator', 'bookcase', 'pogo stick', 'printer', 'wristwatch', 'baseball bat', 'wallet', 'sword', 'lawn mower', 'notepad', 'fire extinguisher', 'hair dryer', 'ring binder', 'stroller', 'diving fins', 'network switch', 'stethoscope', 'snow shovel', 'spatula', 'wheelchair', 'electric shaver', 'smartphone', 'desk lamp', 'puncher', 'hot glue gun', 'stand mixer', 'laptop', 'cellphone', 'ice skates', 'hand mixer', 'axe', 'acoustic guitar', 'office chair', 'flat iron', 'binoculars', 'skeleton', 'brachiosaurus', 'mixing console', 'backpack', 'bicycle helmet', 'bicycle', 'hard-wired fixed phone', 'hourglass']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20/20 [00:01<00:00, 15.10it/s]\n",
            "100%|██████████| 20/20 [00:00<00:00, 32.13it/s]\n"
          ]
        }
      ],
      "source": [
        "!mkdir adaptiope_small\n",
        "classes = listdir(\"Adaptiope/product_images\")\n",
        "print(classes)\n",
        "classes = [\"backpack\", \"bookcase\", \"car jack\", \"comb\", \"crown\", \"file cabinet\", \"flat iron\", \"game controller\", \"glasses\",\n",
        "           \"helicopter\", \"ice skates\", \"letter tray\", \"monitor\", \"mug\", \"network switch\", \"over-ear headphones\", \"pen\",\n",
        "           \"purse\", \"stand mixer\", \"stroller\"]\n",
        "domain_classes = [\"product_images\", \"real_life\"]\n",
        "for d, td in zip([\"Adaptiope/product_images\", \"Adaptiope/real_life\"], [\"adaptiope_small/product_images\", \"adaptiope_small/real_life\"]):\n",
        "  makedirs(td)\n",
        "  for c in tqdm(classes):\n",
        "    c_path = join(d, c)\n",
        "    c_target = join(td, c)\n",
        "    copytree(c_path, c_target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "S8yOthKskBmC",
      "metadata": {
        "id": "S8yOthKskBmC"
      },
      "outputs": [],
      "source": [
        "product_path = 'adaptiope_small/product_images'\n",
        "real_life_path = 'adaptiope_small/real_life'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "uUHNozN6ggRr",
      "metadata": {
        "id": "uUHNozN6ggRr"
      },
      "source": [
        "## 2: Domain-Adversarial training of Neural Network \n",
        "\n",
        "We implement DANN UDA method [DANN](https://arxiv.org/pdf/1505.07818.pdf)  \n",
        "\n",
        "![DANN.png](https://raw.githubusercontent.com/CrazyAlvaro/UDA/main/images/DANN.png)\n",
        "\n",
        "As displayed in the model architecture above, DANN is consist of three component: feature extractor, domain classifier, and label predictor(classifier).\n",
        "\n",
        "While in order to adversarial training from both label predictor and domain classifier, a gradient reversal layer(GRL) is added."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fwiJOBXWpeS0",
      "metadata": {
        "id": "fwiJOBXWpeS0"
      },
      "source": [
        "### 2.0: Import Libraries and Data Loading\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "_GZxbLlT6O8m",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_GZxbLlT6O8m",
        "outputId": "b1634904-3012-47be-d9c2-7d7472874b93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image size:  (679, 679)\n"
          ]
        }
      ],
      "source": [
        "from PIL import Image\n",
        "from os.path import join\n",
        "import math\n",
        "\n",
        "img = Image.open(join(product_path, 'backpack', 'backpack_003.jpg'))\n",
        "print('Image size: ', img.size)\n",
        "#img"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "RE2WUwy9BORV",
      "metadata": {
        "id": "RE2WUwy9BORV"
      },
      "source": [
        "import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "Vei6SzEeggzU",
      "metadata": {
        "id": "Vei6SzEeggzU"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import softmax\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision.models import vgg11, alexnet \n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision.transforms.transforms import ToTensor"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "M5rt9x7nBKiT",
      "metadata": {
        "id": "M5rt9x7nBKiT"
      },
      "source": [
        "configuration constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "JXznFSNkAzn3",
      "metadata": {
        "id": "JXznFSNkAzn3"
      },
      "outputs": [],
      "source": [
        "img_size = 256\n",
        "# mean, std used by pre-trained models from PyTorch\n",
        "mean, std = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n",
        "config = dict(epochs=10, batch_size=64,lr=0.01, wd=0.001, momentum=0.9, alpha=10, beta=0.75, gamma=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_Rz4CI9spEkN",
      "metadata": {
        "id": "_Rz4CI9spEkN"
      },
      "source": [
        "Configue GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "nHv2o65FpDpn",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nHv2o65FpDpn",
        "outputId": "8f0d9228-170f-476e-c90a-d49322735eba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "GF3YyTBAhJk8",
      "metadata": {
        "id": "GF3YyTBAhJk8"
      },
      "outputs": [],
      "source": [
        "def get_dataset(root_path):\n",
        "  '''\n",
        "    Get dataset from specific data path\n",
        "\n",
        "    # parameters:\n",
        "        root_path: path to image folder\n",
        "\n",
        "    # return: train_loader, test_loader\n",
        "  '''\n",
        "  # Construct image transform\n",
        "  image_transform = transforms.Compose([\n",
        "    transforms.Resize(img_size),\n",
        "    transforms.CenterCrop(img_size),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std)\n",
        "  ])\n",
        "\n",
        "  # Load data from filesystem\n",
        "  image_dataset = ImageFolder(root_path, transform=image_transform)\n",
        "\n",
        "  return image_dataset\n",
        "\n",
        "def get_dataloader(dataset, batch_size, shuffle_train=True, shuffle_test=False):\n",
        "  '''\n",
        "    Get DataLoader from specific data path\n",
        "\n",
        "    # parameters:\n",
        "        dataset: ImageFolder instance\n",
        "        batch_size: batch_size for DataLoader\n",
        "        shuffle_train: whether to shuffle training data\n",
        "        shuffle_test: whether to shuffle test data\n",
        "  '''\n",
        "  # Get train, test number\n",
        "  num_total = len(dataset)\n",
        "  num_train = int(num_total * 0.8 + 1)\n",
        "  num_test  = num_total - num_train\n",
        "\n",
        "  # random split dataset\n",
        "  data_train, data_test = random_split(dataset, [num_train, num_test])\n",
        "\n",
        "  # initialize dataloaders\n",
        "  loader_train = DataLoader(data_train, batch_size=batch_size, shuffle=shuffle_train)\n",
        "  loader_test  = DataLoader(data_test, batch_size=batch_size, shuffle=shuffle_test)\n",
        "\n",
        "  return loader_train, loader_test"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-J4MSc3spcYU",
      "metadata": {
        "id": "-J4MSc3spcYU"
      },
      "source": [
        "### 2.1 Define Feature Extractor with Pretrain Network"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf92bdc8",
      "metadata": {
        "id": "bf92bdc8"
      },
      "source": [
        "For the feature extractor, we select pretrained AlexNet. \n",
        "The reason to choose AlexNet comparing more recent Network like \n",
        "Residural Network is because it has a good balance between model \n",
        "performance and traning complexity. Even though ResNet may perform \n",
        "better than AlexNet by a reasonable amount of gain, it takes way much longer \n",
        "to train or tune a complex network like this, which will dramatically increase the training time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "95ff2db4",
      "metadata": {
        "id": "95ff2db4"
      },
      "outputs": [],
      "source": [
        "class FeatureExtractor(nn.Module):\n",
        "  \"\"\"\n",
        "  FeatureExtractor\n",
        "\n",
        "  Pretrained neural network as a backbone for later domain adaptation task\n",
        "  \"\"\"\n",
        "  def __init__(self):\n",
        "    super(FeatureExtractor, self).__init__()\n",
        "\n",
        "    # Feature Extractor with AlexNet\n",
        "    self.feature_extractor = alexnet(weights='DEFAULT')\n",
        "    self.feature_dim = self.feature_extractor.classifier[-1].in_features\n",
        "\n",
        "    # make the last layer identity\n",
        "    self.feature_extractor.classifier[-1] = nn.Identity()\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.feature_extractor(x)\n",
        "  \n",
        "  def output_dim(self):\n",
        "    return self.feature_dim"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ZjWy3ak98L0F",
      "metadata": {
        "id": "ZjWy3ak98L0F"
      },
      "source": [
        "### 2.2 Define Classifier, Discriminator with RevereLayerF for training the Feature Extractor"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ea4a118",
      "metadata": {
        "id": "2ea4a118"
      },
      "source": [
        "For the classifier, we implement a three fully connected linear layer \n",
        "with LeakyReLU because of its advantage over regular ReLU activation function. \n",
        "And finally we use Logrithm Softmax function as the selection layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "517118c3",
      "metadata": {
        "id": "517118c3"
      },
      "outputs": [],
      "source": [
        "from torch.nn.modules.activation import Softmax\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(Classifier, self).__init__()\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(input_dim, 1024),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(512, output_dim),\n",
        "            nn.LogSoftmax(dim=1)\n",
        "        )\n",
        "    \n",
        "    def forward(self, X):\n",
        "        return self.classifier(X) "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "963e7ba1",
      "metadata": {
        "id": "963e7ba1"
      },
      "source": [
        "Here we implement a ReverseLayer Function to pass thourgh the data \n",
        "as it is without doing any compuation, but on backward propagation, \n",
        "it will reverse the sign of the value to provide the capability \n",
        "to adversarial training from the later Discriminator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "RES6EY4PO7KF",
      "metadata": {
        "id": "RES6EY4PO7KF"
      },
      "outputs": [],
      "source": [
        "from torch.autograd import Function\n",
        "\n",
        "class ReverseLayerF(Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, tensor): \n",
        "        \"\"\"\n",
        "        Without doing any computation\n",
        "        \"\"\"\n",
        "        return tensor.view_as(tensor)\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        \"\"\"\n",
        "        Change the sign of the gradient \n",
        "        \"\"\"\n",
        "        return grad_output.neg(), None"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ecf1ca0",
      "metadata": {
        "id": "7ecf1ca0"
      },
      "source": [
        "Here for the discriminator, we only implement a two-layer linear connection here to avoid overly complicate the Discriminator, becuase usually more complex discriminator will have a negative effect on adversarial training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "K9syaPhFxOFd",
      "metadata": {
        "id": "K9syaPhFxOFd"
      },
      "outputs": [],
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.discriminator =  nn.Sequential(\n",
        "            nn.Linear(int(input_dim), 1024),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(1024,1),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        validity = self.discriminator(x)\n",
        "        return validity "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13a088d9",
      "metadata": {
        "id": "13a088d9"
      },
      "source": [
        "Finally we connect the feature extractor, classifer, and discriminator to form a \n",
        "Domain Adversarial Neural Network. Both classifier and discriminator will take in \n",
        "data from the fearture extractor processed the imput images. The classifer take the \n",
        "number of classes as input, the output is the prediction of which class of the current \n",
        "image belong. While the discriminator are supposed to distinguish the image from two \n",
        "different domains.\n",
        "\n",
        "The discriminator will serve as a doamin alignment unit to train the feature extractor \n",
        "to extract domain independent features from both domains."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "BsbwoZkZwjjl",
      "metadata": {
        "id": "BsbwoZkZwjjl"
      },
      "outputs": [],
      "source": [
        "class DANN(nn.Module):\n",
        "  \"\"\" \n",
        "  DANN\n",
        "\n",
        "  Implement the domain adversarial neural network that train the feature extractor from both classification and discrimination of \n",
        "  different domains.\n",
        "  \"\"\"\n",
        "  def __init__(self, num_classes):\n",
        "    \"\"\" \n",
        "    Parameter:\n",
        "      @num_classes: number of classes of different images\n",
        "    \"\"\"\n",
        "    super(DANN, self).__init__()\n",
        "    self.output_dim = num_classes\n",
        "\n",
        "    # define inner network component\n",
        "    self.feature_extractor = FeatureExtractor()\n",
        "    self.classifier = Classifier(self.feature_extractor.output_dim(), num_classes)\n",
        "    self.discriminator = Discriminator(self.feature_extractor.output_dim())  \n",
        "  \n",
        "  def forward(self, x):\n",
        "    feature_output = self.feature_extractor(x)\n",
        "\n",
        "    class_pred = self.classifier(feature_output)\n",
        "\n",
        "    # Add a ReverseLayer here for negative gradient computation\n",
        "    reverse_feature = ReverseLayerF.apply(feature_output)\n",
        "    domain_pred = self.discriminator(reverse_feature)\n",
        "\n",
        "    return class_pred, domain_pred "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "IhnsX7lq5saz",
      "metadata": {
        "id": "IhnsX7lq5saz"
      },
      "source": [
        "### 2.3 Cost function"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54ef3494",
      "metadata": {
        "id": "54ef3494"
      },
      "source": [
        "For the classification loss function, we use the common\n",
        "Cross Entropy Loss function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "q6BkiCH_5sOr",
      "metadata": {
        "id": "q6BkiCH_5sOr"
      },
      "outputs": [],
      "source": [
        "def get_class_loss_func():\n",
        "  return nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "B-y0dTPj5zrR",
      "metadata": {
        "id": "B-y0dTPj5zrR"
      },
      "source": [
        "### 2.4 Optimizer\n",
        "\n",
        "Setting the **learning rate** according to the original [paper](https://arxiv.org/pdf/1505.07818.pdf) section 5.2.2\n",
        "\n",
        "$$ \\mu_p =  \\frac{\\mu_0}{(1+\\alpha \\cdot p)^\\beta}$$\n",
        "\n",
        "where p is the training progress linearly changing from 0 to 1.\n",
        "\n",
        "And for the learning rate, for the pretrain weights, we set the learning rate only to be 1/10 \n",
        "of the learning rate for the classifier. And we use Stochastic Gradient Descent to optimize the \n",
        "model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "ZsoNnQCq2aEt",
      "metadata": {
        "id": "ZsoNnQCq2aEt"
      },
      "outputs": [],
      "source": [
        "def get_optimizer(model, config, progress, adversarial=True):\n",
        "  '''\n",
        "  get_optimizer\n",
        "\n",
        "  parameter:\n",
        "    @model: Neural Network to be optimizd\n",
        "    @config: configuration dictionary contains parameters\n",
        "    @progress: training progress to configurate learning rate\n",
        "    @adersarial: if we are in adversarial traning scenario \n",
        "\n",
        "  return:\n",
        "    @optimizer: the optimizer we use to train our model\n",
        "\n",
        "  '''\n",
        "  learning_rate = config['lr']\n",
        "  learning_rate = learning_rate / ((1 + config['alpha']*progress)**config['beta'])\n",
        "\n",
        "  weight_decay  = config['wd']\n",
        "  momentum      = config['momentum']\n",
        "\n",
        "  feature_ext   = model.get_submodule(\"feature_extractor\")\n",
        "  classifier    = model.get_submodule(\"classifier\")\n",
        "  discriminator = model.get_submodule(\"discriminator\")\n",
        "\n",
        "  pre_trained_weights = feature_ext.parameters()\n",
        "\n",
        "  if adversarial:\n",
        "    other_weights = list(classifier.parameters()) + list(discriminator.parameters())\n",
        "  else:\n",
        "    other_weights = list(classifier.parameters())\n",
        "\n",
        "  # assign parameters to parameters\n",
        "  optimizer = torch.optim.SGD([\n",
        "    {'params': pre_trained_weights},\n",
        "    {'params': other_weights, 'lr': learning_rate}\n",
        "  ], lr= learning_rate/10, weight_decay=weight_decay, momentum=momentum)\n",
        "  \n",
        "  return optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Atdanl9REs3F",
      "metadata": {
        "id": "Atdanl9REs3F"
      },
      "source": [
        "### 2.5 Training Loop and Testing Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "ORDqPkiT5r1s",
      "metadata": {
        "id": "ORDqPkiT5r1s"
      },
      "outputs": [],
      "source": [
        "def train_loop(dataloader, model, device, progress):\n",
        "  \"\"\"\n",
        "  train_loop\n",
        "\n",
        "  Iterate through dataloader to train the network with SGD optimizer.\n",
        "\n",
        "  Parameters:\n",
        "    @dataloader: Pytorch dataloader to iterate through training\n",
        "    @model: Neural Network model that we are training\n",
        "    @device: GPU or CPU\n",
        "    @progress: the progress of traning based on current epoch over total epochs\n",
        "  \"\"\"\n",
        "  size = len(dataloader.dataset)\n",
        "  loss_fn = get_class_loss_func()\n",
        "\n",
        "  optimizer = get_optimizer(model, config, progress, adversarial=False)\n",
        "\n",
        "  for batch, (X, y) in enumerate(dataloader):\n",
        "    X, y = X.to(device), y.to(device)\n",
        "    \n",
        "    # compute prediction and loss\n",
        "    class_pred, _ = model(X)\n",
        "\n",
        "    # classification loss\n",
        "    loss = loss_fn(class_pred, y) \n",
        "    curr_loss = loss.item()\n",
        "    \n",
        "    # backpropagation\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if batch % 10 == 0:\n",
        "      current = batch * len(X)\n",
        "      print(f\"## Meter ## current loss: {curr_loss:>7f} [{current:>5d}/{size:>5d}]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "LlvQLNUDLGJF",
      "metadata": {
        "id": "LlvQLNUDLGJF"
      },
      "outputs": [],
      "source": [
        "def test_loop(dataloader, model, device):\n",
        "  \"\"\" \n",
        "  test_loop\n",
        "\n",
        "  Test the model by iterate through the dataloader and compute the correctness.\n",
        "\n",
        "  Parameters:\n",
        "    @dataloader: Pytorch dataloader to iterate through training\n",
        "    @model: Neural Network model that we are training\n",
        "    @progress: the progress of traning based on current epoch over total epochs\n",
        "  \n",
        "  @return:\n",
        "    @test_loss: test loss\n",
        "    @correct: correctness of the test data set\n",
        "  \"\"\"\n",
        "  test_loss, correct = 0, 0\n",
        "  loss_fn = get_class_loss_func()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for X, y in dataloader:\n",
        "      X, y = X.to(device), y.to(device)\n",
        "      class_pred, _ = model(X)\n",
        "\n",
        "      test_loss += loss_fn(class_pred, y).item()\n",
        "      correct += (class_pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "  size = len(dataloader.dataset)\n",
        "  num_batches = len(dataloader)\n",
        "\n",
        "  test_loss /= num_batches\n",
        "  correct /= size\n",
        "  print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "\n",
        "  return test_loss, correct"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "z5uhItnwhj8S",
      "metadata": {
        "id": "z5uhItnwhj8S"
      },
      "source": [
        "### 2.6 Training Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "nM76Syqahknd",
      "metadata": {
        "id": "nM76Syqahknd"
      },
      "outputs": [],
      "source": [
        "def training(model, train_dataloader, test_dataloader, config, device):\n",
        "  \"\"\" \n",
        "  training\n",
        "\n",
        "  Acturall training function iterate through the training epochs\n",
        "  \"\"\"\n",
        "  epochs = config['epochs']\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    print(f\"Epoch {epoch+1}\\n------------------\")\n",
        "    progress = epoch/epochs\n",
        "\n",
        "    train_loop(train_dataloader, model, device, progress)\n",
        "\n",
        "  test_loop(train_dataloader, model, device)\n",
        "  print(\"Done\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "PIkYlRhi4tkK",
      "metadata": {
        "id": "PIkYlRhi4tkK"
      },
      "source": [
        "## 3 Training without using Domain Adaptation techniques"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "chQgbFmYorL-",
      "metadata": {
        "id": "chQgbFmYorL-"
      },
      "source": [
        " ### 3.1 Product Domain -> Real Life"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "OCWnRXkc79a4",
      "metadata": {
        "id": "OCWnRXkc79a4"
      },
      "outputs": [],
      "source": [
        "# Get dataloader\n",
        "product_dataset   = get_dataset(product_path)\n",
        "real_life_dataset = get_dataset(real_life_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "raxew0mKm-gM",
      "metadata": {
        "id": "raxew0mKm-gM"
      },
      "source": [
        "#### 3.1.1 Training on Source Domain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "V93UE4rXi3C9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 972,
          "referenced_widgets": [
            "006b6f04be3e49aca1a1a6304487bc22",
            "f26b7b7198ac4a2ba2160bb0185710bc",
            "3cfa0650b073425b9db88e950a6375e2",
            "920d754e6f1e4c98a35a2cf7acfdfdec",
            "452ca156ac4144b4b9e3cd7a07bdeafd",
            "7eba7b50481942449ffde61c040fc878",
            "eb174833685340019c0312f0b44239d6",
            "cca62b159ef74ae79e4944e0b57d2638",
            "d57549fedbe5434ab712cf33ac731bd4",
            "61abad4ab57a49a5999f056f4e3542a7",
            "2ecd86596c24481dac72f910d7d7a4ec"
          ]
        },
        "id": "V93UE4rXi3C9",
        "outputId": "66c76356-fb11-40bc-81d2-c44fceb37484"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/alexnet-owt-7be5be79.pth\" to /root/.cache/torch/hub/checkpoints/alexnet-owt-7be5be79.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/233M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "006b6f04be3e49aca1a1a6304487bc22"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "------------------\n",
            "## Meter ## current loss: 3.022623 [    0/ 1601]\n",
            "## Meter ## current loss: 2.370648 [  640/ 1601]\n",
            "## Meter ## current loss: 0.623379 [ 1280/ 1601]\n",
            "Epoch 2\n",
            "------------------\n",
            "## Meter ## current loss: 0.561586 [    0/ 1601]\n",
            "## Meter ## current loss: 0.368818 [  640/ 1601]\n",
            "## Meter ## current loss: 0.529716 [ 1280/ 1601]\n",
            "Epoch 3\n",
            "------------------\n",
            "## Meter ## current loss: 0.151164 [    0/ 1601]\n",
            "## Meter ## current loss: 0.156962 [  640/ 1601]\n",
            "## Meter ## current loss: 0.271796 [ 1280/ 1601]\n",
            "Epoch 4\n",
            "------------------\n",
            "## Meter ## current loss: 0.132108 [    0/ 1601]\n",
            "## Meter ## current loss: 0.124176 [  640/ 1601]\n",
            "## Meter ## current loss: 0.094285 [ 1280/ 1601]\n",
            "Epoch 5\n",
            "------------------\n",
            "## Meter ## current loss: 0.102055 [    0/ 1601]\n",
            "## Meter ## current loss: 0.076171 [  640/ 1601]\n",
            "## Meter ## current loss: 0.113716 [ 1280/ 1601]\n",
            "Epoch 6\n",
            "------------------\n",
            "## Meter ## current loss: 0.055870 [    0/ 1601]\n",
            "## Meter ## current loss: 0.056029 [  640/ 1601]\n",
            "## Meter ## current loss: 0.137478 [ 1280/ 1601]\n",
            "Epoch 7\n",
            "------------------\n",
            "## Meter ## current loss: 0.023269 [    0/ 1601]\n",
            "## Meter ## current loss: 0.091175 [  640/ 1601]\n",
            "## Meter ## current loss: 0.065120 [ 1280/ 1601]\n",
            "Epoch 8\n",
            "------------------\n",
            "## Meter ## current loss: 0.111641 [    0/ 1601]\n",
            "## Meter ## current loss: 0.043276 [  640/ 1601]\n",
            "## Meter ## current loss: 0.026053 [ 1280/ 1601]\n",
            "Epoch 9\n",
            "------------------\n",
            "## Meter ## current loss: 0.027613 [    0/ 1601]\n",
            "## Meter ## current loss: 0.067207 [  640/ 1601]\n",
            "## Meter ## current loss: 0.019714 [ 1280/ 1601]\n",
            "Epoch 10\n",
            "------------------\n",
            "## Meter ## current loss: 0.034192 [    0/ 1601]\n",
            "## Meter ## current loss: 0.060513 [  640/ 1601]\n",
            "## Meter ## current loss: 0.028750 [ 1280/ 1601]\n",
            "Done\n"
          ]
        }
      ],
      "source": [
        "train_dataloader, test_dataloader = get_dataloader(product_dataset, config['batch_size'])\n",
        "\n",
        "model = DANN(len(product_dataset.classes)).to(device)\n",
        "\n",
        "# Training\n",
        "training(model, train_dataloader, test_dataloader, config, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "__d71xhd5SVx",
      "metadata": {
        "id": "__d71xhd5SVx"
      },
      "source": [
        "#### 3.1.2 Test on Real Life"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "E54Vm3jmTRcc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E54Vm3jmTRcc",
        "outputId": "8543b30a-742b-4230-8336-efcd1ab98794"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Error: \n",
            " Accuracy: 63.5%, Avg loss: 1.235210 \n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1.2352098813280463, 0.635)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "loader_target_dataset = DataLoader(real_life_dataset, batch_size=config['batch_size'], shuffle=False)\n",
        "\n",
        "# model.load_state_dict(torch.load('model_state.pt', map_location='cpu'))\n",
        "test_loop(loader_target_dataset, model, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "Cw4GP_z-_iy_",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cw4GP_z-_iy_",
        "outputId": "708f4551-d19b-4702-9e6e-5a0c6ff95195"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "513480192\n"
          ]
        }
      ],
      "source": [
        "del train_dataloader, test_dataloader, loader_target_dataset\n",
        "del model\n",
        "print(torch.cuda.memory_allocated())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ITPgzgfR5BQB",
      "metadata": {
        "id": "ITPgzgfR5BQB"
      },
      "source": [
        "### 3.2 Real Life -> Product\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42FQtZSXoRSI",
      "metadata": {
        "id": "42FQtZSXoRSI"
      },
      "source": [
        "#### 3.2.1 Training on Real Life"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "Sb5mqdPMBHli",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sb5mqdPMBHli",
        "outputId": "1ca7e284-b040-4b1d-c2d0-cbbadfb13a75"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "------------------\n",
            "## Meter ## current loss: 3.017269 [    0/ 1601]\n",
            "## Meter ## current loss: 2.727951 [  640/ 1601]\n",
            "## Meter ## current loss: 1.711068 [ 1280/ 1601]\n",
            "Epoch 2\n",
            "------------------\n",
            "## Meter ## current loss: 2.275105 [    0/ 1601]\n",
            "## Meter ## current loss: 1.220824 [  640/ 1601]\n",
            "## Meter ## current loss: 1.335409 [ 1280/ 1601]\n",
            "Epoch 3\n",
            "------------------\n",
            "## Meter ## current loss: 0.973024 [    0/ 1601]\n",
            "## Meter ## current loss: 0.608552 [  640/ 1601]\n",
            "## Meter ## current loss: 0.610510 [ 1280/ 1601]\n",
            "Epoch 4\n",
            "------------------\n",
            "## Meter ## current loss: 0.376343 [    0/ 1601]\n",
            "## Meter ## current loss: 0.473844 [  640/ 1601]\n",
            "## Meter ## current loss: 0.344296 [ 1280/ 1601]\n",
            "Epoch 5\n",
            "------------------\n",
            "## Meter ## current loss: 0.477025 [    0/ 1601]\n",
            "## Meter ## current loss: 0.330870 [  640/ 1601]\n",
            "## Meter ## current loss: 0.323125 [ 1280/ 1601]\n",
            "Epoch 6\n",
            "------------------\n",
            "## Meter ## current loss: 0.224157 [    0/ 1601]\n",
            "## Meter ## current loss: 0.283363 [  640/ 1601]\n",
            "## Meter ## current loss: 0.233867 [ 1280/ 1601]\n",
            "Epoch 7\n",
            "------------------\n",
            "## Meter ## current loss: 0.244277 [    0/ 1601]\n",
            "## Meter ## current loss: 0.146179 [  640/ 1601]\n",
            "## Meter ## current loss: 0.232419 [ 1280/ 1601]\n",
            "Epoch 8\n",
            "------------------\n",
            "## Meter ## current loss: 0.143267 [    0/ 1601]\n",
            "## Meter ## current loss: 0.060132 [  640/ 1601]\n",
            "## Meter ## current loss: 0.071144 [ 1280/ 1601]\n",
            "Epoch 9\n",
            "------------------\n",
            "## Meter ## current loss: 0.154751 [    0/ 1601]\n",
            "## Meter ## current loss: 0.123206 [  640/ 1601]\n",
            "## Meter ## current loss: 0.204870 [ 1280/ 1601]\n",
            "Epoch 10\n",
            "------------------\n",
            "## Meter ## current loss: 0.082381 [    0/ 1601]\n",
            "## Meter ## current loss: 0.206807 [  640/ 1601]\n",
            "## Meter ## current loss: 0.127214 [ 1280/ 1601]\n",
            "Done\n"
          ]
        }
      ],
      "source": [
        "train_dataloader, test_dataloader = get_dataloader(real_life_dataset, config['batch_size'])\n",
        "\n",
        "model = DANN(len(real_life_dataset.classes)).to(device)\n",
        "\n",
        "# Training\n",
        "training(model, train_dataloader, test_dataloader, config, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7HMoVx_85eur",
      "metadata": {
        "id": "7HMoVx_85eur"
      },
      "source": [
        "#### 3.2.2 Testing on Product\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "shPERUI05drr",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "shPERUI05drr",
        "outputId": "033865e3-84fa-41a3-ffc0-fb6b0a736adc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Error: \n",
            " Accuracy: 80.3%, Avg loss: 0.723673 \n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.7236725512193516, 0.8035)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "loader_target_dataset = DataLoader(product_dataset, batch_size=config['batch_size'], shuffle=False)\n",
        "\n",
        "# model.load_state_dict(torch.load('model_state.pt', map_location='cpu'))\n",
        "test_loop(loader_target_dataset, model, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "6aT-zlOq862D",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6aT-zlOq862D",
        "outputId": "77b96451-ebcc-43cd-bde2-bd50a72477f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "513021440\n"
          ]
        }
      ],
      "source": [
        "del train_dataloader, test_dataloader, loader_target_dataset\n",
        "del model\n",
        "print(torch.cuda.memory_allocated())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7q7jwnwwhRz2",
      "metadata": {
        "id": "7q7jwnwwhRz2"
      },
      "source": [
        "## 4: Define UDA functions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "425ysNsFjUy-",
      "metadata": {
        "id": "425ysNsFjUy-"
      },
      "source": [
        "### 4.1 Adversarial Discriminator Loss"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d9613319",
      "metadata": {
        "id": "d9613319"
      },
      "source": [
        "Here we compute the discrimination loss of from the discriminator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "81ccf625",
      "metadata": {
        "id": "81ccf625"
      },
      "outputs": [],
      "source": [
        "def get_discriminator_loss(source_pred, target_pred): \n",
        "    \"\"\" \n",
        "    get_discriminator_loss\n",
        "\n",
        "    parameters:\n",
        "        @source_pred: model prediction from the source data\n",
        "        @target_pred: model prediction from the target data\n",
        "\n",
        "    return:\n",
        "        @domain_loss: computed domain loss\n",
        "    \"\"\"\n",
        "    domain_pred = torch.cat((source_pred, target_pred),dim=0).cuda()\n",
        "    #print(domain_pred.shape) # [128,1024]\n",
        "    source_truth = torch.zeros(len(source_pred))\n",
        "    target_truth = torch.ones(len(target_pred))\n",
        "    domain_truth = torch.cat((source_truth, target_truth),dim=0).cuda()\n",
        "    #print(domain_truth.shape) # [128]\n",
        "\n",
        "    domain_loss = domain_truth*torch.log(1/domain_pred)+(1-domain_truth)*torch.log(1/(1-domain_pred))\n",
        "    domain_loss = domain_loss.mean()\n",
        "\n",
        "    return domain_loss "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "BBaNX5GgjK7F",
      "metadata": {
        "id": "BBaNX5GgjK7F"
      },
      "source": [
        "### 4.2 Adversarial optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f048c0a5",
      "metadata": {
        "id": "f048c0a5"
      },
      "source": [
        "We are using the Stochastic Gradient Descent optimizer and \n",
        "set learning rate for the pre_trained_weights to be 1/10\n",
        "of other learning rate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "zKscrnYohp7k",
      "metadata": {
        "id": "zKscrnYohp7k"
      },
      "outputs": [],
      "source": [
        "def get_adversarial_optimizer(model, config, progress, adversarial=True):\n",
        "  '''\n",
        "  Get Adversarial Optimizers\n",
        "  '''\n",
        "  lr, wd, momtm = config['lr'], config['wd'], config['momentum']\n",
        "  lr = lr / ((1 + config['alpha']*progress)**config['beta'])\n",
        "\n",
        "  feature_ext   = model.get_submodule(\"feature_extractor\")\n",
        "  classifier    = model.get_submodule(\"classifier\")\n",
        "  discriminator = model.get_submodule(\"discriminator\")\n",
        "\n",
        "  pre_trained_weights   = feature_ext.parameters()\n",
        "  classifier_weights    = classifier.parameters()\n",
        "  discriminator_weights = discriminator.parameters()\n",
        "\n",
        "  feature_optim       = torch.optim.SGD([{'params': pre_trained_weights}],     lr=lr/10, weight_decay=wd, momentum=momtm)\n",
        "  classifier_optim    = torch.optim.SGD([{'params': classifier_weights}],      lr=lr,    weight_decay=wd, momentum=momtm)\n",
        "  discriminator_optim = torch.optim.SGD([{'params': discriminator_weights}],   lr=lr,    weight_decay=wd, momentum=momtm)\n",
        "  \n",
        "  return feature_optim, classifier_optim, discriminator_optim "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hRbDuyfU929u",
      "metadata": {
        "id": "hRbDuyfU929u"
      },
      "source": [
        "### 4.3 Adversarial Train Loop\n",
        "\n",
        "Setting the **domain adaptation parameter** according to the original [paper](https://arxiv.org/pdf/1505.07818.pdf) section 5.2.2\n",
        "\n",
        "$$ \\lambda_p = \\frac{2}{1 + exp(-\\gamma \\cdot p)} - 1 $$\n",
        "\n",
        "where p is the training progress linearly changing from 0 to 1.\n",
        "\n",
        "So here we optimize the model by calculating the classification loss and discrimination loss. \n",
        "Then we optimize the classifier, the discriminator, and the feature extractor based\n",
        "on the loss we get."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "1SQHSVPE93hi",
      "metadata": {
        "id": "1SQHSVPE93hi"
      },
      "outputs": [],
      "source": [
        "def adversarial_train_loop(source_loader, target_loader, model, config, progress, device):\n",
        "  \"\"\"\n",
        "  parameters:\n",
        "    @source_loader\n",
        "    @target_loader\n",
        "    @model\n",
        "    @config\n",
        "    @progress\n",
        "    @device\n",
        "\n",
        "  return:\n",
        "    @best_state\n",
        "    @best_loss\n",
        "  \"\"\"\n",
        "  size = len(source_loader.dataset)\n",
        "  \n",
        "  # cross entropy loss\n",
        "  classification_loss = get_class_loss_func()\n",
        "\n",
        "  # Get three optimizer\n",
        "  feature_optim, class_optim, discriminator_optim = get_adversarial_optimizer(model, config, progress)\n",
        "\n",
        "  # Target data loader iterator\n",
        "  iter_target = iter(target_loader)\n",
        "\n",
        "  domain_adapt = 2 / (1 + math.exp(-config['gamma']*progress)) - 1\n",
        "\n",
        "  for batch, (X_source, y_source) in enumerate(source_loader):\n",
        "    try:\n",
        "      X_target, _ = next(iter_target)\n",
        "    except:\n",
        "      iter_target = iter(target_loader)\n",
        "      X_target, _ = next(iter_target)  \n",
        "\n",
        "    # Some internal bug return nested tesnor with size 1\n",
        "    if len(X_source) < 64:\n",
        "      continue\n",
        "\n",
        "    X_source, y_source, X_target = X_source.to(device), y_source.to(device), X_target.to(device)\n",
        "\n",
        "    class_pred_source, domain_pred_source = model(X_source)\n",
        "    _,                 domain_pred_target = model(X_target)\n",
        "\n",
        "    class_loss   = classification_loss(class_pred_source, y_source)\n",
        "    discrim_loss = get_discriminator_loss(domain_pred_source, domain_pred_target)\n",
        "\n",
        "    feature_optim.zero_grad()\n",
        "\n",
        "    # Update discriminator\n",
        "    discriminator_optim.zero_grad()\n",
        "    discrim_loss.backward(retain_graph=True)\n",
        "    discriminator_optim.step()\n",
        "\n",
        "    # Update classifier\n",
        "    class_optim.zero_grad()\n",
        "    class_loss.backward(retain_graph=True)\n",
        "    class_optim.step()\n",
        "\n",
        "    # Update feature extractor\n",
        "    feature_optim.step()  \n",
        "\n",
        "    # Total loss\n",
        "    total_loss = class_loss - domain_adapt * discrim_loss \n",
        "\n",
        "    if batch % 10 == 0:\n",
        "      class_loss, discrim_loss, current = class_loss.item(), discrim_loss.item(), batch * len(X_source)\n",
        "      total_loss = total_loss.item()\n",
        "      # print(f\"## Meter  ## [{current:>5d}/{size:>5d}]\")\n",
        "      print(f\"## Meter  ## classification loss: {class_loss:>7f} discrim loss: {discrim_loss:>7f} total loss: {total_loss:>7f}[{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "    del class_loss, discrim_loss \n",
        "    del X_source, y_source, X_target, class_pred_source, domain_pred_source, domain_pred_target\n",
        "  \n",
        "  # return best_state, best_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1jHD6IJ6H8vF",
      "metadata": {
        "id": "1jHD6IJ6H8vF"
      },
      "source": [
        "### 4.4 Adversarial Test Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "KyhdZbs3H9Ue",
      "metadata": {
        "id": "KyhdZbs3H9Ue"
      },
      "outputs": [],
      "source": [
        "def adversarial_test_loop(dataloader, model, device, name=\"\"):\n",
        "  \"\"\" \n",
        "  adversarial_test_loop\n",
        "\n",
        "  Test the model compute the loss and accuracy\n",
        "  \"\"\"\n",
        "  test_loss, correct = 0, 0\n",
        "\n",
        "  class_loss_func = get_class_loss_func()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for X, y in dataloader:\n",
        "      X, y = X.to(device), y.to(device)\n",
        "      class_pred, _ = model(X)\n",
        "\n",
        "      test_loss += class_loss_func(class_pred, y).item()\n",
        "      correct += (class_pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "  size = len(dataloader.dataset)\n",
        "  num_batches = len(dataloader)\n",
        "\n",
        "  test_loss /= num_batches\n",
        "  correct /= size\n",
        "  print(f\"{name} Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "\n",
        "  return test_loss, correct"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tyopBjWr-FAs",
      "metadata": {
        "id": "tyopBjWr-FAs"
      },
      "source": [
        "### 4.5 Adversarial Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "dBCrfNPj-Bxa",
      "metadata": {
        "id": "dBCrfNPj-Bxa"
      },
      "outputs": [],
      "source": [
        "def adversarial_training(model, source_loader, source_test_loader, target_loader, config, device):\n",
        "  \"\"\" \n",
        "  adversarial_training\n",
        "\n",
        "  Training the adversarial model with the config\n",
        "  \"\"\"\n",
        "  no_improve_count = 0\n",
        "\n",
        "  for epoch in range(config['epochs']):\n",
        "    print(f\"Epoch {epoch+1}\\n------------------\")\n",
        "    progress = epoch/config['epochs']\n",
        "\n",
        "    adversarial_train_loop(source_loader, target_loader, model, config, progress, device)\n",
        "\n",
        "    source_loss, _ = adversarial_test_loop(source_test_loader, model, device, \"Source Test\")\n",
        "\n",
        "  print(\"Done\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "x4ePILfZ4jOA",
      "metadata": {
        "id": "x4ePILfZ4jOA"
      },
      "source": [
        "## 5 Training with UDA Techniques"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "wt-IOOThNShC",
      "metadata": {
        "id": "wt-IOOThNShC"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "rcLFqgOelDsW",
      "metadata": {
        "id": "rcLFqgOelDsW"
      },
      "outputs": [],
      "source": [
        "adv_model = DANN(len(product_dataset.classes)).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "H7QPRLosqltl",
      "metadata": {
        "id": "H7QPRLosqltl"
      },
      "source": [
        "### 5.1 Product -> Real Life"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "VKR4440ZqxJV",
      "metadata": {
        "id": "VKR4440ZqxJV"
      },
      "source": [
        "#### 5.1.1 Training on Product\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "IcUMoe8x_qrC",
      "metadata": {
        "id": "IcUMoe8x_qrC"
      },
      "outputs": [],
      "source": [
        "train_dataloader, train_test_dataloader = get_dataloader(product_dataset, config['batch_size'])\n",
        "target_dataloader, target_test_dataloader = get_dataloader(real_life_dataset, config['batch_size'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "ZgN_WIqqPtjk",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZgN_WIqqPtjk",
        "outputId": "1b44e911-54b9-4871-d383-86559efb3d16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "------------------\n",
            "## Meter  ## classification loss: 3.021090 discrim loss: 0.696953 total loss: 3.021090[    0/ 1601]\n",
            "## Meter  ## classification loss: 2.344038 discrim loss: 0.696019 total loss: 2.344038[  640/ 1601]\n",
            "## Meter  ## classification loss: 0.490591 discrim loss: 0.695255 total loss: 0.490591[ 1280/ 1601]\n",
            "Source Test Test Error: \n",
            " Accuracy: 78.9%, Avg loss: 0.787940 \n",
            "\n",
            "Epoch 2\n",
            "------------------\n",
            "## Meter  ## classification loss: 0.333283 discrim loss: 0.696012 total loss: 0.011644[    0/ 1601]\n",
            "## Meter  ## classification loss: 0.363338 discrim loss: 0.695608 total loss: 0.041885[  640/ 1601]\n",
            "## Meter  ## classification loss: 0.492627 discrim loss: 0.695451 total loss: 0.171248[ 1280/ 1601]\n",
            "Source Test Test Error: \n",
            " Accuracy: 91.0%, Avg loss: 0.356655 \n",
            "\n",
            "Epoch 3\n",
            "------------------\n",
            "## Meter  ## classification loss: 0.238965 discrim loss: 0.694827 total loss: -0.290211[    0/ 1601]\n",
            "## Meter  ## classification loss: 0.219590 discrim loss: 0.694609 total loss: -0.309420[  640/ 1601]\n",
            "## Meter  ## classification loss: 0.153543 discrim loss: 0.694763 total loss: -0.375584[ 1280/ 1601]\n",
            "Source Test Test Error: \n",
            " Accuracy: 90.0%, Avg loss: 0.288513 \n",
            "\n",
            "Epoch 4\n",
            "------------------\n",
            "## Meter  ## classification loss: 0.112897 discrim loss: 0.694656 total loss: -0.515870[    0/ 1601]\n",
            "## Meter  ## classification loss: 0.150388 discrim loss: 0.694313 total loss: -0.478068[  640/ 1601]\n",
            "## Meter  ## classification loss: 0.142302 discrim loss: 0.694992 total loss: -0.486769[ 1280/ 1601]\n",
            "Source Test Test Error: \n",
            " Accuracy: 91.5%, Avg loss: 0.286431 \n",
            "\n",
            "Epoch 5\n",
            "------------------\n",
            "## Meter  ## classification loss: 0.087729 discrim loss: 0.694566 total loss: -0.581851[    0/ 1601]\n",
            "## Meter  ## classification loss: 0.061096 discrim loss: 0.694679 total loss: -0.608594[  640/ 1601]\n",
            "## Meter  ## classification loss: 0.101559 discrim loss: 0.694609 total loss: -0.568062[ 1280/ 1601]\n",
            "Source Test Test Error: \n",
            " Accuracy: 93.2%, Avg loss: 0.225057 \n",
            "\n",
            "Epoch 6\n",
            "------------------\n",
            "## Meter  ## classification loss: 0.070670 discrim loss: 0.694606 total loss: -0.614638[    0/ 1601]\n",
            "## Meter  ## classification loss: 0.044589 discrim loss: 0.695129 total loss: -0.641235[  640/ 1601]\n",
            "## Meter  ## classification loss: 0.130785 discrim loss: 0.694509 total loss: -0.554427[ 1280/ 1601]\n",
            "Source Test Test Error: \n",
            " Accuracy: 92.7%, Avg loss: 0.275630 \n",
            "\n",
            "Epoch 7\n",
            "------------------\n",
            "## Meter  ## classification loss: 0.043378 discrim loss: 0.694456 total loss: -0.647644[    0/ 1601]\n",
            "## Meter  ## classification loss: 0.050608 discrim loss: 0.694527 total loss: -0.640484[  640/ 1601]\n",
            "## Meter  ## classification loss: 0.048671 discrim loss: 0.694470 total loss: -0.642364[ 1280/ 1601]\n",
            "Source Test Test Error: \n",
            " Accuracy: 92.7%, Avg loss: 0.238769 \n",
            "\n",
            "Epoch 8\n",
            "------------------\n",
            "## Meter  ## classification loss: 0.049508 discrim loss: 0.695376 total loss: -0.644601[    0/ 1601]\n",
            "## Meter  ## classification loss: 0.014313 discrim loss: 0.694443 total loss: -0.678865[  640/ 1601]\n",
            "## Meter  ## classification loss: 0.056189 discrim loss: 0.694842 total loss: -0.637386[ 1280/ 1601]\n",
            "Source Test Test Error: \n",
            " Accuracy: 93.5%, Avg loss: 0.197375 \n",
            "\n",
            "Epoch 9\n",
            "------------------\n",
            "## Meter  ## classification loss: 0.030193 discrim loss: 0.694721 total loss: -0.664062[    0/ 1601]\n",
            "## Meter  ## classification loss: 0.030535 discrim loss: 0.694916 total loss: -0.663915[  640/ 1601]\n",
            "## Meter  ## classification loss: 0.058568 discrim loss: 0.694539 total loss: -0.635505[ 1280/ 1601]\n",
            "Source Test Test Error: \n",
            " Accuracy: 90.5%, Avg loss: 0.277224 \n",
            "\n",
            "Epoch 10\n",
            "------------------\n",
            "## Meter  ## classification loss: 0.042411 discrim loss: 0.694673 total loss: -0.652091[    0/ 1601]\n",
            "## Meter  ## classification loss: 0.036083 discrim loss: 0.694900 total loss: -0.658646[  640/ 1601]\n",
            "## Meter  ## classification loss: 0.025790 discrim loss: 0.694572 total loss: -0.668610[ 1280/ 1601]\n",
            "Source Test Test Error: \n",
            " Accuracy: 91.5%, Avg loss: 0.260730 \n",
            "\n",
            "Done\n"
          ]
        }
      ],
      "source": [
        "torch.autograd.set_detect_anomaly(True)\n",
        "adversarial_training(adv_model, train_dataloader, train_test_dataloader, target_dataloader, config, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1s0Z_xXXq5gO",
      "metadata": {
        "id": "1s0Z_xXXq5gO"
      },
      "source": [
        "#### 5.1.2 Testing on Real Life"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "oUUo7mSiJasc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oUUo7mSiJasc",
        "outputId": "1637628f-92ee-4b7a-d8e2-9c8172c6d995"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Error: \n",
            " Accuracy: 63.5%, Avg loss: 1.246334 \n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1.2463341187685728, 0.6355)"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "loader_target_dataset = DataLoader(real_life_dataset, batch_size=config['batch_size'], shuffle=False)\n",
        "\n",
        "test_loop(loader_target_dataset, adv_model, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "FNofveL_-8gX",
      "metadata": {
        "id": "FNofveL_-8gX"
      },
      "source": [
        "### 5.2 Real Life -> Product"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6YxXx0Lzresh",
      "metadata": {
        "id": "6YxXx0Lzresh"
      },
      "source": [
        "#### 5.2.1 Training on Real Life"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "b8MUwY3CFb6m",
      "metadata": {
        "id": "b8MUwY3CFb6m"
      },
      "outputs": [],
      "source": [
        "del adv_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "enMKc_Em_tWq",
      "metadata": {
        "id": "enMKc_Em_tWq"
      },
      "outputs": [],
      "source": [
        "train_dataloader, train_test_dataloader = get_dataloader(real_life_dataset, config['batch_size'])\n",
        "target_dataloader, target_test_dataloader = get_dataloader(product_dataset, config['batch_size'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "Msifip2P-7UB",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Msifip2P-7UB",
        "outputId": "c8bf0d59-d8b6-49ad-c4e8-ef364cceade7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "------------------\n",
            "## Meter  ## classification loss: 2.983728 discrim loss: 0.699459 total loss: 2.983728[    0/ 1601]\n",
            "## Meter  ## classification loss: 2.726490 discrim loss: 0.695359 total loss: 2.726490[  640/ 1601]\n",
            "## Meter  ## classification loss: 1.870590 discrim loss: 0.694424 total loss: 1.870590[ 1280/ 1601]\n",
            "Source Test Test Error: \n",
            " Accuracy: 67.7%, Avg loss: 1.198479 \n",
            "\n",
            "Epoch 2\n",
            "------------------\n",
            "## Meter  ## classification loss: 1.087310 discrim loss: 0.695395 total loss: 0.765956[    0/ 1601]\n",
            "## Meter  ## classification loss: 1.110395 discrim loss: 0.695068 total loss: 0.789192[  640/ 1601]\n",
            "## Meter  ## classification loss: 0.514551 discrim loss: 0.695518 total loss: 0.193140[ 1280/ 1601]\n",
            "Source Test Test Error: \n",
            " Accuracy: 72.7%, Avg loss: 0.866361 \n",
            "\n",
            "Epoch 3\n",
            "------------------\n",
            "## Meter  ## classification loss: 0.638297 discrim loss: 0.694762 total loss: 0.109170[    0/ 1601]\n",
            "## Meter  ## classification loss: 0.457286 discrim loss: 0.694884 total loss: -0.071934[  640/ 1601]\n",
            "## Meter  ## classification loss: 0.667600 discrim loss: 0.694741 total loss: 0.138489[ 1280/ 1601]\n",
            "Source Test Test Error: \n",
            " Accuracy: 75.9%, Avg loss: 0.769801 \n",
            "\n",
            "Epoch 4\n",
            "------------------\n",
            "## Meter  ## classification loss: 0.336833 discrim loss: 0.695117 total loss: -0.292351[    0/ 1601]\n",
            "## Meter  ## classification loss: 0.431986 discrim loss: 0.695083 total loss: -0.197167[  640/ 1601]\n",
            "## Meter  ## classification loss: 0.222286 discrim loss: 0.695349 total loss: -0.407107[ 1280/ 1601]\n",
            "Source Test Test Error: \n",
            " Accuracy: 75.4%, Avg loss: 0.777168 \n",
            "\n",
            "Epoch 5\n",
            "------------------\n",
            "## Meter  ## classification loss: 0.250751 discrim loss: 0.694968 total loss: -0.419218[    0/ 1601]\n",
            "## Meter  ## classification loss: 0.372841 discrim loss: 0.695360 total loss: -0.297505[  640/ 1601]\n",
            "## Meter  ## classification loss: 0.288102 discrim loss: 0.695444 total loss: -0.382325[ 1280/ 1601]\n",
            "Source Test Test Error: \n",
            " Accuracy: 74.4%, Avg loss: 0.804030 \n",
            "\n",
            "Epoch 6\n",
            "------------------\n",
            "## Meter  ## classification loss: 0.239985 discrim loss: 0.695456 total loss: -0.446162[    0/ 1601]\n",
            "## Meter  ## classification loss: 0.358790 discrim loss: 0.694968 total loss: -0.326875[  640/ 1601]\n",
            "## Meter  ## classification loss: 0.251069 discrim loss: 0.695401 total loss: -0.435024[ 1280/ 1601]\n",
            "Source Test Test Error: \n",
            " Accuracy: 75.7%, Avg loss: 0.795693 \n",
            "\n",
            "Epoch 7\n",
            "------------------\n",
            "## Meter  ## classification loss: 0.110788 discrim loss: 0.695464 total loss: -0.581237[    0/ 1601]\n",
            "## Meter  ## classification loss: 0.139444 discrim loss: 0.695501 total loss: -0.552617[  640/ 1601]\n",
            "## Meter  ## classification loss: 0.186007 discrim loss: 0.695237 total loss: -0.505791[ 1280/ 1601]\n",
            "Source Test Test Error: \n",
            " Accuracy: 76.7%, Avg loss: 0.798509 \n",
            "\n",
            "Epoch 8\n",
            "------------------\n",
            "## Meter  ## classification loss: 0.089797 discrim loss: 0.697212 total loss: -0.606145[    0/ 1601]\n",
            "## Meter  ## classification loss: 0.249068 discrim loss: 0.695066 total loss: -0.444732[  640/ 1601]\n",
            "## Meter  ## classification loss: 0.206116 discrim loss: 0.696041 total loss: -0.488657[ 1280/ 1601]\n",
            "Source Test Test Error: \n",
            " Accuracy: 75.2%, Avg loss: 0.844384 \n",
            "\n",
            "Epoch 9\n",
            "------------------\n",
            "## Meter  ## classification loss: 0.119872 discrim loss: 0.695179 total loss: -0.574841[    0/ 1601]\n",
            "## Meter  ## classification loss: 0.111938 discrim loss: 0.694967 total loss: -0.582563[  640/ 1601]\n",
            "## Meter  ## classification loss: 0.114675 discrim loss: 0.696162 total loss: -0.581020[ 1280/ 1601]\n",
            "Source Test Test Error: \n",
            " Accuracy: 76.4%, Avg loss: 0.763087 \n",
            "\n",
            "Epoch 10\n",
            "------------------\n",
            "## Meter  ## classification loss: 0.081924 discrim loss: 0.695153 total loss: -0.613057[    0/ 1601]\n",
            "## Meter  ## classification loss: 0.084847 discrim loss: 0.694831 total loss: -0.609813[  640/ 1601]\n",
            "## Meter  ## classification loss: 0.043882 discrim loss: 0.695395 total loss: -0.651341[ 1280/ 1601]\n",
            "Source Test Test Error: \n",
            " Accuracy: 76.9%, Avg loss: 0.841807 \n",
            "\n",
            "Done\n"
          ]
        }
      ],
      "source": [
        "# Training\n",
        "adv_model = DANN(len(product_dataset.classes)).to(device)\n",
        "adversarial_training(adv_model, train_dataloader, train_test_dataloader, target_dataloader, config, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "nSh7gaC8JaVC",
      "metadata": {
        "id": "nSh7gaC8JaVC"
      },
      "source": [
        "#### 5.2.2 Testing on Product"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "DfSRjr84-8DF",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DfSRjr84-8DF",
        "outputId": "5c6a1325-91b2-42e5-8efc-187f90f103a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Error: \n",
            " Accuracy: 80.0%, Avg loss: 0.708086 \n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.7080864875169937, 0.7995)"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "loader_target_dataset = DataLoader(product_dataset, batch_size=config['batch_size'], shuffle=False)\n",
        "\n",
        "test_loop(loader_target_dataset, adv_model, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "bFtADV4AtN1Z",
      "metadata": {
        "id": "bFtADV4AtN1Z"
      },
      "outputs": [],
      "source": [
        "# del source_dataset, train_dataloader, test_dataloader, target_dataset, loader_target_dataset\n",
        "del adv_model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "YOKgE6LQuqHC",
      "metadata": {
        "id": "YOKgE6LQuqHC"
      },
      "source": [
        "## 6 Result Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.1 Training Accuracy and Test Result Comparison\n",
        "\n",
        "AlexNet(Training) serve as an upper bound of the model accuracy\n",
        "\n",
        "| Method   | Product -> Real Life | Real Life -> Product | Avg |\n",
        "|----------|:-------------:|:------:|:---:|\n",
        "| AlexNet(Training) |  | |\n",
        "| AlexNet |  63.5% | 80.3%  | |\n",
        "| DANN(UDA) |    63.5%   |   80.0%  | |\n",
        "| UDA Gain | | |\n",
        "\n",
        "\n",
        "- [ ] Accuracy Gain Discuss \n",
        "- [ ] Why there is no Gain\n",
        "- [ ] What potentially could be done to improve the accuracy\n"
      ],
      "metadata": {
        "id": "1dwDuTVu4Woe"
      },
      "id": "1dwDuTVu4Woe"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.2 Model Selection and Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "Ux13Y3xoEr7t"
      },
      "id": "Ux13Y3xoEr7t"
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "YkkzzowKEr28"
      },
      "id": "YkkzzowKEr28"
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "VDitRu8HErx5"
      },
      "id": "VDitRu8HErx5"
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "9fW0bfScErpv"
      },
      "id": "9fW0bfScErpv"
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "OzKPWGAsErlo"
      },
      "id": "OzKPWGAsErlo"
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "vOBewqboErhT"
      },
      "id": "vOBewqboErhT"
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "nUCxKmFsErdS"
      },
      "id": "nUCxKmFsErdS"
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "TLHcT6fwErZL"
      },
      "id": "TLHcT6fwErZL"
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "XZ1BuPmsErU-"
      },
      "id": "XZ1BuPmsErU-"
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "vrkN23dAErQ5"
      },
      "id": "vrkN23dAErQ5"
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "wKtSYKYoErL6"
      },
      "id": "wKtSYKYoErL6"
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "lBjxfAsNErHN"
      },
      "id": "lBjxfAsNErHN"
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "gxFWEZ1iEq2U"
      },
      "id": "gxFWEZ1iEq2U"
    },
    {
      "cell_type": "markdown",
      "id": "S41pMBv6uy9b",
      "metadata": {
        "id": "S41pMBv6uy9b"
      },
      "source": [
        "### 6.1 Product -> Real Life\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bkflD8rau94r",
      "metadata": {
        "id": "bkflD8rau94r"
      },
      "source": [
        "#### 6.1.1 Purely training on Source Domain-Product\n",
        "\n",
        "Without using any domain adaptation techniques, within 10 epochs training, the classifier network achieves  64.6% accuracy on the target domain."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vxoGKmCJvHqD",
      "metadata": {
        "id": "vxoGKmCJvHqD"
      },
      "source": [
        "#### 6.1.2 Training on both Source and Target Domains\n",
        "\n",
        "Using DANN doamin adaptation technique, the classifier with feature extractor trained on both source and target domain achives 64.6% accuracy, which is the same as the one trained solely on the source domain, with no improvement on accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "QTVdi9zvuysa",
      "metadata": {
        "id": "QTVdi9zvuysa"
      },
      "source": [
        "### 6.2 Real Life -> Product"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_rV09wirvMcM",
      "metadata": {
        "id": "_rV09wirvMcM"
      },
      "source": [
        "#### 6.2.1 Purely Training on Source Domain-Real Life\n",
        "\n",
        "The classifier trained solely on the source domain achives 79.2% accuracy on the target domain."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "VShk9r-GvM2U",
      "metadata": {
        "id": "VShk9r-GvM2U"
      },
      "source": [
        "#### 6.2.2 Training on both Source and Target Domains\n",
        "\n",
        "With feature extractor trained on both source and target domain, the classifer achives 91.9% accuracy on the target domain, which is about 12% improvement over feature extractor purely trained on source domain."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "UDA_GAN.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.0 64-bit ('3.9.0')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "6623cd70e16cbfe1462b5ad5248bf727ae53c1a0c9be44857d7434f2a44555a7"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "006b6f04be3e49aca1a1a6304487bc22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f26b7b7198ac4a2ba2160bb0185710bc",
              "IPY_MODEL_3cfa0650b073425b9db88e950a6375e2",
              "IPY_MODEL_920d754e6f1e4c98a35a2cf7acfdfdec"
            ],
            "layout": "IPY_MODEL_452ca156ac4144b4b9e3cd7a07bdeafd"
          }
        },
        "f26b7b7198ac4a2ba2160bb0185710bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7eba7b50481942449ffde61c040fc878",
            "placeholder": "​",
            "style": "IPY_MODEL_eb174833685340019c0312f0b44239d6",
            "value": "100%"
          }
        },
        "3cfa0650b073425b9db88e950a6375e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cca62b159ef74ae79e4944e0b57d2638",
            "max": 244408911,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d57549fedbe5434ab712cf33ac731bd4",
            "value": 244408911
          }
        },
        "920d754e6f1e4c98a35a2cf7acfdfdec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_61abad4ab57a49a5999f056f4e3542a7",
            "placeholder": "​",
            "style": "IPY_MODEL_2ecd86596c24481dac72f910d7d7a4ec",
            "value": " 233M/233M [00:01&lt;00:00, 158MB/s]"
          }
        },
        "452ca156ac4144b4b9e3cd7a07bdeafd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7eba7b50481942449ffde61c040fc878": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eb174833685340019c0312f0b44239d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cca62b159ef74ae79e4944e0b57d2638": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d57549fedbe5434ab712cf33ac731bd4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "61abad4ab57a49a5999f056f4e3542a7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2ecd86596c24481dac72f910d7d7a4ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}