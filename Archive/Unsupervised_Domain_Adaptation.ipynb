{"cells":[{"cell_type":"markdown","source":["# Unsupervised Domain Adaptation Project\n"],"metadata":{"id":"UOmUNvGYgPRN"},"id":"UOmUNvGYgPRN"},{"cell_type":"markdown","source":["## Part-1: Data download\n","Load data to project from Google Drive. Copy a subset of classes of images to the path:\n","- `adaptiope_small/product_images`\n","- `adaptiope_small/real_life` \n","\n","two directories. They represent images from two different domain **product** and **real_life**"],"metadata":{"id":"ExU_wmMAgVsR"},"id":"ExU_wmMAgVsR"},{"cell_type":"code","execution_count":null,"id":"4134f6cf","metadata":{"id":"4134f6cf","colab":{"base_uri":"https://localhost:8080/"},"outputId":"24b97a7c-9baa-484a-e11d-b9f5b2f4fd66","executionInfo":{"status":"ok","timestamp":1653183100591,"user_tz":-120,"elapsed":189089,"user":{"displayName":"Tianyu Xu","userId":"02171164844935640871"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}],"source":["from os import makedirs, listdir\n","from tqdm import tqdm\n","from google.colab import drive\n","from os.path import join\n","from shutil import copytree\n","\n","drive.mount('/content/gdrive')\n","\n","!mkdir dataset\n","!cp \"gdrive/My Drive/Colab Notebooks/data/Adaptiope.zip\" dataset/\n","# !ls dataset\n","\n","!unzip -qq dataset/Adaptiope.zip   # unzip file\n","\n","!rm -rf adaptiope_small"]},{"cell_type":"code","execution_count":null,"id":"0a6cac67","metadata":{"id":"0a6cac67","colab":{"base_uri":"https://localhost:8080/"},"outputId":"1668f34a-13ea-40b4-8822-6020c3667eb9","executionInfo":{"status":"ok","timestamp":1653183114494,"user_tz":-120,"elapsed":8274,"user":{"displayName":"Tianyu Xu","userId":"02171164844935640871"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["['grill', 'skateboard', 'crown', 'nail clipper', 'notepad', 'quadcopter', 'scooter', 'compass', 'file cabinet', 'rubber boat', 'bottle', 'rc car', 'monitor', 'network switch', 'fighter jet', 'tank', 'hat', 'screwdriver', 'flat iron', 'keyboard', 'toothbrush', 'knife', 'computer mouse', 'pogo stick', 'wristwatch', 'bookcase', 'fan', 'sleeping bag', 'rifle', 'motorbike helmet', 'hair dryer', 'scissors', 'baseball bat', 'corkscrew', 'cordless fixed phone', 'ice cube tray', 'over-ear headphones', 'ruler', 'umbrella', 'snow shovel', 'puncher', 'vr goggles', 'stroller', 'axe', 'magic lamp', 'usb stick', 'ring binder', 'wheelchair', 'pipe wrench', 'hourglass', 'toilet brush', 'game controller', 'helicopter', 'mug', 'tyrannosaurus', 'sewing machine', 'comb', 'roller skates', 'printer', 'smoking pipe', 'stand mixer', 'ladder', 'microwave', 'hoverboard', 'sword', 'hot glue gun', 'cellphone', 'trash can', 'binoculars', 'handcuffs', 'backpack', 'golf club', 'computer', 'car jack', 'syringe', 'shower head', 'glasses', 'lawn mower', 'pikachu', 'hard-wired fixed phone', 'vacuum cleaner', 'power drill', 'acoustic guitar', 'laptop', 'watering can', 'tent', 'stapler', 'pen', 'brachiosaurus', 'chainsaw', 'office chair', 'wallet', 'razor', 'spatula', 'bicycle helmet', 'electric guitar', 'purse', 'tape dispenser', 'desk lamp', 'in-ear headphones', 'speakers', 'boxing gloves', 'telescope', 'fire extinguisher', 'mixing console', 'diving fins', 'coat hanger', 'skeleton', 'stethoscope', 'dart', 'drum set', 'letter tray', 'smartphone', 'phonograph', 'power strip', 'ice skates', 'projector', 'webcam', 'handgun', 'bicycle', 'calculator', 'hand mixer', 'electric shaver']\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 20/20 [00:02<00:00,  7.31it/s]\n","100%|██████████| 20/20 [00:05<00:00,  3.79it/s]\n"]}],"source":["!mkdir adaptiope_small\n","classes = listdir(\"Adaptiope/product_images\")\n","print(classes)\n","classes = [\"backpack\", \"bookcase\", \"car jack\", \"comb\", \"crown\", \"file cabinet\", \"flat iron\", \"game controller\", \"glasses\",\n","           \"helicopter\", \"ice skates\", \"letter tray\", \"monitor\", \"mug\", \"network switch\", \"over-ear headphones\", \"pen\",\n","           \"purse\", \"stand mixer\", \"stroller\"]\n","domain_classes = [\"product_images\", \"real_life\"]\n","for d, td in zip([\"Adaptiope/product_images\", \"Adaptiope/real_life\"], [\"adaptiope_small/product_images\", \"adaptiope_small/real_life\"]):\n","  makedirs(td)\n","  for c in tqdm(classes):\n","    c_path = join(d, c)\n","    c_target = join(td, c)\n","    copytree(c_path, c_target)"]},{"cell_type":"markdown","source":["## Part-2: Image Classification Neural Network\n","\n"," "],"metadata":{"id":"uUHNozN6ggRr"},"id":"uUHNozN6ggRr"},{"cell_type":"markdown","source":["### Part-2.0: Data Loading\n","\n","First we load the data and preprocessing them"],"metadata":{"id":"fwiJOBXWpeS0"},"id":"fwiJOBXWpeS0"},{"cell_type":"code","source":["product_path = 'adaptiope_small/product_images'\n","real_life_path = 'adaptiope_small/real_life'"],"metadata":{"id":"hmth8mlu1oov"},"id":"hmth8mlu1oov","execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pwd\n","!ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ht9bXOHU6zTQ","outputId":"3fb428de-9df2-4ace-bb6c-15ff8318bf56","executionInfo":{"status":"ok","timestamp":1653183119643,"user_tz":-120,"elapsed":584,"user":{"displayName":"Tianyu Xu","userId":"02171164844935640871"}}},"id":"ht9bXOHU6zTQ","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content\n","Adaptiope  adaptiope_small  dataset  gdrive  sample_data\n"]}]},{"cell_type":"code","source":["from PIL import Image\n","from os.path import join\n","\n","img = Image.open(join(product_path, 'backpack', 'backpack_003.jpg'))\n","print('Image size: ', img.size)\n","#img"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_GZxbLlT6O8m","outputId":"3edad831-eb19-4938-e425-99505591d8bb","executionInfo":{"status":"ok","timestamp":1653183120089,"user_tz":-120,"elapsed":3,"user":{"displayName":"Tianyu Xu","userId":"02171164844935640871"}}},"id":"_GZxbLlT6O8m","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Image size:  (679, 679)\n"]}]},{"cell_type":"markdown","source":["import libraries"],"metadata":{"id":"RE2WUwy9BORV"},"id":"RE2WUwy9BORV"},{"cell_type":"code","source":["import torch\n","from torchvision import transforms\n","from torchvision.datasets import ImageFolder\n","from torchvision.models import vgg16, resnet18, resnet34\n","from torch.utils.data import DataLoader, random_split"],"metadata":{"id":"Vei6SzEeggzU"},"id":"Vei6SzEeggzU","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["configuration constants"],"metadata":{"id":"M5rt9x7nBKiT"},"id":"M5rt9x7nBKiT"},{"cell_type":"code","source":["img_size = 256\n","# mean, std used by pre-trained models from PyTorch\n","mean, std = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n","batch_size = 100\n","learning_rate = 0.001\n","num_epochs = 15"],"metadata":{"id":"JXznFSNkAzn3"},"id":"JXznFSNkAzn3","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Configue GPU"],"metadata":{"id":"_Rz4CI9spEkN"},"id":"_Rz4CI9spEkN"},{"cell_type":"code","source":["device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","print(device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nHv2o65FpDpn","outputId":"8ca63800-5d3a-4feb-c3c3-0015f4b0e4d3","executionInfo":{"status":"ok","timestamp":1653183128963,"user_tz":-120,"elapsed":2,"user":{"displayName":"Tianyu Xu","userId":"02171164844935640871"}}},"id":"nHv2o65FpDpn","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["cuda:0\n"]}]},{"cell_type":"code","source":["from torchvision.transforms.transforms import ToTensor\n","\n","def get_dataset(root_path):\n","  '''\n","    Get dataset from specific data path\n","\n","    # parameters:\n","        root_path: path to image folder\n","\n","    # return: train_loader, test_loader\n","  '''\n","  # Construct image transform\n","  image_transform = transforms.Compose([\n","    transforms.Resize(img_size),\n","    transforms.CenterCrop(img_size),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean, std)\n","  ])\n","\n","  # Load data from filesystem\n","  image_dataset = ImageFolder(root_path, transform=image_transform)\n","\n","  return image_dataset\n","\n","def get_dataloader(dataset, batch_size, shuffle_train=True, shuffle_test=False):\n","  '''\n","    Get DataLoader from specific data path\n","\n","    # parameters:\n","        dataset: ImageFolder instance\n","        batch_size: batch_size for DataLoader\n","        shuffle_train: whether to shuffle training data\n","        shuffle_test: whether to shuffle test data\n","  '''\n","  # Get train, test number\n","  num_total = len(dataset)\n","  num_train = int(num_total * 0.8 + 1)\n","  num_test  = num_total - num_train\n","\n","  # random split dataset\n","  data_train, data_test = random_split(dataset, [num_train, num_test])\n","\n","  # initialize dataloaders\n","  loader_train = DataLoader(data_train, batch_size=batch_size, shuffle=shuffle_train)\n","  loader_test  = DataLoader(data_test, batch_size=batch_size, shuffle=shuffle_test)\n","\n","  return loader_train, loader_test"],"metadata":{"id":"GF3YyTBAhJk8"},"id":"GF3YyTBAhJk8","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Part-2.1 Pretrain Network\n","\n","Here we use a pretrain Neural Network to start with, then we fine tune it with the data set we have from **Adaptiope** in one domain, and test it on the target domain. Compare the two result, and set the benchmark for later UDA enriched method. "],"metadata":{"id":"-J4MSc3spcYU"},"id":"-J4MSc3spcYU"},{"cell_type":"code","source":["pd_dataset = get_dataset(product_path)\n","len(pd_dataset.classes)"],"metadata":{"id":"ijDPD_jrvlz_","colab":{"base_uri":"https://localhost:8080/"},"outputId":"c4fc5e93-8ff4-423a-8788-947094b458de","executionInfo":{"status":"ok","timestamp":1653183133439,"user_tz":-120,"elapsed":3,"user":{"displayName":"Tianyu Xu","userId":"02171164844935640871"}}},"id":"ijDPD_jrvlz_","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["20"]},"metadata":{},"execution_count":11}]},{"cell_type":"markdown","source":["### Part-2.2 Define the Deep Residual Network"],"metadata":{"id":"ZjWy3ak98L0F"},"id":"ZjWy3ak98L0F"},{"cell_type":"code","source":["def initialize_model(num_classes, model_type=\"ResNet\"):\n","  if model_type.startswith(\"ResNet\"):\n","    model = resnet18(pretrained=True)\n","    in_features = model.fc.in_features\n","    model.fc = torch.nn.Linear(in_features=in_features, out_features=num_classes)\n","  else:\n","    model = vgg16(pretrained=True)\n","    in_features = model.classifier[-1].in_features\n","    model.classifier[-1] = torch.nn.Linear(in_features=in_features, out_features=num_classes)\n","\n","  return model"],"metadata":{"id":"JfloSw5b2jVe"},"id":"JfloSw5b2jVe","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# model = initialize_model(20, \"vgg\")\n","# count  = 0\n","# for name, param in model.named_parameters():\n","#   # if name.startswith('fc'):\n","#   # print(name) \n","#   count += 1\n","# print(count)\n","\n","# print(model.__class__.__name__)"],"metadata":{"id":"Smgp5vnehJWW"},"id":"Smgp5vnehJWW","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Part-2.3 Cost function\n","\n","Divide parameters intro two groups, in which the last fully conneted layer with learning_rate, the other layers with 0.1 * learning_rate."],"metadata":{"id":"IhnsX7lq5saz"},"id":"IhnsX7lq5saz"},{"cell_type":"code","source":["def get_cost_function():\n","  return torch.nn.CrossEntropyLoss()"],"metadata":{"id":"q6BkiCH_5sOr"},"id":"q6BkiCH_5sOr","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Part-2.4 Optimizer"],"metadata":{"id":"B-y0dTPj5zrR"},"id":"B-y0dTPj5zrR"},{"cell_type":"code","source":["def get_optimizer(model, learning_rate, weight_decay, momentum):\n","\n","  # Get model name\n","  model_name = model.__class__.__name__\n","\n","  # define final layer name by different model\n","  if model_name == \"ResNet\":\n","    final_layer_name = \"fc\"\n","  elif model_name == \"VGG\":\n","    final_layer_name = \"classifier.6\"\n","  else:\n","    raise Exception(f'## GET_OPTIMIZER ## - Undefined Model Type {model_name}')\n","\n","  pre_trained_weights = []\n","  final_layer_weights = []\n","\n","  # get all the parameters required gradient updates\n","  for name, param in model.named_parameters():\n","    if param.requires_grad == True:\n","      if name.startswith(final_layer_name):\n","        final_layer_weights.append(param)\n","      else:\n","        pre_trained_weights.append(param)\n","\n","  # assign parameters to parameters\n","  optimizer = torch.optim.SGD([\n","    {'params': pre_trained_weights},\n","    {'params': final_layer_weights, 'lr': learning_rate}\n","  ], lr= learning_rate/10, weight_decay=weight_decay, momentum=momentum)\n","  \n","  return optimizer"],"metadata":{"id":"zKRb56iY5sGt"},"id":"zKRb56iY5sGt","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Part-2.5 Training and Testing Step"],"metadata":{"id":"Atdanl9REs3F"},"id":"Atdanl9REs3F"},{"cell_type":"code","source":["def train_loop(dataloader, model, loss_fn, optimizer, device):\n","  size = len(dataloader.dataset)\n","\n","  for batch, (X, y) in enumerate(dataloader):\n","    X, y = X.to(device), y.to(device)\n","    \n","    # compute prediction and loss\n","    predicts = model(X)\n","    loss = loss_fn(predicts, y)\n","\n","    # backpropagation\n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()\n","\n","    if batch % 100 == 0:\n","      loss, current = loss.item(), batch * len(X)\n","      print(f\"loss: {loss:>7f} [{current:>5d}/{size:>5d}]\")"],"metadata":{"id":"ORDqPkiT5r1s"},"id":"ORDqPkiT5r1s","execution_count":null,"outputs":[]},{"cell_type":"code","source":["def test_loop(dataloader, model, loss_fn, device):\n","  test_loss, correct = 0, 0\n","\n","  with torch.no_grad():\n","    for X, y in dataloader:\n","      X, y = X.to(device), y.to(device)\n","      predicts = model(X)\n","      test_loss += loss_fn(predicts, y).item()\n","      correct += (predicts.argmax(1) == y).type(torch.float).sum().item()\n","\n","  size = len(dataloader.dataset)\n","  num_batches = len(dataloader)\n","\n","  test_loss /= num_batches\n","  correct /= size\n","  print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n","\n","  return test_loss, correct"],"metadata":{"id":"LlvQLNUDLGJF"},"id":"LlvQLNUDLGJF","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Part-2.6 Training"],"metadata":{"id":"z5uhItnwhj8S"},"id":"z5uhItnwhj8S"},{"cell_type":"code","source":["def training(model, train_dataloader, test_dataloader, device, epochs=10, lr=0.001, wd=0.001, momentum=0.9):\n","  print(f\"Learning_rate {lr}, weight_decay {wd}\")\n","  loss_fn = get_cost_function()\n","  optimizer = get_optimizer(model, lr, wd, momentum)\n","\n","  for epoch in range(epochs):\n","    print(f\"Epoch {epoch+1}\\n------------------\")\n","    train_loop(train_dataloader, model, loss_fn, optimizer, device)\n","    test_loop(test_dataloader, model, loss_fn, device)\n","\n","  print(\"Done\")"],"metadata":{"id":"nM76Syqahknd"},"id":"nM76Syqahknd","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Get train_dataloader, test_dataloader\n","dataset = get_dataset(product_path)\n","train_dataloader, test_dataloader = get_dataloader(dataset, 50)\n","\n","# Get model\n","model = initialize_model(len(dataset.classes)).to(device)\n","\n","# Training\n","training(model, train_dataloader, test_dataloader, device, num_epochs, learning_rate)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V93UE4rXi3C9","outputId":"2a6e8bb9-01d6-4fbf-ab9e-e2c403c56689","executionInfo":{"status":"ok","timestamp":1653186776527,"user_tz":-120,"elapsed":585560,"user":{"displayName":"Tianyu Xu","userId":"02171164844935640871"}}},"id":"V93UE4rXi3C9","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Learning_rate 0.001, weight_decay 0.001\n","Epoch 1\n","------------------\n","loss: 3.031182 [    0/ 1601]\n","Test Error: \n"," Accuracy: 52.1%, Avg loss: 2.268782 \n","\n","Epoch 2\n","------------------\n","loss: 2.164959 [    0/ 1601]\n","Test Error: \n"," Accuracy: 78.2%, Avg loss: 1.468856 \n","\n","Epoch 3\n","------------------\n","loss: 1.253728 [    0/ 1601]\n","Test Error: \n"," Accuracy: 88.0%, Avg loss: 0.990233 \n","\n","Epoch 4\n","------------------\n","loss: 0.934313 [    0/ 1601]\n","Test Error: \n"," Accuracy: 90.0%, Avg loss: 0.746172 \n","\n","Epoch 5\n","------------------\n","loss: 0.645032 [    0/ 1601]\n","Test Error: \n"," Accuracy: 92.7%, Avg loss: 0.588191 \n","\n","Epoch 6\n","------------------\n","loss: 0.566190 [    0/ 1601]\n","Test Error: \n"," Accuracy: 93.7%, Avg loss: 0.493843 \n","\n","Epoch 7\n","------------------\n","loss: 0.362208 [    0/ 1601]\n","Test Error: \n"," Accuracy: 94.2%, Avg loss: 0.429325 \n","\n","Epoch 8\n","------------------\n","loss: 0.254228 [    0/ 1601]\n","Test Error: \n"," Accuracy: 93.2%, Avg loss: 0.396686 \n","\n","Epoch 9\n","------------------\n","loss: 0.237645 [    0/ 1601]\n","Test Error: \n"," Accuracy: 95.0%, Avg loss: 0.346476 \n","\n","Epoch 10\n","------------------\n","loss: 0.267836 [    0/ 1601]\n","Test Error: \n"," Accuracy: 93.7%, Avg loss: 0.328436 \n","\n","Epoch 11\n","------------------\n","loss: 0.216011 [    0/ 1601]\n","Test Error: \n"," Accuracy: 95.7%, Avg loss: 0.307842 \n","\n","Epoch 12\n","------------------\n","loss: 0.233881 [    0/ 1601]\n","Test Error: \n"," Accuracy: 94.5%, Avg loss: 0.290659 \n","\n","Epoch 13\n","------------------\n","loss: 0.186744 [    0/ 1601]\n","Test Error: \n"," Accuracy: 95.2%, Avg loss: 0.267583 \n","\n","Epoch 14\n","------------------\n","loss: 0.152284 [    0/ 1601]\n","Test Error: \n"," Accuracy: 95.0%, Avg loss: 0.250017 \n","\n","Epoch 15\n","------------------\n","loss: 0.136662 [    0/ 1601]\n","Test Error: \n"," Accuracy: 95.2%, Avg loss: 0.245470 \n","\n","Done\n"]}]},{"cell_type":"code","source":["torch.save(model.state_dict(), 'model_state.pt')"],"metadata":{"id":"HfgePVF1npNW"},"id":"HfgePVF1npNW","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Part-2.7 Testing on Target Domain\n","#### Apply the model trained on the source domain directly to the target domain. This result will be used for comparison with the results obtained after domain adaptation."],"metadata":{"id":"05P25psLS27W"},"id":"05P25psLS27W"},{"cell_type":"code","source":["target_dataset = get_dataset(real_life_path)\n","loader_target_dataset = DataLoader(target_dataset, batch_size=100, shuffle=False)\n","\n","# model.load_state_dict(torch.load('model_state.pt', map_location='cpu'))\n","loss_fn = get_cost_function()\n","test_loop(loader_target_dataset, model, loss_fn, device)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E54Vm3jmTRcc","executionInfo":{"status":"ok","timestamp":1653190457800,"user_tz":-120,"elapsed":69605,"user":{"displayName":"Tianyu Xu","userId":"02171164844935640871"}},"outputId":"94fc09ba-6d35-42b8-9b5e-d93335b2f205"},"id":"E54Vm3jmTRcc","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Test Error: \n"," Accuracy: 9.6%, Avg loss: 3.616436 \n","\n"]},{"output_type":"execute_result","data":{"text/plain":["(3.616436266899109, 0.096)"]},"metadata":{},"execution_count":41}]},{"cell_type":"markdown","source":["## TODO\n","\n","### TODO: Dataset unzip Google Drive, Copy to folder\n","\n","TODO: Batch progress number error\n","\n","Otherwise Continue UDA"],"metadata":{"id":"kTUMJfmMw-tQ"},"id":"kTUMJfmMw-tQ"},{"cell_type":"markdown","source":["## Part-3: UDA \n","\n","Here we use Contrastive Domain Adaptation method proposed [here]().\n","We train the previous network and run the test on both Source Domain and Target Domain. "],"metadata":{"id":"7q7jwnwwhRz2"},"id":"7q7jwnwwhRz2"},{"cell_type":"code","source":["#DANN\n","import numpy as np\n","\n","dataloader_source = DataLoader(dataset, batch_size=100, shuffle=True)\n","dataloader_target = DataLoader(target_dataset, batch_size=100, shuffle=False)\n","\n","n_epoch = 15\n","for epoch in range(n_epoch):\n","\n","    len_dataloader = min(len(dataloader_source), len(dataloader_target))\n","    # print(len_dataloader)\n","    data_source_iter = iter(dataloader_source)\n","    data_target_iter = iter(dataloader_target)\n","    print('train start')\n","\n","    for i in range(0, len_dataloader):\n","        p = float(i + epoch * len_dataloader) / n_epoch / len_dataloader\n","        alpha = 2. / (1. + np.exp(-10 * p)) - 1\n","\n","        # training model using source data\n","        data_source = data_source_iter.next()\n","        s_img, s_label = data_source\n","\n","        batch_size = len(s_label)\n","        # 0代表源域\n","        domain_label = torch.zeros(batch_size).long()"],"metadata":{"id":"psgAJw-YhZGJ"},"id":"psgAJw-YhZGJ","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"zKscrnYohp7k"},"id":"zKscrnYohp7k","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"hGZqRFWfhpx2"},"id":"hGZqRFWfhpx2","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Part-4: Comparison & Discussion\n","Here we compare the test result from the direct method and the UDA method. "],"metadata":{"id":"CKsMcxgihquu"},"id":"CKsMcxgihquu"},{"cell_type":"code","source":[""],"metadata":{"id":"6C0fg_GZh4ou"},"id":"6C0fg_GZh4ou","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"lcDs1PTfh9G9"},"id":"lcDs1PTfh9G9","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Part-5: Conclusion"],"metadata":{"id":"u3VyXbyth9sO"},"id":"u3VyXbyth9sO"},{"cell_type":"code","source":[""],"metadata":{"id":"xgSBA7eMh-Nk"},"id":"xgSBA7eMh-Nk","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.9"},"colab":{"name":"Unsupervised_Domain_Adaptation.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}