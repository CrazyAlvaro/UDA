{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "UOmUNvGYgPRN",
      "metadata": {
        "id": "UOmUNvGYgPRN"
      },
      "source": [
        "# Unsupervised Domain Adaptation Project\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0: Running Instruction\n",
        "\n",
        "This Notebook is supporsed to be **run by order**.\n",
        "\n",
        "- Running Parameter configuration can be modified [here](#configuration).\n",
        "\n",
        "- DANN Model are defined [here](#DANN)\n",
        "\n",
        "- Source Only Training Starts [here](#source)\n",
        "\n",
        "- UDA functions defined [here](#UDA) while UDA training starts [here](#UDA_training)\n",
        "\n",
        "- UDA Mid define and runs [here](#UDA_mid)\n",
        "\n",
        "- Summary is [here](#summary)"
      ],
      "metadata": {
        "id": "KR33XKApdIV6"
      },
      "id": "KR33XKApdIV6"
    },
    {
      "cell_type": "markdown",
      "id": "ExU_wmMAgVsR",
      "metadata": {
        "id": "ExU_wmMAgVsR"
      },
      "source": [
        "## 1: Data download\n",
        "Load data to project from Google Drive. Copy a subset of classes of images to the path:\n",
        "- `adaptiope_small/product_images`\n",
        "- `adaptiope_small/real_life` \n",
        "\n",
        "two directories. They represent images from two different domain **product** and **real_life**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "4134f6cf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4134f6cf",
        "outputId": "520e8b65-5ee6-4978-880d-de38eeddd92b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from os import makedirs, listdir\n",
        "from tqdm import tqdm\n",
        "from google.colab import drive\n",
        "from os.path import join\n",
        "from shutil import copytree\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "!mkdir dataset\n",
        "!cp \"gdrive/My Drive/Colab Notebooks/data/Adaptiope.zip\" dataset/\n",
        "# !ls dataset\n",
        "\n",
        "!unzip -qq dataset/Adaptiope.zip   # unzip file\n",
        "\n",
        "!rm -rf dataset/Adaptiope.zip \n",
        "!rm -rf adaptiope_small"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "0a6cac67",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0a6cac67",
        "outputId": "bcd62efd-9833-41ca-b2bc-b3c38af824d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['tape dispenser', 'comb', 'car jack', 'toothbrush', 'handcuffs', 'magic lamp', 'usb stick', 'glasses', 'tank', 'computer', 'rubber boat', 'shower head', 'telescope', 'quadcopter', 'scooter', 'tyrannosaurus', 'rc car', 'syringe', 'cordless fixed phone', 'bottle', 'vr goggles', 'razor', 'helicopter', 'hat', 'dart', 'compass', 'hoverboard', 'corkscrew', 'projector', 'file cabinet', 'smoking pipe', 'rifle', 'mug', 'fan', 'sewing machine', 'pen', 'keyboard', 'knife', 'trash can', 'tent', 'drum set', 'nail clipper', 'phonograph', 'monitor', 'toilet brush', 'skateboard', 'electric guitar', 'screwdriver', 'coat hanger', 'speakers', 'boxing gloves', 'roller skates', 'computer mouse', 'ladder', 'motorbike helmet', 'scissors', 'handgun', 'power strip', 'ruler', 'microwave', 'golf club', 'stapler', 'watering can', 'over-ear headphones', 'umbrella', 'pipe wrench', 'vacuum cleaner', 'purse', 'in-ear headphones', 'webcam', 'pikachu', 'letter tray', 'chainsaw', 'ice cube tray', 'fighter jet', 'grill', 'power drill', 'sleeping bag', 'crown', 'game controller', 'calculator', 'bookcase', 'pogo stick', 'printer', 'wristwatch', 'baseball bat', 'wallet', 'sword', 'lawn mower', 'notepad', 'fire extinguisher', 'hair dryer', 'ring binder', 'stroller', 'diving fins', 'network switch', 'stethoscope', 'snow shovel', 'spatula', 'wheelchair', 'electric shaver', 'smartphone', 'desk lamp', 'puncher', 'hot glue gun', 'stand mixer', 'laptop', 'cellphone', 'ice skates', 'hand mixer', 'axe', 'acoustic guitar', 'office chair', 'flat iron', 'binoculars', 'skeleton', 'brachiosaurus', 'mixing console', 'backpack', 'bicycle helmet', 'bicycle', 'hard-wired fixed phone', 'hourglass']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20/20 [00:00<00:00, 20.13it/s]\n",
            "100%|██████████| 20/20 [00:00<00:00, 29.37it/s]\n"
          ]
        }
      ],
      "source": [
        "!mkdir adaptiope_small\n",
        "classes = listdir(\"Adaptiope/product_images\")\n",
        "print(classes)\n",
        "classes = [\"backpack\", \"bookcase\", \"car jack\", \"comb\", \"crown\", \"file cabinet\", \"flat iron\", \"game controller\", \"glasses\",\n",
        "           \"helicopter\", \"ice skates\", \"letter tray\", \"monitor\", \"mug\", \"network switch\", \"over-ear headphones\", \"pen\",\n",
        "           \"purse\", \"stand mixer\", \"stroller\"]\n",
        "domain_classes = [\"product_images\", \"real_life\"]\n",
        "for d, td in zip([\"Adaptiope/product_images\", \"Adaptiope/real_life\"], [\"adaptiope_small/product_images\", \"adaptiope_small/real_life\"]):\n",
        "  makedirs(td)\n",
        "  for c in tqdm(classes):\n",
        "    c_path = join(d, c)\n",
        "    c_target = join(td, c)\n",
        "    copytree(c_path, c_target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "S8yOthKskBmC",
      "metadata": {
        "id": "S8yOthKskBmC"
      },
      "outputs": [],
      "source": [
        "product_path = 'adaptiope_small/product_images'\n",
        "real_life_path = 'adaptiope_small/real_life'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "uUHNozN6ggRr",
      "metadata": {
        "id": "uUHNozN6ggRr"
      },
      "source": [
        "<a name=\"DANN\">\n",
        "</a>\n",
        "\n",
        "## 2: Domain-Adversarial training of Neural Network \n",
        "\n",
        "We implement DANN UDA method [DANN](https://arxiv.org/pdf/1505.07818.pdf)  \n",
        "\n",
        "![DANN.png](https://raw.githubusercontent.com/CrazyAlvaro/UDA/main/images/DANN.png)\n",
        "\n",
        "As displayed in the model architecture above, DANN is consist of three component: feature extractor, domain classifier, and label predictor(classifier).\n",
        "\n",
        "While in order to adversarial training from both label predictor and domain classifier, a gradient reversal layer(GRL) is added."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fwiJOBXWpeS0",
      "metadata": {
        "id": "fwiJOBXWpeS0"
      },
      "source": [
        "### 2.0: Import Libraries and Data Loading\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "_GZxbLlT6O8m",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_GZxbLlT6O8m",
        "outputId": "439e4e08-b054-4fb1-8743-c62f8f9a3e1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image size:  (679, 679)\n"
          ]
        }
      ],
      "source": [
        "from PIL import Image\n",
        "from os.path import join\n",
        "import math\n",
        "\n",
        "img = Image.open(join(product_path, 'backpack', 'backpack_003.jpg'))\n",
        "print('Image size: ', img.size)\n",
        "#img"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "RE2WUwy9BORV",
      "metadata": {
        "id": "RE2WUwy9BORV"
      },
      "source": [
        "Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "Vei6SzEeggzU",
      "metadata": {
        "id": "Vei6SzEeggzU"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import softmax\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision.models import vgg11, alexnet \n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision.transforms.transforms import ToTensor"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "M5rt9x7nBKiT",
      "metadata": {
        "id": "M5rt9x7nBKiT"
      },
      "source": [
        "Config Parameters\n",
        "<a name=\"configuration\">\n",
        "</a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "id": "JXznFSNkAzn3",
      "metadata": {
        "id": "JXznFSNkAzn3"
      },
      "outputs": [],
      "source": [
        "img_size = 256\n",
        "# mean, std used by pre-trained models from PyTorch\n",
        "mean, std = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n",
        "config = dict(epochs=10, batch_size=64,lr=0.01, wd=0.001, momentum=0.9, alpha=10, beta=0.75, gamma=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_Rz4CI9spEkN",
      "metadata": {
        "id": "_Rz4CI9spEkN"
      },
      "source": [
        "Configue GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "nHv2o65FpDpn",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nHv2o65FpDpn",
        "outputId": "1d12e464-ebc1-467b-e2c9-da4b90a395bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "GF3YyTBAhJk8",
      "metadata": {
        "id": "GF3YyTBAhJk8"
      },
      "outputs": [],
      "source": [
        "def get_dataset(root_path):\n",
        "  '''\n",
        "    Get dataset from specific data path\n",
        "\n",
        "    # parameters:\n",
        "        root_path: path to image folder\n",
        "\n",
        "    # return: train_loader, test_loader\n",
        "  '''\n",
        "  # Construct image transform\n",
        "  image_transform = transforms.Compose([\n",
        "    transforms.Resize(img_size),\n",
        "    transforms.CenterCrop(img_size),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std)\n",
        "  ])\n",
        "\n",
        "  # Load data from filesystem\n",
        "  image_dataset = ImageFolder(root_path, transform=image_transform)\n",
        "\n",
        "  return image_dataset\n",
        "\n",
        "def get_dataloader(dataset, batch_size, shuffle_train=True, shuffle_test=False):\n",
        "  '''\n",
        "    Get DataLoader from specific data path\n",
        "\n",
        "    # parameters:\n",
        "        dataset: ImageFolder instance\n",
        "        batch_size: batch_size for DataLoader\n",
        "        shuffle_train: whether to shuffle training data\n",
        "        shuffle_test: whether to shuffle test data\n",
        "  '''\n",
        "  # Get train, test number\n",
        "  num_total = len(dataset)\n",
        "  num_train = int(num_total * 0.8 + 1)\n",
        "  num_test  = num_total - num_train\n",
        "\n",
        "  # random split dataset\n",
        "  data_train, data_test = random_split(dataset, [num_train, num_test])\n",
        "\n",
        "  # initialize dataloaders\n",
        "  loader_train = DataLoader(data_train, batch_size=batch_size, shuffle=shuffle_train)\n",
        "  loader_test  = DataLoader(data_test, batch_size=batch_size, shuffle=shuffle_test)\n",
        "\n",
        "  return loader_train, loader_test"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-J4MSc3spcYU",
      "metadata": {
        "id": "-J4MSc3spcYU"
      },
      "source": [
        "### 2.1 Define Feature Extractor with Pretrain Network"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the feature extractor, we select pretrained AlexNet. \n",
        "The reason to choose AlexNet comparing more recent Network like \n",
        "Residural Network is because it has a good balance between model \n",
        "performance and traning complexity. Even though ResNet may perform \n",
        "better than AlexNet by a reasonable amount of gain, it takes way much longer \n",
        "to train or tune a complex network like this, which will dramatically increase the training time."
      ],
      "metadata": {
        "id": "EKH8Vkprl0_6"
      },
      "id": "EKH8Vkprl0_6"
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "id": "95ff2db4",
      "metadata": {
        "id": "95ff2db4"
      },
      "outputs": [],
      "source": [
        "class FeatureExtractor(nn.Module):\n",
        "  \"\"\"\n",
        "  FeatureExtractor\n",
        "\n",
        "  Pretrained neural network as a backbone for later domain adaptation task\n",
        "  \"\"\"\n",
        "  def __init__(self):\n",
        "    super(FeatureExtractor, self).__init__()\n",
        "\n",
        "    # Feature Extractor with AlexNet\n",
        "    self.feature_extractor = alexnet(weights='DEFAULT')\n",
        "    self.feature_dim = self.feature_extractor.classifier[-1].in_features\n",
        "\n",
        "    # make the last layer identity\n",
        "    self.feature_extractor.classifier[-1] = nn.Identity()\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.feature_extractor(x)\n",
        "  \n",
        "  def output_dim(self):\n",
        "    return self.feature_dim"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ZjWy3ak98L0F",
      "metadata": {
        "id": "ZjWy3ak98L0F"
      },
      "source": [
        "### 2.2 Define Classifier, Discriminator with RevereLayerF for training the Feature Extractor"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the classifier, we implement a three fully connected linear layer \n",
        "with LeakyReLU because of its advantage over regular ReLU activation function. \n",
        "And finally we use Logrithm Softmax function as the selection layer."
      ],
      "metadata": {
        "id": "nmxb5tQgl7l7"
      },
      "id": "nmxb5tQgl7l7"
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "id": "517118c3",
      "metadata": {
        "id": "517118c3"
      },
      "outputs": [],
      "source": [
        "class Classifier(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(Classifier, self).__init__()\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(input_dim, 1024),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(512, output_dim),\n",
        "            nn.LogSoftmax(dim=1)\n",
        "        )\n",
        "    \n",
        "    def forward(self, X):\n",
        "        return self.classifier(X) "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we implement a ReverseLayer Function to pass thourgh the data \n",
        "as it is without doing any compuation, but on backward propagation, \n",
        "it will reverse the sign of the value to provide the capability \n",
        "to adversarial training from the later Discriminator."
      ],
      "metadata": {
        "id": "78iO-tGgmC5W"
      },
      "id": "78iO-tGgmC5W"
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "id": "RES6EY4PO7KF",
      "metadata": {
        "id": "RES6EY4PO7KF"
      },
      "outputs": [],
      "source": [
        "from torch.autograd import Function\n",
        "\n",
        "class ReverseLayerF(Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, tensor): \n",
        "        \"\"\"\n",
        "        Without doing any computation\n",
        "        \"\"\"\n",
        "        return tensor.view_as(tensor)\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        \"\"\"\n",
        "        Change the sign of the gradient \n",
        "        \"\"\"\n",
        "        return grad_output.neg(), None"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here for the discriminator, we only implement a two-layer linear connection here to avoid overly complicate the Discriminator, becuase usually more complex discriminator will have a negative effect on adversarial training."
      ],
      "metadata": {
        "id": "VTY_pZSqmKlN"
      },
      "id": "VTY_pZSqmKlN"
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "id": "K9syaPhFxOFd",
      "metadata": {
        "id": "K9syaPhFxOFd"
      },
      "outputs": [],
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.discriminator =  nn.Sequential(\n",
        "            nn.Linear(int(input_dim), 1024),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(1024,1),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        validity = self.discriminator(x)\n",
        "        return validity "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally we connect the feature extractor, classifer, and discriminator to form a \n",
        "Domain Adversarial Neural Network. Both classifier and discriminator will take in \n",
        "data from the fearture extractor processed the imput images. The classifer take the \n",
        "number of classes as input, the output is the prediction of which class of the current \n",
        "image belong. While the discriminator are supposed to distinguish the image from two \n",
        "different domains.\n",
        "\n",
        "The discriminator will serve as a doamin alignment unit to train the feature extractor \n",
        "to extract domain independent features from both domains."
      ],
      "metadata": {
        "id": "HMQI0iuzmNPD"
      },
      "id": "HMQI0iuzmNPD"
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "id": "BsbwoZkZwjjl",
      "metadata": {
        "id": "BsbwoZkZwjjl"
      },
      "outputs": [],
      "source": [
        "class DANN(nn.Module):\n",
        "  \"\"\" \n",
        "  DANN\n",
        "\n",
        "  Implement the domain adversarial neural network that train the feature extractor from both classification and discrimination of \n",
        "  different domains.\n",
        "  \"\"\"\n",
        "  def __init__(self, num_classes):\n",
        "    \"\"\" \n",
        "    Parameter:\n",
        "      @num_classes: number of classes of different images\n",
        "    \"\"\"\n",
        "    super(DANN, self).__init__()\n",
        "    self.output_dim = num_classes\n",
        "\n",
        "    # define inner network component\n",
        "    self.feature_extractor = FeatureExtractor()\n",
        "    self.classifier = Classifier(self.feature_extractor.output_dim(), num_classes)\n",
        "    self.discriminator = Discriminator(self.feature_extractor.output_dim())  \n",
        "  \n",
        "  def forward(self, x):\n",
        "    feature_output = self.feature_extractor(x)\n",
        "\n",
        "    class_pred = self.classifier(feature_output)\n",
        "\n",
        "    # Add a ReverseLayer here for negative gradient computation\n",
        "    reverse_feature = ReverseLayerF.apply(feature_output)\n",
        "    domain_pred = self.discriminator(reverse_feature)\n",
        "\n",
        "    return class_pred, domain_pred "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "IhnsX7lq5saz",
      "metadata": {
        "id": "IhnsX7lq5saz"
      },
      "source": [
        "### 2.3 Cost function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "id": "2uZKaSA3nd7t",
      "metadata": {
        "id": "2uZKaSA3nd7t"
      },
      "outputs": [],
      "source": [
        "class BinaryDiceLoss(nn.Module):\n",
        "    \"\"\"Dice loss of binary class\n",
        "    Args:\n",
        "        smooth: A float number to smooth loss, and avoid NaN error, default: 1\n",
        "        p: Denominator value: \\sum{x^p} + \\sum{y^p}, default: 2\n",
        "        predict: A tensor of shape [N, *]\n",
        "        target: A tensor of shape same with predict\n",
        "        reduction: Reduction method to apply, return mean over batch if 'mean',\n",
        "            return sum if 'sum', return a tensor of shape [N,] if 'none'\n",
        "    Returns:\n",
        "        Loss tensor according to arg reduction\n",
        "    Raise:\n",
        "        Exception if unexpected reduction\n",
        "    \"\"\"\n",
        "    def __init__(self, smooth=1, p=2, reduction='mean'):\n",
        "        super(BinaryDiceLoss, self).__init__()\n",
        "        self.smooth = smooth\n",
        "        self.p = p\n",
        "        self.reduction = reduction\n",
        "\n",
        "    def forward(self, predict, target):\n",
        "        assert predict.shape[0] == target.shape[0], \"predict & target batch size don't match\"\n",
        "        predict = predict.contiguous().view(predict.shape[0], -1)\n",
        "        target = target.contiguous().view(target.shape[0], -1)\n",
        "\n",
        "        num = torch.sum(torch.mul(predict, target), dim=1) + self.smooth\n",
        "        den = torch.sum(predict.pow(self.p) + target.pow(self.p), dim=1) + self.smooth\n",
        "\n",
        "        loss = 1 - num / den\n",
        "\n",
        "        if self.reduction == 'mean':\n",
        "            return loss.mean()\n",
        "        elif self.reduction == 'sum':\n",
        "            return loss.sum()\n",
        "        elif self.reduction == 'none':\n",
        "            return loss\n",
        "        else:\n",
        "            raise Exception('Unexpected reduction {}'.format(self.reduction))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "id": "N0-rxY3rnh6J",
      "metadata": {
        "id": "N0-rxY3rnh6J"
      },
      "outputs": [],
      "source": [
        "class DiceLoss(nn.Module):\n",
        "    \"\"\"Dice loss, need one hot encode input\n",
        "    Args:\n",
        "        weight: An array of shape [num_classes,]\n",
        "        ignore_index: class index to ignore\n",
        "        predict: A tensor of shape [N, C, *]\n",
        "        target: A tensor of same shape with predict\n",
        "        other args pass to BinaryDiceLoss\n",
        "    Return:\n",
        "        same as BinaryDiceLoss\n",
        "    \"\"\"\n",
        "    def __init__(self, weight=None, ignore_index=None, **kwargs):\n",
        "        super(DiceLoss, self).__init__()\n",
        "        self.kwargs = kwargs\n",
        "        self.weight = weight\n",
        "        self.ignore_index = ignore_index\n",
        "\n",
        "    def forward(self, predict, target):\n",
        "        # one hot encode input\n",
        "        num_class = predict.shape[1]\n",
        "        # one hot\n",
        "        target = F.one_hot(target, num_classes=num_class)\n",
        "        \n",
        "        assert predict.shape == target.shape, 'predict & target shape do not match'\n",
        "        dice = BinaryDiceLoss(**self.kwargs)\n",
        "        total_loss = 0\n",
        "        predict = F.softmax(predict, dim=1)\n",
        "\n",
        "        for i in range(target.shape[1]):\n",
        "            if i != self.ignore_index:\n",
        "                dice_loss = dice(predict[:, i], target[:, i])\n",
        "                if self.weight is not None:\n",
        "                    assert self.weight.shape[0] == target.shape[1], \\\n",
        "                        'Expect weight shape [{}], get[{}]'.format(target.shape[1], self.weight.shape[0])\n",
        "                    dice_loss *= self.weights[i]\n",
        "                total_loss += dice_loss\n",
        "\n",
        "        return total_loss/target.shape[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this experiment, two loss functions, dice loss and cross entropy, were selected for comparison in the classifier."
      ],
      "metadata": {
        "id": "8GgOnWlIvQfO"
      },
      "id": "8GgOnWlIvQfO"
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "id": "q6BkiCH_5sOr",
      "metadata": {
        "id": "q6BkiCH_5sOr"
      },
      "outputs": [],
      "source": [
        "def get_class_loss_func(dice_loss=False):\n",
        "  if dice_loss:\n",
        "    print(\"## Loss Function ## dice_loss being used.\")\n",
        "    return DiceLoss()\n",
        "  else:\n",
        "    print(\"## Loss Function ## CrossEntropyLoss being used.\")\n",
        "    return nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "B-y0dTPj5zrR",
      "metadata": {
        "id": "B-y0dTPj5zrR"
      },
      "source": [
        "### 2.4 Optimizer\n",
        "\n",
        "Setting the **learning rate** according to the original [paper](https://arxiv.org/pdf/1505.07818.pdf) section 5.2.2\n",
        "\n",
        "$$ \\mu_p =  \\frac{\\mu_0}{(1+\\alpha \\cdot p)^\\beta}$$\n",
        "\n",
        "where p is the training progress linearly changing from 0 to 1.\n",
        "\n",
        "And for the learning rate, for the pretrain weights, we set the learning rate only to be 1/10 \n",
        "of the learning rate for the classifier. And we use Stochastic Gradient Descent to optimize the \n",
        "model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "id": "ZsoNnQCq2aEt",
      "metadata": {
        "id": "ZsoNnQCq2aEt"
      },
      "outputs": [],
      "source": [
        "def get_optimizer(model, config, progress, adversarial=True):\n",
        "  '''\n",
        "  get_optimizer\n",
        "\n",
        "  parameter:\n",
        "    @model: Neural Network to be optimizd\n",
        "    @config: configuration dictionary contains parameters\n",
        "    @progress: training progress to configurate learning rate\n",
        "    @adersarial: if we are in adversarial traning scenario \n",
        "\n",
        "  return:\n",
        "    @optimizer: the optimizer we use to train our model\n",
        "\n",
        "  '''\n",
        "  learning_rate = config['lr']\n",
        "  learning_rate = learning_rate / ((1 + config['alpha']*progress)**config['beta'])\n",
        "\n",
        "  weight_decay  = config['wd']\n",
        "  momentum      = config['momentum']\n",
        "\n",
        "  feature_ext   = model.get_submodule(\"feature_extractor\")\n",
        "  classifier    = model.get_submodule(\"classifier\")\n",
        "  discriminator = model.get_submodule(\"discriminator\")\n",
        "\n",
        "  pre_trained_weights = feature_ext.parameters()\n",
        "\n",
        "  if adversarial:\n",
        "    other_weights = list(classifier.parameters()) + list(discriminator.parameters())\n",
        "  else:\n",
        "    other_weights = list(classifier.parameters())\n",
        "\n",
        "  # assign parameters to parameters\n",
        "  optimizer = torch.optim.SGD([\n",
        "    {'params': pre_trained_weights},\n",
        "    {'params': other_weights, 'lr': learning_rate}\n",
        "  ], lr= learning_rate/10, weight_decay=weight_decay, momentum=momentum)\n",
        "  \n",
        "  return optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Atdanl9REs3F",
      "metadata": {
        "id": "Atdanl9REs3F"
      },
      "source": [
        "### 2.5 Training Loop and Testing Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "id": "ORDqPkiT5r1s",
      "metadata": {
        "id": "ORDqPkiT5r1s"
      },
      "outputs": [],
      "source": [
        "def train_loop(dataloader, model, device, progress, dice_loss=False):\n",
        "  \"\"\"\n",
        "  train_loop\n",
        "\n",
        "  Iterate through dataloader to train the network with SGD optimizer.\n",
        "\n",
        "  Parameters:\n",
        "    @dataloader: Pytorch dataloader to iterate through training\n",
        "    @model: Neural Network model that we are training\n",
        "    @device: GPU or CPU\n",
        "    @progress: the progress of traning based on current epoch over total epochs\n",
        "  \"\"\"\n",
        "  size = len(dataloader.dataset)\n",
        "  loss_fn = get_class_loss_func(dice_loss)\n",
        "\n",
        "  optimizer = get_optimizer(model, config, progress, adversarial=False)\n",
        "\n",
        "  for batch, (X, y) in enumerate(dataloader):\n",
        "    X, y = X.to(device), y.to(device)\n",
        "    \n",
        "    # compute prediction and loss\n",
        "    class_pred, _ = model(X)\n",
        "\n",
        "    # classification loss\n",
        "    loss = loss_fn(class_pred, y) \n",
        "    curr_loss = loss.item()\n",
        "    \n",
        "    # backpropagation\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if batch % 10 == 0:\n",
        "      current = batch * len(X)\n",
        "      print(f\"## Meter ## current loss: {curr_loss:>7f} [{current:>5d}/{size:>5d}]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "id": "LlvQLNUDLGJF",
      "metadata": {
        "id": "LlvQLNUDLGJF"
      },
      "outputs": [],
      "source": [
        "def test_loop(dataloader, model, device, dice_loss=False):\n",
        "  \"\"\" \n",
        "  test_loop\n",
        "\n",
        "  Test the model by iterate through the dataloader and compute the correctness.\n",
        "\n",
        "  Parameters:\n",
        "    @dataloader: Pytorch dataloader to iterate through training\n",
        "    @model: Neural Network model that we are training\n",
        "    @progress: the progress of traning based on current epoch over total epochs\n",
        "  \n",
        "  @return:\n",
        "    @test_loss: test loss\n",
        "    @correct: correctness of the test data set\n",
        "  \"\"\"\n",
        "  test_loss, correct = 0, 0\n",
        "  loss_fn = get_class_loss_func(dice_loss)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for X, y in dataloader:\n",
        "      X, y = X.to(device), y.to(device)\n",
        "      class_pred, _ = model(X)\n",
        "\n",
        "      test_loss += loss_fn(class_pred, y).item()\n",
        "      correct += (class_pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "  size = len(dataloader.dataset)\n",
        "  num_batches = len(dataloader)\n",
        "\n",
        "  test_loss /= num_batches\n",
        "  correct /= size\n",
        "  print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "\n",
        "  return test_loss, correct"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "z5uhItnwhj8S",
      "metadata": {
        "id": "z5uhItnwhj8S"
      },
      "source": [
        "### 2.6 Training Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "id": "nM76Syqahknd",
      "metadata": {
        "id": "nM76Syqahknd"
      },
      "outputs": [],
      "source": [
        "def training(model, train_dataloader, test_dataloader, config, device, dice_loss=False):\n",
        "  \"\"\" \n",
        "  training\n",
        "\n",
        "  Acturall training function iterate through the training epochs\n",
        "  \"\"\"\n",
        "  epochs = config['epochs']\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    print(f\"Epoch {epoch+1}\\n------------------\")\n",
        "    progress = epoch/epochs\n",
        "\n",
        "    train_loop(train_dataloader, model, device, progress, dice_loss)\n",
        "\n",
        "  test_loop(test_dataloader, model, device, dice_loss)\n",
        "  print(\"Done\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "PIkYlRhi4tkK",
      "metadata": {
        "id": "PIkYlRhi4tkK"
      },
      "source": [
        "<a name=\"source\">\n",
        "</a>\n",
        "\n",
        "## 3 Training without using Domain Adaptation techniques"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "chQgbFmYorL-",
      "metadata": {
        "id": "chQgbFmYorL-"
      },
      "source": [
        " ### 3.1 Product Domain -> Real Life"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "id": "OCWnRXkc79a4",
      "metadata": {
        "id": "OCWnRXkc79a4"
      },
      "outputs": [],
      "source": [
        "# Get dataloader\n",
        "product_dataset   = get_dataset(product_path)\n",
        "real_life_dataset = get_dataset(real_life_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "raxew0mKm-gM",
      "metadata": {
        "id": "raxew0mKm-gM"
      },
      "source": [
        "#### 3.1.1 Training on Product"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "id": "V93UE4rXi3C9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V93UE4rXi3C9",
        "outputId": "7e1679bf-cbdb-4cff-f284-0922d2c1af2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "------------------\n",
            "## Loss Function ## CrossEntropyLoss being used.\n",
            "## Meter ## current loss: 3.000221 [    0/ 1601]\n",
            "## Meter ## current loss: 2.353845 [  640/ 1601]\n",
            "## Meter ## current loss: 0.540513 [ 1280/ 1601]\n",
            "Epoch 2\n",
            "------------------\n",
            "## Loss Function ## CrossEntropyLoss being used.\n",
            "## Meter ## current loss: 0.441889 [    0/ 1601]\n",
            "## Meter ## current loss: 0.260454 [  640/ 1601]\n",
            "## Meter ## current loss: 0.255725 [ 1280/ 1601]\n",
            "Epoch 3\n",
            "------------------\n",
            "## Loss Function ## CrossEntropyLoss being used.\n",
            "## Meter ## current loss: 0.177589 [    0/ 1601]\n",
            "## Meter ## current loss: 0.148569 [  640/ 1601]\n",
            "## Meter ## current loss: 0.189561 [ 1280/ 1601]\n",
            "Epoch 4\n",
            "------------------\n",
            "## Loss Function ## CrossEntropyLoss being used.\n",
            "## Meter ## current loss: 0.109997 [    0/ 1601]\n",
            "## Meter ## current loss: 0.121884 [  640/ 1601]\n",
            "## Meter ## current loss: 0.144018 [ 1280/ 1601]\n",
            "Epoch 5\n",
            "------------------\n",
            "## Loss Function ## CrossEntropyLoss being used.\n",
            "## Meter ## current loss: 0.085178 [    0/ 1601]\n",
            "## Meter ## current loss: 0.058818 [  640/ 1601]\n",
            "## Meter ## current loss: 0.083681 [ 1280/ 1601]\n",
            "## Loss Function ## CrossEntropyLoss being used.\n",
            "Test Error: \n",
            " Accuracy: 93.5%, Avg loss: 0.258468 \n",
            "\n",
            "Done\n"
          ]
        }
      ],
      "source": [
        "train_dataloader, test_dataloader = get_dataloader(product_dataset, config['batch_size'])\n",
        "\n",
        "source_product_model = DANN(len(product_dataset.classes)).to(device)\n",
        "\n",
        "# Training\n",
        "training(source_product_model, train_dataloader, test_dataloader, config, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "__d71xhd5SVx",
      "metadata": {
        "id": "__d71xhd5SVx"
      },
      "source": [
        "#### 3.1.2 Test on Real Life"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "id": "E54Vm3jmTRcc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E54Vm3jmTRcc",
        "outputId": "c3439a0e-6460-4386-f1ba-8ea169793b38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "## Loss Function ## CrossEntropyLoss being used.\n",
            "Test Error: \n",
            " Accuracy: 63.8%, Avg loss: 1.262631 \n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1.2626310754567385, 0.638)"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ],
      "source": [
        "loader_target_dataset = DataLoader(real_life_dataset, batch_size=config['batch_size'], shuffle=False)\n",
        "\n",
        "# model.load_state_dict(torch.load('model_state.pt', map_location='cpu'))\n",
        "test_loop(loader_target_dataset, source_product_model, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "id": "Cw4GP_z-_iy_",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cw4GP_z-_iy_",
        "outputId": "a9829f25-d050-4156-b3e7-f348f7948acc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5634485760\n"
          ]
        }
      ],
      "source": [
        "del train_dataloader, test_dataloader, loader_target_dataset\n",
        "# del model\n",
        "print(torch.cuda.memory_allocated())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ITPgzgfR5BQB",
      "metadata": {
        "id": "ITPgzgfR5BQB"
      },
      "source": [
        "### 3.2 Real Life -> Product\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42FQtZSXoRSI",
      "metadata": {
        "id": "42FQtZSXoRSI"
      },
      "source": [
        "#### 3.2.1 Training on Real Life"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "id": "Sb5mqdPMBHli",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sb5mqdPMBHli",
        "outputId": "73fda117-b255-427f-abc9-13f15e93704e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "------------------\n",
            "## Loss Function ## CrossEntropyLoss being used.\n",
            "## Meter ## current loss: 3.018649 [    0/ 1601]\n",
            "## Meter ## current loss: 2.755090 [  640/ 1601]\n",
            "## Meter ## current loss: 1.871956 [ 1280/ 1601]\n",
            "Epoch 2\n",
            "------------------\n",
            "## Loss Function ## CrossEntropyLoss being used.\n",
            "## Meter ## current loss: 1.195980 [    0/ 1601]\n",
            "## Meter ## current loss: 1.003482 [  640/ 1601]\n",
            "## Meter ## current loss: 0.784357 [ 1280/ 1601]\n",
            "Epoch 3\n",
            "------------------\n",
            "## Loss Function ## CrossEntropyLoss being used.\n",
            "## Meter ## current loss: 0.450122 [    0/ 1601]\n",
            "## Meter ## current loss: 0.732290 [  640/ 1601]\n",
            "## Meter ## current loss: 0.453970 [ 1280/ 1601]\n",
            "Epoch 4\n",
            "------------------\n",
            "## Loss Function ## CrossEntropyLoss being used.\n",
            "## Meter ## current loss: 0.393326 [    0/ 1601]\n",
            "## Meter ## current loss: 0.442337 [  640/ 1601]\n",
            "## Meter ## current loss: 0.413421 [ 1280/ 1601]\n",
            "Epoch 5\n",
            "------------------\n",
            "## Loss Function ## CrossEntropyLoss being used.\n",
            "## Meter ## current loss: 0.486966 [    0/ 1601]\n",
            "## Meter ## current loss: 0.304995 [  640/ 1601]\n",
            "## Meter ## current loss: 0.435148 [ 1280/ 1601]\n",
            "## Loss Function ## CrossEntropyLoss being used.\n",
            "Test Error: \n",
            " Accuracy: 67.7%, Avg loss: 1.113008 \n",
            "\n",
            "Done\n"
          ]
        }
      ],
      "source": [
        "train_dataloader, test_dataloader = get_dataloader(real_life_dataset, config['batch_size'])\n",
        "\n",
        "source_real_model = DANN(len(real_life_dataset.classes)).to(device)\n",
        "\n",
        "# Training\n",
        "training(source_real_model, train_dataloader, test_dataloader, config, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7HMoVx_85eur",
      "metadata": {
        "id": "7HMoVx_85eur"
      },
      "source": [
        "#### 3.2.2 Testing on Product\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "id": "shPERUI05drr",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "shPERUI05drr",
        "outputId": "95a0a924-bfc2-4088-8575-6497367eca3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "## Loss Function ## CrossEntropyLoss being used.\n",
            "Test Error: \n",
            " Accuracy: 70.5%, Avg loss: 0.951106 \n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.9511063224636018, 0.7045)"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ],
      "source": [
        "loader_target_dataset = DataLoader(product_dataset, batch_size=config['batch_size'], shuffle=False)\n",
        "\n",
        "# model.load_state_dict(torch.load('model_state.pt', map_location='cpu'))\n",
        "test_loop(loader_target_dataset, source_real_model, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "id": "6aT-zlOq862D",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6aT-zlOq862D",
        "outputId": "fb5eed77-cc06-48a1-fb87-ee6dd7082967"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6147605504\n"
          ]
        }
      ],
      "source": [
        "del train_dataloader, test_dataloader, loader_target_dataset\n",
        "# del model\n",
        "print(torch.cuda.memory_allocated())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7q7jwnwwhRz2",
      "metadata": {
        "id": "7q7jwnwwhRz2"
      },
      "source": [
        "<a name=\"UDA\">\n",
        "</a>\n",
        "\n",
        "## 4: Define UDA functions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "425ysNsFjUy-",
      "metadata": {
        "id": "425ysNsFjUy-"
      },
      "source": [
        "### 4.1 Adversarial Discriminator Loss\n",
        "\n",
        "Here we compute the discrimination loss of from the discriminator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "id": "81ccf625",
      "metadata": {
        "id": "81ccf625"
      },
      "outputs": [],
      "source": [
        "def get_discriminator_loss(source_pred, target_pred): \n",
        "    \"\"\" \n",
        "    get_discriminator_loss\n",
        "\n",
        "    parameters:\n",
        "        @source_pred: model prediction from the source data\n",
        "        @target_pred: model prediction from the target data\n",
        "\n",
        "    return:\n",
        "        @domain_loss: computed domain loss\n",
        "    \"\"\"\n",
        "    domain_pred = torch.cat((source_pred, target_pred),dim=0).cuda()\n",
        "    #print(domain_pred.shape) # [128,1024]\n",
        "    source_truth = torch.zeros(len(source_pred))\n",
        "    target_truth = torch.ones(len(target_pred))\n",
        "    domain_truth = torch.cat((source_truth, target_truth),dim=0).cuda()\n",
        "    #print(domain_truth.shape) # [128]\n",
        "\n",
        "    domain_loss = domain_truth*torch.log(1/domain_pred)+(1-domain_truth)*torch.log(1/(1-domain_pred))\n",
        "    domain_loss = domain_loss.mean()\n",
        "\n",
        "    return domain_loss "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "BBaNX5GgjK7F",
      "metadata": {
        "id": "BBaNX5GgjK7F"
      },
      "source": [
        "### 4.2 Adversarial optimizer\n",
        "\n",
        "We are using the Stochastic Gradient Descent optimizer and \n",
        "set learning rate for the pre_trained_weights to be 1/10\n",
        "of other learning rate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "id": "zKscrnYohp7k",
      "metadata": {
        "id": "zKscrnYohp7k"
      },
      "outputs": [],
      "source": [
        "def get_adversarial_optimizer(model, config, progress, adversarial=True):\n",
        "  '''\n",
        "  Get Adversarial Optimizers\n",
        "  '''\n",
        "  lr, wd, momtm = config['lr'], config['wd'], config['momentum']\n",
        "  lr = lr / ((1 + config['alpha']*progress)**config['beta'])\n",
        "\n",
        "  feature_ext   = model.get_submodule(\"feature_extractor\")\n",
        "  classifier    = model.get_submodule(\"classifier\")\n",
        "  discriminator = model.get_submodule(\"discriminator\")\n",
        "\n",
        "  pre_trained_weights   = feature_ext.parameters()\n",
        "  classifier_weights    = classifier.parameters()\n",
        "  discriminator_weights = discriminator.parameters()\n",
        "\n",
        "  feature_optim       = torch.optim.SGD([{'params': pre_trained_weights}],     lr=lr/10, weight_decay=wd, momentum=momtm)\n",
        "  classifier_optim    = torch.optim.SGD([{'params': classifier_weights}],      lr=lr,    weight_decay=wd, momentum=momtm)\n",
        "  discriminator_optim = torch.optim.SGD([{'params': discriminator_weights}],   lr=lr,    weight_decay=wd, momentum=momtm)\n",
        "  \n",
        "  return feature_optim, classifier_optim, discriminator_optim "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hRbDuyfU929u",
      "metadata": {
        "id": "hRbDuyfU929u"
      },
      "source": [
        "### 4.3 Adversarial Train Loop\n",
        "\n",
        "Setting the **domain adaptation parameter** according to the original [paper](https://arxiv.org/pdf/1505.07818.pdf) section 5.2.2\n",
        "\n",
        "$$ \\lambda_p = \\frac{2}{1 + exp(-\\gamma \\cdot p)} - 1 $$\n",
        "\n",
        "where p is the training progress linearly changing from 0 to 1.\n",
        "\n",
        "So here we optimize the model by calculating the classification loss and discrimination loss. \n",
        "Then we optimize the classifier, the discriminator, and the feature extractor based\n",
        "on the loss we get."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "id": "1SQHSVPE93hi",
      "metadata": {
        "id": "1SQHSVPE93hi"
      },
      "outputs": [],
      "source": [
        "def adversarial_train_loop(source_loader, target_loader, model, config, progress, device):\n",
        "  \"\"\"\n",
        "  parameters:\n",
        "    @source_loader\n",
        "    @target_loader\n",
        "    @model\n",
        "    @config\n",
        "    @progress\n",
        "    @device\n",
        "\n",
        "  return:\n",
        "    @best_state\n",
        "    @best_loss\n",
        "  \"\"\"\n",
        "  size = len(source_loader.dataset)\n",
        "  \n",
        "  # cross entropy loss\n",
        "  classification_loss = get_class_loss_func()\n",
        "\n",
        "  # Get three optimizer\n",
        "  feature_optim, class_optim, discriminator_optim = get_adversarial_optimizer(model, config, progress)\n",
        "\n",
        "  # Target data loader iterator\n",
        "  iter_target = iter(target_loader)\n",
        "\n",
        "  domain_adapt = 2 / (1 + math.exp(-config['gamma']*progress)) - 1\n",
        "\n",
        "  for batch, (X_source, y_source) in enumerate(source_loader):\n",
        "    try:\n",
        "      X_target, _ = next(iter_target)\n",
        "    except:\n",
        "      iter_target = iter(target_loader)\n",
        "      X_target, _ = next(iter_target)  \n",
        "\n",
        "    # Some internal bug return nested tesnor with size 1\n",
        "    if len(X_source) < 64:\n",
        "      continue\n",
        "\n",
        "    X_source, y_source, X_target = X_source.to(device), y_source.to(device), X_target.to(device)\n",
        "\n",
        "    class_pred_source, domain_pred_source = model(X_source)\n",
        "    _,                 domain_pred_target = model(X_target)\n",
        "\n",
        "    class_loss   = classification_loss(class_pred_source, y_source)\n",
        "    discrim_loss = get_discriminator_loss(domain_pred_source, domain_pred_target)\n",
        "\n",
        "    feature_optim.zero_grad()\n",
        "\n",
        "    # Update discriminator\n",
        "    discriminator_optim.zero_grad()\n",
        "    discrim_loss.backward(retain_graph=True)\n",
        "    discriminator_optim.step()\n",
        "\n",
        "    # Update classifier\n",
        "    class_optim.zero_grad()\n",
        "    class_loss.backward(retain_graph=True)\n",
        "    class_optim.step()\n",
        "\n",
        "    # Update feature extractor\n",
        "    feature_optim.step()  \n",
        "\n",
        "    # Total loss\n",
        "    total_loss = class_loss - domain_adapt * discrim_loss \n",
        "\n",
        "    if batch % 10 == 0:\n",
        "      class_loss, discrim_loss, current = class_loss.item(), discrim_loss.item(), batch * len(X_source)\n",
        "      total_loss = total_loss.item()\n",
        "      # print(f\"## Meter  ## [{current:>5d}/{size:>5d}]\")\n",
        "      print(f\"## Meter  ## classification loss: {class_loss:>7f} discrim loss: {discrim_loss:>7f} total loss: {total_loss:>7f}[{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "    del class_loss, discrim_loss \n",
        "    del X_source, y_source, X_target, class_pred_source, domain_pred_source, domain_pred_target\n",
        "  \n",
        "  # return best_state, best_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1jHD6IJ6H8vF",
      "metadata": {
        "id": "1jHD6IJ6H8vF"
      },
      "source": [
        "### 4.4 Adversarial Test Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "id": "KyhdZbs3H9Ue",
      "metadata": {
        "id": "KyhdZbs3H9Ue"
      },
      "outputs": [],
      "source": [
        "def adversarial_test_loop(dataloader, model, device, name=\"\"):\n",
        "  \"\"\" \n",
        "  adversarial_test_loop\n",
        "\n",
        "  Test the model compute the loss and accuracy\n",
        "  \"\"\"\n",
        "  test_loss, correct = 0, 0\n",
        "\n",
        "  class_loss_func = get_class_loss_func()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for X, y in dataloader:\n",
        "      X, y = X.to(device), y.to(device)\n",
        "      class_pred, _ = model(X)\n",
        "\n",
        "      test_loss += class_loss_func(class_pred, y).item()\n",
        "      correct += (class_pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "  size = len(dataloader.dataset)\n",
        "  num_batches = len(dataloader)\n",
        "\n",
        "  test_loss /= num_batches\n",
        "  correct /= size\n",
        "  print(f\"{name} Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "\n",
        "  return test_loss, correct"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tyopBjWr-FAs",
      "metadata": {
        "id": "tyopBjWr-FAs"
      },
      "source": [
        "### 4.5 Adversarial Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "id": "dBCrfNPj-Bxa",
      "metadata": {
        "id": "dBCrfNPj-Bxa"
      },
      "outputs": [],
      "source": [
        "def adversarial_training(model, source_loader, source_test_loader, target_loader, config, device):\n",
        "  \"\"\" \n",
        "  adversarial_training\n",
        "\n",
        "  Training the adversarial model with the config\n",
        "  \"\"\"\n",
        "  no_improve_count = 0\n",
        "\n",
        "  for epoch in range(config['epochs']):\n",
        "    print(f\"Epoch {epoch+1}\\n------------------\")\n",
        "    progress = epoch/config['epochs']\n",
        "\n",
        "    adversarial_train_loop(source_loader, target_loader, model, config, progress, device)\n",
        "\n",
        "    source_loss, _ = adversarial_test_loop(source_test_loader, model, device, \"Source Test\")\n",
        "\n",
        "  print(\"Done\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "x4ePILfZ4jOA",
      "metadata": {
        "id": "x4ePILfZ4jOA"
      },
      "source": [
        "<a name=\"UDA_training\">\n",
        "</a>\n",
        "\n",
        "## 5 Training with UDA Techniques"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "id": "wt-IOOThNShC",
      "metadata": {
        "id": "wt-IOOThNShC"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "id": "rcLFqgOelDsW",
      "metadata": {
        "id": "rcLFqgOelDsW"
      },
      "outputs": [],
      "source": [
        "product_adv_model = DANN(len(product_dataset.classes)).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "H7QPRLosqltl",
      "metadata": {
        "id": "H7QPRLosqltl"
      },
      "source": [
        "### 5.1 Product -> Real Life"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "VKR4440ZqxJV",
      "metadata": {
        "id": "VKR4440ZqxJV"
      },
      "source": [
        "#### 5.1.1 Training on Product"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "id": "IcUMoe8x_qrC",
      "metadata": {
        "id": "IcUMoe8x_qrC"
      },
      "outputs": [],
      "source": [
        "train_dataloader, train_test_dataloader = get_dataloader(product_dataset, config['batch_size'])\n",
        "target_dataloader, target_test_dataloader = get_dataloader(real_life_dataset, config['batch_size'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "id": "ZgN_WIqqPtjk",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZgN_WIqqPtjk",
        "outputId": "44664e46-dec3-4fed-c670-6aa69c11c139"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "------------------\n",
            "## Loss Function ## CrossEntropyLoss being used.\n",
            "## Meter  ## classification loss: 2.968360 discrim loss: 0.697306 total loss: 2.968360[    0/ 1601]\n",
            "## Meter  ## classification loss: 2.331265 discrim loss: 0.695401 total loss: 2.331265[  640/ 1601]\n",
            "## Meter  ## classification loss: 0.402743 discrim loss: 0.694994 total loss: 0.402743[ 1280/ 1601]\n",
            "## Loss Function ## CrossEntropyLoss being used.\n",
            "Source Test Test Error: \n",
            " Accuracy: 83.5%, Avg loss: 0.693817 \n",
            "\n",
            "Epoch 2\n",
            "------------------\n",
            "## Loss Function ## CrossEntropyLoss being used.\n",
            "## Meter  ## classification loss: 0.814303 discrim loss: 0.698190 total loss: 0.282565[    0/ 1601]\n",
            "## Meter  ## classification loss: 0.423594 discrim loss: 0.695006 total loss: -0.105718[  640/ 1601]\n",
            "## Meter  ## classification loss: 0.364948 discrim loss: 0.694518 total loss: -0.163993[ 1280/ 1601]\n",
            "## Loss Function ## CrossEntropyLoss being used.\n",
            "Source Test Test Error: \n",
            " Accuracy: 90.2%, Avg loss: 0.406736 \n",
            "\n",
            "Epoch 3\n",
            "------------------\n",
            "## Loss Function ## CrossEntropyLoss being used.\n",
            "## Meter  ## classification loss: 0.337996 discrim loss: 0.694375 total loss: -0.331401[    0/ 1601]\n",
            "## Meter  ## classification loss: 0.096053 discrim loss: 0.694847 total loss: -0.573799[  640/ 1601]\n",
            "## Meter  ## classification loss: 0.223545 discrim loss: 0.695290 total loss: -0.446733[ 1280/ 1601]\n",
            "## Loss Function ## CrossEntropyLoss being used.\n",
            "Source Test Test Error: \n",
            " Accuracy: 90.2%, Avg loss: 0.377241 \n",
            "\n",
            "Epoch 4\n",
            "------------------\n",
            "## Loss Function ## CrossEntropyLoss being used.\n",
            "## Meter  ## classification loss: 0.098003 discrim loss: 0.695726 total loss: -0.594283[    0/ 1601]\n",
            "## Meter  ## classification loss: 0.163034 discrim loss: 0.695347 total loss: -0.528874[  640/ 1601]\n",
            "## Meter  ## classification loss: 0.139597 discrim loss: 0.694787 total loss: -0.551754[ 1280/ 1601]\n",
            "## Loss Function ## CrossEntropyLoss being used.\n",
            "Source Test Test Error: \n",
            " Accuracy: 91.2%, Avg loss: 0.305145 \n",
            "\n",
            "Epoch 5\n",
            "------------------\n",
            "## Loss Function ## CrossEntropyLoss being used.\n",
            "## Meter  ## classification loss: 0.113922 discrim loss: 0.694589 total loss: -0.580202[    0/ 1601]\n",
            "## Meter  ## classification loss: 0.051970 discrim loss: 0.695308 total loss: -0.642872[  640/ 1601]\n",
            "## Meter  ## classification loss: 0.137522 discrim loss: 0.694365 total loss: -0.556377[ 1280/ 1601]\n",
            "## Loss Function ## CrossEntropyLoss being used.\n",
            "Source Test Test Error: \n",
            " Accuracy: 91.2%, Avg loss: 0.322169 \n",
            "\n",
            "Done\n"
          ]
        }
      ],
      "source": [
        "torch.autograd.set_detect_anomaly(True)\n",
        "adversarial_training(product_adv_model, train_dataloader, train_test_dataloader, target_dataloader, config, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1s0Z_xXXq5gO",
      "metadata": {
        "id": "1s0Z_xXXq5gO"
      },
      "source": [
        "#### 5.1.2 Testing on Real Life"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "id": "oUUo7mSiJasc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oUUo7mSiJasc",
        "outputId": "d0b4c3db-1d17-4071-9f82-086ae7b6d208"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "## Loss Function ## CrossEntropyLoss being used.\n",
            "Test Error: \n",
            " Accuracy: 63.9%, Avg loss: 1.276491 \n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1.2764913607388735, 0.6395)"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ],
      "source": [
        "loader_target_dataset = DataLoader(real_life_dataset, batch_size=config['batch_size'], shuffle=False)\n",
        "\n",
        "test_loop(loader_target_dataset, product_adv_model, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "FNofveL_-8gX",
      "metadata": {
        "id": "FNofveL_-8gX"
      },
      "source": [
        "### 5.2 Real Life -> Product"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6YxXx0Lzresh",
      "metadata": {
        "id": "6YxXx0Lzresh"
      },
      "source": [
        "#### 5.2.1 Training on Real Life"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "id": "enMKc_Em_tWq",
      "metadata": {
        "id": "enMKc_Em_tWq"
      },
      "outputs": [],
      "source": [
        "train_dataloader, train_test_dataloader = get_dataloader(real_life_dataset, config['batch_size'])\n",
        "target_dataloader, target_test_dataloader = get_dataloader(product_dataset, config['batch_size'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "id": "Msifip2P-7UB",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Msifip2P-7UB",
        "outputId": "8228ee59-78ed-4da6-f4a9-828a4e104c9b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "------------------\n",
            "## Loss Function ## CrossEntropyLoss being used.\n",
            "## Meter  ## classification loss: 2.999510 discrim loss: 0.695183 total loss: 2.999510[    0/ 1601]\n",
            "## Meter  ## classification loss: 2.776052 discrim loss: 0.694211 total loss: 2.776052[  640/ 1601]\n",
            "## Meter  ## classification loss: 1.730621 discrim loss: 0.694183 total loss: 1.730621[ 1280/ 1601]\n",
            "## Loss Function ## CrossEntropyLoss being used.\n",
            "Source Test Test Error: \n",
            " Accuracy: 63.2%, Avg loss: 1.210765 \n",
            "\n",
            "Epoch 2\n",
            "------------------\n",
            "## Loss Function ## CrossEntropyLoss being used.\n",
            "## Meter  ## classification loss: 1.088701 discrim loss: 0.694209 total loss: 0.559996[    0/ 1601]\n",
            "## Meter  ## classification loss: 0.725001 discrim loss: 0.694996 total loss: 0.195696[  640/ 1601]\n",
            "## Meter  ## classification loss: 0.809038 discrim loss: 0.695145 total loss: 0.279619[ 1280/ 1601]\n",
            "## Loss Function ## CrossEntropyLoss being used.\n",
            "Source Test Test Error: \n",
            " Accuracy: 69.9%, Avg loss: 0.925751 \n",
            "\n",
            "Epoch 3\n",
            "------------------\n",
            "## Loss Function ## CrossEntropyLoss being used.\n",
            "## Meter  ## classification loss: 0.687453 discrim loss: 0.695670 total loss: 0.016808[    0/ 1601]\n",
            "## Meter  ## classification loss: 0.569870 discrim loss: 0.695500 total loss: -0.100611[  640/ 1601]\n",
            "## Meter  ## classification loss: 0.521565 discrim loss: 0.695168 total loss: -0.148596[ 1280/ 1601]\n",
            "## Loss Function ## CrossEntropyLoss being used.\n",
            "Source Test Test Error: \n",
            " Accuracy: 76.7%, Avg loss: 0.726582 \n",
            "\n",
            "Epoch 4\n",
            "------------------\n",
            "## Loss Function ## CrossEntropyLoss being used.\n",
            "## Meter  ## classification loss: 0.468766 discrim loss: 0.695469 total loss: -0.223263[    0/ 1601]\n",
            "## Meter  ## classification loss: 0.453375 discrim loss: 0.695358 total loss: -0.238544[  640/ 1601]\n",
            "## Meter  ## classification loss: 0.554679 discrim loss: 0.694795 total loss: -0.136679[ 1280/ 1601]\n",
            "## Loss Function ## CrossEntropyLoss being used.\n",
            "Source Test Test Error: \n",
            " Accuracy: 72.9%, Avg loss: 0.758679 \n",
            "\n",
            "Epoch 5\n",
            "------------------\n",
            "## Loss Function ## CrossEntropyLoss being used.\n",
            "## Meter  ## classification loss: 0.318834 discrim loss: 0.695463 total loss: -0.376163[    0/ 1601]\n",
            "## Meter  ## classification loss: 0.325561 discrim loss: 0.695204 total loss: -0.369178[  640/ 1601]\n",
            "## Meter  ## classification loss: 0.222285 discrim loss: 0.694968 total loss: -0.472217[ 1280/ 1601]\n",
            "## Loss Function ## CrossEntropyLoss being used.\n",
            "Source Test Test Error: \n",
            " Accuracy: 75.7%, Avg loss: 0.708048 \n",
            "\n",
            "Done\n"
          ]
        }
      ],
      "source": [
        "# Training\n",
        "real_adv_model = DANN(len(product_dataset.classes)).to(device)\n",
        "adversarial_training(real_adv_model, train_dataloader, train_test_dataloader, target_dataloader, config, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "nSh7gaC8JaVC",
      "metadata": {
        "id": "nSh7gaC8JaVC"
      },
      "source": [
        "#### 5.2.2 Testing on Product"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "id": "DfSRjr84-8DF",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DfSRjr84-8DF",
        "outputId": "b8fd315c-763a-4cce-8b8e-61aa71b88513"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "## Loss Function ## CrossEntropyLoss being used.\n",
            "Test Error: \n",
            " Accuracy: 80.8%, Avg loss: 0.591975 \n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.591975215356797, 0.8075)"
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ],
      "source": [
        "loader_target_dataset = DataLoader(product_dataset, batch_size=config['batch_size'], shuffle=False)\n",
        "\n",
        "test_loop(loader_target_dataset, real_adv_model, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aEOlulqbpt9j",
      "metadata": {
        "id": "aEOlulqbpt9j"
      },
      "source": [
        "<a name=\"UDA_mid\">\n",
        "</a>\n",
        "\n",
        "## 6 UDA Ablation Study"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2iF-R_1nqn65",
      "metadata": {
        "id": "2iF-R_1nqn65"
      },
      "source": [
        "### 6.1 How UDA over different levels of feature layer work"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "id": "PWbv3xDqqvde",
      "metadata": {
        "id": "PWbv3xDqqvde"
      },
      "outputs": [],
      "source": [
        "# To test the middle layer feature adversarial training, we copy the model from torchvision and modify it.\n",
        "class AlexNet(nn.Module):\n",
        "    def __init__(self, num_classes: int = 1000, dropout: float = 0.5) -> None:\n",
        "        super().__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "        )# 12544 = (6*6*256)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(p=dropout),\n",
        "            nn.Linear(256 * 6 * 6, 4096), \n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(p=dropout),\n",
        "            nn.Linear(4096, 4096),  # original 4096\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(4096, num_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        feature = self.features(x)\n",
        "        x = self.avgpool(feature)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.classifier(x)\n",
        "        feature = feature.view(feature.size(0), -1)\n",
        "        # return x, x\n",
        "        return x, feature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "id": "rweuEqk5q4SQ",
      "metadata": {
        "id": "rweuEqk5q4SQ"
      },
      "outputs": [],
      "source": [
        "class FeatureExtractor(nn.Module):\n",
        "  def __init__(self, pretrained=True):\n",
        "    super(FeatureExtractor, self).__init__()\n",
        "    if pretrained:\n",
        "      state_dict = alexnet(weights='DEFAULT').state_dict()\n",
        "    else:\n",
        "      state_dict = alexnet().state_dict()\n",
        "    self.feature_extractor = AlexNet()\n",
        "    self.feature_extractor.load_state_dict(state_dict)\n",
        "    self.feature_dim = self.feature_extractor.classifier[-1].in_features\n",
        "    self.adv_feature_dim = 12544\n",
        "    print(self.feature_dim, self.adv_feature_dim)\n",
        "    # print(f\"Feature dimension: {self.feature_dim}\")\n",
        "    # make the last layer identity\n",
        "    self.feature_extractor.classifier[-1] = nn.Identity()\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = self.feature_extractor(x)\n",
        "    return out\n",
        "  \n",
        "  def output_dim(self):\n",
        "    return self.feature_dim\n",
        "  \n",
        "  def adv_output_dim(self):\n",
        "    return self.adv_feature_dim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "id": "qK6zC5dqq6gn",
      "metadata": {
        "id": "qK6zC5dqq6gn"
      },
      "outputs": [],
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.discriminator =  nn.Sequential(\n",
        "            nn.Linear(int(input_dim), 1024),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(1024,1),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        validity = self.discriminator(x)\n",
        "        return validity "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "id": "xiUoe6xTsGFw",
      "metadata": {
        "id": "xiUoe6xTsGFw"
      },
      "outputs": [],
      "source": [
        "class DANN_Mid(nn.Module):\n",
        "  def __init__(self, num_classes, pretrained=True):\n",
        "    super(DANN_Mid, self).__init__()\n",
        "    self.output_dim = num_classes\n",
        "\n",
        "    # define inner network component\n",
        "    self.feature_extractor = FeatureExtractor(pretrained=pretrained)\n",
        "    self.classifier = Classifier(self.feature_extractor.output_dim(), num_classes)\n",
        "    self.discriminator = Discriminator(self.feature_extractor.adv_output_dim())  \n",
        "  \n",
        "  def forward(self, x):\n",
        "    # 4096, 12544\n",
        "    feature_output, adv_feature = self.feature_extractor(x)\n",
        "    \n",
        "    class_pred = self.classifier(feature_output)\n",
        "\n",
        "    # Add a ReverseLayer here for negative gradient computation\n",
        "    reverse_feature = ReverseLayerF.apply(adv_feature)\n",
        "    domain_pred = self.discriminator(reverse_feature)\n",
        "\n",
        "    return class_pred, domain_pred "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "In6J1hwisLeA",
      "metadata": {
        "id": "In6J1hwisLeA"
      },
      "source": [
        "### 6.2 Product -> Real Life"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "id": "h241WFL4sG-x",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h241WFL4sG-x",
        "outputId": "0ce33166-a1e4-43d8-d78b-cb75a363b5db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4096 12544\n",
            "Epoch 1\n",
            "------------------\n",
            "## Loss Function ## CrossEntropyLoss being used.\n",
            "## Meter  ## classification loss: 3.023133 discrim loss: 0.697544 total loss: 3.023133[    0/ 1601]\n",
            "## Meter  ## classification loss: 2.263576 discrim loss: 0.694350 total loss: 2.263576[  640/ 1601]\n",
            "## Meter  ## classification loss: 0.462087 discrim loss: 0.695956 total loss: 0.462087[ 1280/ 1601]\n",
            "## Loss Function ## CrossEntropyLoss being used.\n",
            "Source Test Test Error: \n",
            " Accuracy: 82.0%, Avg loss: 0.641624 \n",
            "\n",
            "Epoch 2\n",
            "------------------\n",
            "## Loss Function ## CrossEntropyLoss being used.\n",
            "## Meter  ## classification loss: 0.596202 discrim loss: 0.695391 total loss: 0.066597[    0/ 1601]\n",
            "## Meter  ## classification loss: 0.449357 discrim loss: 0.694993 total loss: -0.079945[  640/ 1601]\n",
            "## Meter  ## classification loss: 0.141513 discrim loss: 0.695210 total loss: -0.387955[ 1280/ 1601]\n",
            "## Loss Function ## CrossEntropyLoss being used.\n",
            "Source Test Test Error: \n",
            " Accuracy: 89.5%, Avg loss: 0.373861 \n",
            "\n",
            "Epoch 3\n",
            "------------------\n",
            "## Loss Function ## CrossEntropyLoss being used.\n",
            "## Meter  ## classification loss: 0.196638 discrim loss: 0.694374 total loss: -0.472758[    0/ 1601]\n",
            "## Meter  ## classification loss: 0.151954 discrim loss: 0.694585 total loss: -0.517644[  640/ 1601]\n",
            "## Meter  ## classification loss: 0.173645 discrim loss: 0.694510 total loss: -0.495882[ 1280/ 1601]\n",
            "## Loss Function ## CrossEntropyLoss being used.\n",
            "Source Test Test Error: \n",
            " Accuracy: 92.0%, Avg loss: 0.300238 \n",
            "\n",
            "Epoch 4\n",
            "------------------\n",
            "## Loss Function ## CrossEntropyLoss being used.\n",
            "## Meter  ## classification loss: 0.091308 discrim loss: 0.693913 total loss: -0.599173[    0/ 1601]\n",
            "## Meter  ## classification loss: 0.185761 discrim loss: 0.694365 total loss: -0.505170[  640/ 1601]\n",
            "## Meter  ## classification loss: 0.055721 discrim loss: 0.694079 total loss: -0.634926[ 1280/ 1601]\n",
            "## Loss Function ## CrossEntropyLoss being used.\n",
            "Source Test Test Error: \n",
            " Accuracy: 90.5%, Avg loss: 0.295015 \n",
            "\n",
            "Epoch 5\n",
            "------------------\n",
            "## Loss Function ## CrossEntropyLoss being used.\n",
            "## Meter  ## classification loss: 0.076057 discrim loss: 0.694008 total loss: -0.617485[    0/ 1601]\n",
            "## Meter  ## classification loss: 0.092108 discrim loss: 0.694067 total loss: -0.601493[  640/ 1601]\n",
            "## Meter  ## classification loss: 0.108216 discrim loss: 0.694036 total loss: -0.585355[ 1280/ 1601]\n",
            "## Loss Function ## CrossEntropyLoss being used.\n",
            "Source Test Test Error: \n",
            " Accuracy: 91.7%, Avg loss: 0.244276 \n",
            "\n",
            "Done\n"
          ]
        }
      ],
      "source": [
        "train_dataloader, train_test_dataloader = get_dataloader(product_dataset, config['batch_size'])\n",
        "target_dataloader, target_test_dataloader = get_dataloader(real_life_dataset, config['batch_size'])\n",
        "mid_product_adv_model = DANN_Mid(len(product_dataset.classes), pretrained=True).to(device)\n",
        "torch.autograd.set_detect_anomaly(True)\n",
        "adversarial_training(mid_product_adv_model, train_dataloader, train_test_dataloader, target_dataloader, config, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "id": "vNeup5dEsR0h",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vNeup5dEsR0h",
        "outputId": "51ea394b-91b9-4ea3-f1a9-7d44bfa6785a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "## Loss Function ## CrossEntropyLoss being used.\n",
            "Test Error: \n",
            " Accuracy: 62.8%, Avg loss: 1.277676 \n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1.277676198631525, 0.628)"
            ]
          },
          "metadata": {},
          "execution_count": 112
        }
      ],
      "source": [
        "loader_target_dataset = DataLoader(real_life_dataset, batch_size=config['batch_size'], shuffle=False)\n",
        "\n",
        "test_loop(loader_target_dataset, mid_product_adv_model, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "m_tpI5adsUBs",
      "metadata": {
        "id": "m_tpI5adsUBs"
      },
      "source": [
        "### 6.3 Real Life -> Product"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "id": "fWAcM4ADsUXD",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fWAcM4ADsUXD",
        "outputId": "25ac6252-4655-4101-8b28-d28417dcc4e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4096 12544\n",
            "Epoch 1\n",
            "------------------\n",
            "## Loss Function ## CrossEntropyLoss being used.\n",
            "## Meter  ## classification loss: 3.008753 discrim loss: 0.705185 total loss: 3.008753[    0/ 1601]\n",
            "## Meter  ## classification loss: 2.743634 discrim loss: 0.697054 total loss: 2.743634[  640/ 1601]\n",
            "## Meter  ## classification loss: 1.983272 discrim loss: 0.696823 total loss: 1.983272[ 1280/ 1601]\n",
            "## Loss Function ## CrossEntropyLoss being used.\n",
            "Source Test Test Error: \n",
            " Accuracy: 71.4%, Avg loss: 1.171896 \n",
            "\n",
            "Epoch 2\n",
            "------------------\n",
            "## Loss Function ## CrossEntropyLoss being used.\n",
            "## Meter  ## classification loss: 1.160598 discrim loss: 0.696179 total loss: 0.838882[    0/ 1601]\n",
            "## Meter  ## classification loss: 1.296221 discrim loss: 0.694806 total loss: 0.975140[  640/ 1601]\n",
            "## Meter  ## classification loss: 1.015076 discrim loss: 0.695105 total loss: 0.693856[ 1280/ 1601]\n",
            "## Loss Function ## CrossEntropyLoss being used.\n",
            "Source Test Test Error: \n",
            " Accuracy: 76.7%, Avg loss: 0.761417 \n",
            "\n",
            "Epoch 3\n",
            "------------------\n",
            "## Loss Function ## CrossEntropyLoss being used.\n",
            "## Meter  ## classification loss: 0.523140 discrim loss: 0.694434 total loss: -0.005736[    0/ 1601]\n",
            "## Meter  ## classification loss: 0.379140 discrim loss: 0.694527 total loss: -0.149808[  640/ 1601]\n",
            "## Meter  ## classification loss: 0.670534 discrim loss: 0.694876 total loss: 0.141321[ 1280/ 1601]\n",
            "## Loss Function ## CrossEntropyLoss being used.\n",
            "Source Test Test Error: \n",
            " Accuracy: 79.9%, Avg loss: 0.672328 \n",
            "\n",
            "Epoch 4\n",
            "------------------\n",
            "## Loss Function ## CrossEntropyLoss being used.\n",
            "## Meter  ## classification loss: 0.351063 discrim loss: 0.694272 total loss: -0.277356[    0/ 1601]\n",
            "## Meter  ## classification loss: 0.367347 discrim loss: 0.694484 total loss: -0.261264[  640/ 1601]\n",
            "## Meter  ## classification loss: 0.445270 discrim loss: 0.694242 total loss: -0.183121[ 1280/ 1601]\n",
            "## Loss Function ## CrossEntropyLoss being used.\n",
            "Source Test Test Error: \n",
            " Accuracy: 78.9%, Avg loss: 0.711307 \n",
            "\n",
            "Epoch 5\n",
            "------------------\n",
            "## Loss Function ## CrossEntropyLoss being used.\n",
            "## Meter  ## classification loss: 0.298073 discrim loss: 0.694159 total loss: -0.371116[    0/ 1601]\n",
            "## Meter  ## classification loss: 0.435677 discrim loss: 0.694319 total loss: -0.233666[  640/ 1601]\n",
            "## Meter  ## classification loss: 0.522574 discrim loss: 0.694337 total loss: -0.146787[ 1280/ 1601]\n",
            "## Loss Function ## CrossEntropyLoss being used.\n",
            "Source Test Test Error: \n",
            " Accuracy: 78.9%, Avg loss: 0.679263 \n",
            "\n",
            "Epoch 6\n",
            "------------------\n",
            "## Loss Function ## CrossEntropyLoss being used.\n",
            "## Meter  ## classification loss: 0.287247 discrim loss: 0.693948 total loss: -0.397413[    0/ 1601]\n",
            "## Meter  ## classification loss: 0.324448 discrim loss: 0.693881 total loss: -0.360145[  640/ 1601]\n",
            "## Meter  ## classification loss: 0.192111 discrim loss: 0.694174 total loss: -0.492771[ 1280/ 1601]\n",
            "## Loss Function ## CrossEntropyLoss being used.\n",
            "Source Test Test Error: \n",
            " Accuracy: 79.4%, Avg loss: 0.667058 \n",
            "\n",
            "Epoch 7\n",
            "------------------\n",
            "## Loss Function ## CrossEntropyLoss being used.\n",
            "## Meter  ## classification loss: 0.161958 discrim loss: 0.694002 total loss: -0.528612[    0/ 1601]\n",
            "## Meter  ## classification loss: 0.159111 discrim loss: 0.693913 total loss: -0.531371[  640/ 1601]\n",
            "## Meter  ## classification loss: 0.113610 discrim loss: 0.694055 total loss: -0.577012[ 1280/ 1601]\n",
            "## Loss Function ## CrossEntropyLoss being used.\n",
            "Source Test Test Error: \n",
            " Accuracy: 79.9%, Avg loss: 0.657768 \n",
            "\n",
            "Epoch 8\n",
            "------------------\n",
            "## Loss Function ## CrossEntropyLoss being used.\n",
            "## Meter  ## classification loss: 0.126745 discrim loss: 0.693813 total loss: -0.565804[    0/ 1601]\n",
            "## Meter  ## classification loss: 0.175403 discrim loss: 0.693670 total loss: -0.517003[  640/ 1601]\n",
            "## Meter  ## classification loss: 0.178969 discrim loss: 0.693976 total loss: -0.513742[ 1280/ 1601]\n",
            "## Loss Function ## CrossEntropyLoss being used.\n",
            "Source Test Test Error: \n",
            " Accuracy: 81.7%, Avg loss: 0.642107 \n",
            "\n",
            "Epoch 9\n",
            "------------------\n",
            "## Loss Function ## CrossEntropyLoss being used.\n",
            "## Meter  ## classification loss: 0.107304 discrim loss: 0.693745 total loss: -0.585976[    0/ 1601]\n",
            "## Meter  ## classification loss: 0.096802 discrim loss: 0.693718 total loss: -0.596451[  640/ 1601]\n",
            "## Meter  ## classification loss: 0.147240 discrim loss: 0.693747 total loss: -0.546042[ 1280/ 1601]\n",
            "## Loss Function ## CrossEntropyLoss being used.\n",
            "Source Test Test Error: \n",
            " Accuracy: 81.7%, Avg loss: 0.622418 \n",
            "\n",
            "Epoch 10\n",
            "------------------\n",
            "## Loss Function ## CrossEntropyLoss being used.\n",
            "## Meter  ## classification loss: 0.093167 discrim loss: 0.693786 total loss: -0.600448[    0/ 1601]\n",
            "## Meter  ## classification loss: 0.078104 discrim loss: 0.693884 total loss: -0.615608[  640/ 1601]\n",
            "## Meter  ## classification loss: 0.083481 discrim loss: 0.693853 total loss: -0.610201[ 1280/ 1601]\n",
            "## Loss Function ## CrossEntropyLoss being used.\n",
            "Source Test Test Error: \n",
            " Accuracy: 79.2%, Avg loss: 0.679269 \n",
            "\n",
            "Done\n"
          ]
        }
      ],
      "source": [
        "config = dict(epochs=10, batch_size=64, lr=0.01, wd=0.001, momentum=0.9, alpha=10, beta=0.75, gamma=10)\n",
        "train_dataloader, train_test_dataloader = get_dataloader(real_life_dataset, config['batch_size'])\n",
        "target_dataloader, target_test_dataloader = get_dataloader(product_dataset, config['batch_size'])\n",
        "mid_real_adv_model = DANN_Mid(len(real_life_dataset.classes), pretrained=True).to(device)\n",
        "torch.autograd.set_detect_anomaly(True)\n",
        "adversarial_training(mid_real_adv_model, train_dataloader, train_test_dataloader, target_dataloader, config, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "id": "qcDXQ9TysUmn",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qcDXQ9TysUmn",
        "outputId": "3fe5fe90-b408-4da3-e9aa-4ef5b882e082"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "## Loss Function ## CrossEntropyLoss being used.\n",
            "Test Error: \n",
            " Accuracy: 80.8%, Avg loss: 0.644416 \n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.6444155343342572, 0.8075)"
            ]
          },
          "metadata": {},
          "execution_count": 114
        }
      ],
      "source": [
        "loader_target_dataset = DataLoader(product_dataset, batch_size=config['batch_size'], shuffle=False)\n",
        "\n",
        "test_loop(loader_target_dataset, mid_real_adv_model, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "jA4CUBAtqYXB",
      "metadata": {
        "id": "jA4CUBAtqYXB"
      },
      "source": [
        "<a name=\"summary\">\n",
        "</a>\n",
        "\n",
        "## 7 Summary"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.1 Losses Governing Training\n",
        "\n",
        "In this experiment, two loss functions, dice loss and cross entropy, were selected for comparison in the classifier. As expected, the cross-entropy loss is calculated as the average value of the per-pixel loss, and the per-pixel loss is calculated discretely. So the CE loss only considers the microscopic, but not the global consideration; while the Dice loss considers the local and global loss information, perhaps to improve accuracy. The dice training in our case shows a very bad performance.\n",
        "\n",
        "| Domain   | Method | Performance |\n",
        "|----------|:-------------:|:---:|\n",
        "| Real Life -> Product | baseline | 6.6% \n",
        "|  |  UDA | 4.8%  | \n",
        "|  | UDA(Mid) |  5.1%  | \n",
        "| Product -> Real Life | baseline | 4.5% \n",
        "|  |  UDA |  8.9% | \n",
        "|  | UDA(Mid) | 5.2%  | "
      ],
      "metadata": {
        "id": "Y9UOq6ZJvli_"
      },
      "id": "Y9UOq6ZJvli_"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The reason to consider is that CE has better gradients. Suppose p is the softmax outputs and t is the target. The gradients of cross-entropy wrt the logits is something like $ p-t $. While the dice coefficient in a differentiable form is $ \\frac{2pt}{p+t} $ , whose gradients wrt p are much uglier: $ \\frac{2t(t^2-p^2)}{(p^2+t^2)^2} $. Consider cases where p and t are small, the gradient will explode to some huge value. And severe oscillations may occur. Dice loss has good performance for scenes with severely imbalanced samples, but it is not applicable in our case. We choose CE loss as the loss function."
      ],
      "metadata": {
        "id": "u4J6OKXpxGPs"
      },
      "id": "u4J6OKXpxGPs"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.2 Hyperparameter Selection"
      ],
      "metadata": {
        "id": "knNkIa0pq1Gh"
      },
      "id": "knNkIa0pq1Gh"
    },
    {
      "cell_type": "markdown",
      "source": [
        "<table>\n",
        "    <thead>\n",
        "        <tr>\n",
        "            <th rowspan=2>Domain</th>\n",
        "            <th rowspan=2>Method</th>\n",
        "            <th colspan=4>Target Accuracy</th>\n",
        "        </tr>\n",
        "        <tr>\n",
        "            <th>lr = 0.01</th>\n",
        "            <th>lr = 0.005</th>\n",
        "            <th>lr = 0.02</th>\n",
        "            <th>lr = 0.03</th>\n",
        "        </tr>\n",
        "    </thead>\n",
        "    <tbody>\n",
        "        <tr>\n",
        "            <td rowspan=3>Real Life -> Product</td>\n",
        "            <td>baseline</td>\n",
        "            <td>80.7%</td>\n",
        "            <td>79.8%</td>\n",
        "            <td>79.5%</td>\n",
        "            <td>82.4%</td>\n",
        "        </tr>\n",
        "        <tr>\n",
        "            <td>UDA</td>\n",
        "            <td>79.9%</td>\n",
        "            <td>79.3%</td>\n",
        "            <td>81.5%</td>\n",
        "            <td>82.0%</td>\n",
        "        </tr>\n",
        "        <tr>\n",
        "            <td>UDA(mid)</td>\n",
        "            <td>80.7%</td>\n",
        "            <td>79.8%</td>\n",
        "            <td>79.5%</td>\n",
        "            <td>82.4%</td>\n",
        "        </tr>\n",
        "        <tr>\n",
        "            <td rowspan=3>Product -> Real Life</td>\n",
        "            <td>baseline</td>\n",
        "            <td>63.7%</td>\n",
        "            <td>62.7%</td>\n",
        "            <td>62.5%</td>\n",
        "            <td>63.6%</td>\n",
        "        </tr>\n",
        "        <tr>\n",
        "            <td>UDA</td>\n",
        "            <td>62.5%</td>\n",
        "            <td>62.3%</td>\n",
        "            <td>63.2%</td>\n",
        "            <td>61.3%</td>\n",
        "        </tr>\n",
        "        <tr>\n",
        "            <td>UDA(mid)</td>\n",
        "            <td>63.8%</td>\n",
        "            <td>63.5%</td>\n",
        "            <td>64.0%</td>\n",
        "            <td>63.5%</td>\n",
        "        </tr>\n",
        "    </tbody>\n",
        "</table>"
      ],
      "metadata": {
        "id": "vTa6r5q5x3-h"
      },
      "id": "vTa6r5q5x3-h"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.2 Training Accuracy and Test Result Comparison\n",
        "\n",
        "Previously, we use DANN for UDA, and the adversarial predictor and discriminator act on the upper layer of the neural network (feature size 4096). Consider what if the adversarial acts on the lower layer and makes judgments from a more basic place, we add discimination to the features of the mid layer of the neural network (feature size 12544). When the source domain and target domain are Product -> Real life, the mid layer result has a slight improvement; otherwise, there is no improvement.\n",
        "\n",
        "| Method   | Product -> Real Life | Real Life -> Product |\n",
        "|----------|:-------------:|:------:|\n",
        "| AlexNet(Training) | 76.7% | 91% |\n",
        "| AlexNet |  62.8% | 81.5%  | |\n",
        "| DANN |    62.8%   |   78.5%  | |\n",
        "| DANN-Mid | 64.2% | 80.0% |\n",
        "| DANN Gain |0.0% | -3%|\n",
        "| DANN-Mid Gain | 1.4%| -1.5%|\n"
      ],
      "metadata": {
        "id": "-PqQenhhq1Jt"
      },
      "id": "-PqQenhhq1Jt"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Potentially, the drop in performance from the DANN UDA method is because of not enough training data, so the feature extractor is not able to learn a mapping that could represent inter domain features which eventually improve the performance."
      ],
      "metadata": {
        "id": "h_vvNfniq1DD"
      },
      "id": "h_vvNfniq1DD"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "UDA_DANN_v2.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.0 64-bit ('3.9.0')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "6623cd70e16cbfe1462b5ad5248bf727ae53c1a0c9be44857d7434f2a44555a7"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}