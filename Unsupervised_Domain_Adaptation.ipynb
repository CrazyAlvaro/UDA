{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Unsupervised Domain Adaptation Project\n"
      ],
      "metadata": {
        "id": "UOmUNvGYgPRN"
      },
      "id": "UOmUNvGYgPRN"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part-1: Data download\n",
        "Load data to project from Google Drive. Copy a subset of classes of images to the path:\n",
        "- `adaptiope_small/product_images`\n",
        "- `adaptiope_small/real_life` \n",
        "\n",
        "two directories. They represent images from two different domain **product** and **real_life**"
      ],
      "metadata": {
        "id": "ExU_wmMAgVsR"
      },
      "id": "ExU_wmMAgVsR"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "4134f6cf",
      "metadata": {
        "id": "4134f6cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "b7052c43-f27d-4932-993a-3fb021949332"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from os import makedirs, listdir\n",
        "from tqdm import tqdm\n",
        "from google.colab import drive\n",
        "from os.path import join\n",
        "from shutil import copytree\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "!mkdir dataset\n",
        "!cp \"gdrive/My Drive/Colab Notebooks/data/Adaptiope.zip\" dataset/\n",
        "# !ls dataset\n",
        "\n",
        "!unzip -qq dataset/Adaptiope.zip   # unzip file\n",
        "\n",
        "!rm -rf adaptiope_small"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "0a6cac67",
      "metadata": {
        "id": "0a6cac67",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "9ef0e099-ecc6-49f7-da7f-cefc0c004e07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['hat', 'pikachu', 'golf club', 'vr goggles', 'pen', 'hair dryer', 'laptop', 'backpack', 'microwave', 'puncher', 'printer', 'stethoscope', 'wheelchair', 'keyboard', 'bicycle', 'roller skates', 'hot glue gun', 'ice cube tray', 'mixing console', 'lawn mower', 'vacuum cleaner', 'helicopter', 'screwdriver', 'chainsaw', 'hourglass', 'coat hanger', 'stapler', 'purse', 'spatula', 'boxing gloves', 'rubber boat', 'computer mouse', 'tyrannosaurus', 'knife', 'glasses', 'smoking pipe', 'comb', 'hand mixer', 'speakers', 'syringe', 'electric shaver', 'watering can', 'toothbrush', 'tank', 'wristwatch', 'stand mixer', 'skeleton', 'file cabinet', 'toilet brush', 'cordless fixed phone', 'webcam', 'razor', 'ladder', 'motorbike helmet', 'bookcase', 'dart', 'handgun', 'stroller', 'nail clipper', 'ice skates', 'bottle', 'sword', 'office chair', 'fan', 'smartphone', 'grill', 'game controller', 'calculator', 'fire extinguisher', 'brachiosaurus', 'hoverboard', 'binoculars', 'power drill', 'sewing machine', 'pipe wrench', 'snow shovel', 'axe', 'rifle', 'magic lamp', 'baseball bat', 'quadcopter', 'umbrella', 'phonograph', 'bicycle helmet', 'ruler', 'scooter', 'shower head', 'car jack', 'rc car', 'pogo stick', 'desk lamp', 'diving fins', 'sleeping bag', 'projector', 'computer', 'fighter jet', 'power strip', 'mug', 'letter tray', 'hard-wired fixed phone', 'electric guitar', 'crown', 'trash can', 'flat iron', 'drum set', 'compass', 'network switch', 'handcuffs', 'monitor', 'notepad', 'telescope', 'corkscrew', 'skateboard', 'wallet', 'usb stick', 'ring binder', 'tape dispenser', 'tent', 'in-ear headphones', 'over-ear headphones', 'scissors', 'cellphone', 'acoustic guitar']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20/20 [00:03<00:00,  5.82it/s]\n",
            "100%|██████████| 20/20 [00:05<00:00,  3.97it/s]\n"
          ]
        }
      ],
      "source": [
        "!mkdir adaptiope_small\n",
        "classes = listdir(\"Adaptiope/product_images\")\n",
        "print(classes)\n",
        "classes = [\"backpack\", \"bookcase\", \"car jack\", \"comb\", \"crown\", \"file cabinet\", \"flat iron\", \"game controller\", \"glasses\",\n",
        "           \"helicopter\", \"ice skates\", \"letter tray\", \"monitor\", \"mug\", \"network switch\", \"over-ear headphones\", \"pen\",\n",
        "           \"purse\", \"stand mixer\", \"stroller\"]\n",
        "for d, td in zip([\"Adaptiope/product_images\", \"Adaptiope/real_life\"], [\"adaptiope_small/product_images\", \"adaptiope_small/real_life\"]):\n",
        "  makedirs(td)\n",
        "  for c in tqdm(classes):\n",
        "    c_path = join(d, c)\n",
        "    c_target = join(td, c)\n",
        "    copytree(c_path, c_target)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part-2: Image Classification Neural Network\n",
        "\n",
        " "
      ],
      "metadata": {
        "id": "uUHNozN6ggRr"
      },
      "id": "uUHNozN6ggRr"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part-2.0: Data Loading\n",
        "\n",
        "First we load the data and preprocessing them"
      ],
      "metadata": {
        "id": "fwiJOBXWpeS0"
      },
      "id": "fwiJOBXWpeS0"
    },
    {
      "cell_type": "code",
      "source": [
        "product_path = 'adaptiope_small/product_images'\n",
        "real_life_path = 'adaptiope_small/real_life'"
      ],
      "metadata": {
        "id": "hmth8mlu1oov"
      },
      "id": "hmth8mlu1oov",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "ht9bXOHU6zTQ",
        "outputId": "570edf4f-f05f-4905-9892-03e4ae448071"
      },
      "id": "ht9bXOHU6zTQ",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Adaptiope  adaptiope_small  dataset  gdrive  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "from os.path import join\n",
        "\n",
        "img = Image.open(join(product_path, 'backpack', 'backpack_003.jpg'))\n",
        "print('Image size: ', img.size)\n",
        "#img"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "_GZxbLlT6O8m",
        "outputId": "88f59be5-0860-4ac6-eb65-9dd25a8a18cb"
      },
      "id": "_GZxbLlT6O8m",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image size:  (679, 679)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "import libraries"
      ],
      "metadata": {
        "id": "RE2WUwy9BORV"
      },
      "id": "RE2WUwy9BORV"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision.models import vgg16, resnet18, resnet34\n",
        "from torch.utils.data import DataLoader, random_split"
      ],
      "metadata": {
        "id": "Vei6SzEeggzU"
      },
      "id": "Vei6SzEeggzU",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "configuration constants"
      ],
      "metadata": {
        "id": "M5rt9x7nBKiT"
      },
      "id": "M5rt9x7nBKiT"
    },
    {
      "cell_type": "code",
      "source": [
        "img_size = 256\n",
        "# mean, std used by pre-trained models from PyTorch\n",
        "mean, std = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n",
        "batch_size = 100\n",
        "learning_rate = 0.001\n",
        "num_epochs = 15"
      ],
      "metadata": {
        "id": "JXznFSNkAzn3"
      },
      "id": "JXznFSNkAzn3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Configue GPU"
      ],
      "metadata": {
        "id": "_Rz4CI9spEkN"
      },
      "id": "_Rz4CI9spEkN"
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "nHv2o65FpDpn",
        "outputId": "b3afc2db-cd74-4b01-f247-63d7e2478f01"
      },
      "id": "nHv2o65FpDpn",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.transforms.transforms import ToTensor\n",
        "\n",
        "def get_dataset(root_path):\n",
        "  '''\n",
        "    Get dataset from specific data path\n",
        "\n",
        "    # parameters:\n",
        "        root_path: path to image folder\n",
        "\n",
        "    # return: train_loader, test_loader\n",
        "  '''\n",
        "  # Construct image transform\n",
        "  image_transform = transforms.Compose([\n",
        "    transforms.Resize(img_size),\n",
        "    transforms.CenterCrop(img_size),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std)\n",
        "  ])\n",
        "\n",
        "  # Load data from filesystem\n",
        "  image_dataset = ImageFolder(root_path, transform=image_transform)\n",
        "\n",
        "  return image_dataset\n",
        "\n",
        "def get_dataloader(dataset, batch_size, shuffle_train=True, shuffle_test=False):\n",
        "  '''\n",
        "    Get DataLoader from specific data path\n",
        "\n",
        "    # parameters:\n",
        "        dataset: ImageFolder instance\n",
        "        batch_size: batch_size for DataLoader\n",
        "        shuffle_train: whether to shuffle training data\n",
        "        shuffle_test: whether to shuffle test data\n",
        "  '''\n",
        "  # Get train, test number\n",
        "  num_total = len(dataset)\n",
        "  num_train = int(num_total * 0.8 + 1)\n",
        "  num_test  = num_total - num_train\n",
        "\n",
        "  # random split dataset\n",
        "  data_train, data_test = random_split(dataset, [num_train, num_test])\n",
        "\n",
        "  # initialize dataloaders\n",
        "  loader_train = DataLoader(data_train, batch_size=batch_size, shuffle=shuffle_train)\n",
        "  loader_test  = DataLoader(data_test, batch_size=batch_size, shuffle=shuffle_test)\n",
        "\n",
        "  return loader_train, loader_test"
      ],
      "metadata": {
        "id": "GF3YyTBAhJk8"
      },
      "id": "GF3YyTBAhJk8",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part-2.1 Pretrain Network\n",
        "\n",
        "Here we use a pretrain Neural Network to start with, then we fine tune it with the data set we have from **Adaptiope** in one domain, and test it on the target domain. Compare the two result, and set the benchmark for later UDA enriched method. "
      ],
      "metadata": {
        "id": "-J4MSc3spcYU"
      },
      "id": "-J4MSc3spcYU"
    },
    {
      "cell_type": "code",
      "source": [
        "pd_dataset = get_dataset(product_path)\n",
        "len(pd_dataset.classes)"
      ],
      "metadata": {
        "id": "ijDPD_jrvlz_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "7b379052-c6db-44d1-94c2-b4d5597b7285"
      },
      "id": "ijDPD_jrvlz_",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part-2.2 Define the Deep Residual Network"
      ],
      "metadata": {
        "id": "ZjWy3ak98L0F"
      },
      "id": "ZjWy3ak98L0F"
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_model(num_classes, model_type=\"ResNet\"):\n",
        "  if model_type.startswith(\"ResNet\"):\n",
        "    model = resnet18(pretrained=True)\n",
        "    in_features = model.fc.in_features\n",
        "    model.fc = torch.nn.Linear(in_features=in_features, out_features=num_classes)\n",
        "  else:\n",
        "    model = vgg16(pretrained=True)\n",
        "    in_features = model.classifier[-1].in_features\n",
        "    model.classifier[-1] = torch.nn.Linear(in_features=in_features, out_features=num_classes)\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "JfloSw5b2jVe"
      },
      "id": "JfloSw5b2jVe",
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model = initialize_model(20, \"vgg\")\n",
        "# count  = 0\n",
        "# for name, param in model.named_parameters():\n",
        "#   # if name.startswith('fc'):\n",
        "#   # print(name) \n",
        "#   count += 1\n",
        "# print(count)\n",
        "\n",
        "# print(model.__class__.__name__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104,
          "referenced_widgets": [
            "661e5d4535e747478fc50ca9ab0da715",
            "8eb136596e6144f3a310cb72e0a94daf",
            "03bcfbcad85c4a93905d9f4197afef8f",
            "0ff76416bef640bb8ef6a72da882b6c7",
            "df79359fe04742ba862115b4a832a46e",
            "6ad659c6752146deb1405e75f65c9bae",
            "7c260fc013c1439da442cf4a43986173",
            "3a92998020ab42c0b2a1f4c25a2078da",
            "8aa6b78d7cd04b8eaa9cf1e0fdb17fca",
            "c8d2108be39c4ac7b9932e99ae5f04d6",
            "82023da03dd74c03bce87792d31d73d0"
          ]
        },
        "id": "Smgp5vnehJWW",
        "outputId": "daebfd3f-e326-49a8-d780-4a511df00b97"
      },
      "id": "Smgp5vnehJWW",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/528M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "661e5d4535e747478fc50ca9ab0da715"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part-2.3 Cost function\n",
        "\n",
        "Divide parameters intro two groups, in which the last fully conneted layer with learning_rate, the other layers with 0.1 * learning_rate."
      ],
      "metadata": {
        "id": "IhnsX7lq5saz"
      },
      "id": "IhnsX7lq5saz"
    },
    {
      "cell_type": "code",
      "source": [
        "def get_cost_function():\n",
        "  return torch.nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "q6BkiCH_5sOr"
      },
      "id": "q6BkiCH_5sOr",
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part-2.4 Optimizer"
      ],
      "metadata": {
        "id": "B-y0dTPj5zrR"
      },
      "id": "B-y0dTPj5zrR"
    },
    {
      "cell_type": "code",
      "source": [
        "def get_optimizer(model, learning_rate, weight_decay, momentum):\n",
        "\n",
        "  # Get model name\n",
        "  model_name = model.__class__.__name__\n",
        "\n",
        "  # define final layer name by different model\n",
        "  if model_name == \"ResNet\":\n",
        "    final_layer_name = \"fc\"\n",
        "  elif model_name == \"VGG\":\n",
        "    final_layer_name = \"classifier.6\"\n",
        "  else:\n",
        "    raise Exception(f'## GET_OPTIMIZER ## - Undefined Model Type {model_name}')\n",
        "\n",
        "  pre_trained_weights = []\n",
        "  final_layer_weights = []\n",
        "\n",
        "  # get all the parameters required gradient updates\n",
        "  for name, param in model.named_parameters():\n",
        "    if param.requires_grad == True:\n",
        "      if name.startswith(final_layer_name):\n",
        "        final_layer_weights.append(param)\n",
        "      else:\n",
        "        pre_trained_weights.append(param)\n",
        "\n",
        "  # assign parameters to parameters\n",
        "  optimizer = torch.optim.SGD([\n",
        "    {'params': pre_trained_weights},\n",
        "    {'params': final_layer_weights, 'lr': learning_rate}\n",
        "  ], lr= learning_rate/10, weight_decay=weight_decay, momentum=momentum)\n",
        "  \n",
        "  return optimizer"
      ],
      "metadata": {
        "id": "zKRb56iY5sGt"
      },
      "id": "zKRb56iY5sGt",
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part-2.5 Training and Testing Step"
      ],
      "metadata": {
        "id": "Atdanl9REs3F"
      },
      "id": "Atdanl9REs3F"
    },
    {
      "cell_type": "code",
      "source": [
        "def train_loop(dataloader, model, loss_fn, optimizer, device):\n",
        "  size = len(dataloader.dataset)\n",
        "\n",
        "  for batch, (X, y) in enumerate(dataloader):\n",
        "    X, y = X.to(device), y.to(device)\n",
        "    \n",
        "    # compute prediction and loss\n",
        "    predicts = model(X)\n",
        "    loss = loss_fn(predicts, y)\n",
        "\n",
        "    # backpropagation\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "      loss, current = loss.item(), batch * len(X)\n",
        "      print(f\"loss: {loss:>7f} [{current:>5d}/{size:>5d}]\")"
      ],
      "metadata": {
        "id": "ORDqPkiT5r1s"
      },
      "id": "ORDqPkiT5r1s",
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_loop(dataloader, model, loss_fn, device):\n",
        "  test_loss, correct = 0, 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for X, y in dataloader:\n",
        "      X, y = X.to(device), y.to(device)\n",
        "      predicts = model(X)\n",
        "      test_loss += loss_fn(predicts, y).item()\n",
        "      correct += (predicts.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "  size = len(dataloader.dataset)\n",
        "  num_batches = len(dataloader)\n",
        "\n",
        "  test_loss /= num_batches\n",
        "  correct /= size\n",
        "  print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "\n",
        "  return test_loss, correct"
      ],
      "metadata": {
        "id": "LlvQLNUDLGJF"
      },
      "id": "LlvQLNUDLGJF",
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part-2.6 Training"
      ],
      "metadata": {
        "id": "z5uhItnwhj8S"
      },
      "id": "z5uhItnwhj8S"
    },
    {
      "cell_type": "code",
      "source": [
        "def training(model, train_dataloader, test_dataloader, device, epochs=10, lr=0.001, wd=0.001, momentum=0.9):\n",
        "  print(f\"Learning_rate {lr}, weight_decay {wd}\")\n",
        "  loss_fn = get_cost_function()\n",
        "  optimizer = get_optimizer(model, lr, wd, momentum)\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    print(f\"Epoch {epoch+1}\\n------------------\")\n",
        "    train_loop(train_dataloader, model, loss_fn, optimizer, device)\n",
        "    test_loop(test_dataloader, model, loss_fn, device)\n",
        "\n",
        "  print(\"Done\")"
      ],
      "metadata": {
        "id": "nM76Syqahknd"
      },
      "id": "nM76Syqahknd",
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get train_dataloader, test_dataloader\n",
        "dataset = get_dataset(product_path)\n",
        "train_dataloader, test_dataloader = get_dataloader(dataset, 50)\n",
        "\n",
        "# Get model\n",
        "model = initialize_model(len(dataset.classes)).to(device)\n",
        "\n",
        "# Training\n",
        "training(model, train_dataloader, test_dataloader, device, num_epochs, learning_rate)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "V93UE4rXi3C9",
        "outputId": "97406d2f-26b8-4a67-bb74-ff73890a7361"
      },
      "id": "V93UE4rXi3C9",
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Learning_rate 0.001, weight_decay 0.001\n",
            "Epoch 1\n",
            "------------------\n",
            "loss: 3.121286 [    0/ 1601]\n",
            "Test Error: \n",
            " Accuracy: 56.9%, Avg loss: 2.153765 \n",
            "\n",
            "Epoch 2\n",
            "------------------\n",
            "loss: 2.186185 [    0/ 1601]\n",
            "Test Error: \n",
            " Accuracy: 81.0%, Avg loss: 1.369971 \n",
            "\n",
            "Epoch 3\n",
            "------------------\n",
            "loss: 1.203209 [    0/ 1601]\n",
            "Test Error: \n",
            " Accuracy: 90.2%, Avg loss: 0.923603 \n",
            "\n",
            "Epoch 4\n",
            "------------------\n",
            "loss: 0.651633 [    0/ 1601]\n",
            "Test Error: \n",
            " Accuracy: 92.7%, Avg loss: 0.684645 \n",
            "\n",
            "Epoch 5\n",
            "------------------\n",
            "loss: 0.763621 [    0/ 1601]\n",
            "Test Error: \n",
            " Accuracy: 94.2%, Avg loss: 0.551654 \n",
            "\n",
            "Epoch 6\n",
            "------------------\n",
            "loss: 0.426481 [    0/ 1601]\n",
            "Test Error: \n",
            " Accuracy: 94.7%, Avg loss: 0.466251 \n",
            "\n",
            "Epoch 7\n",
            "------------------\n",
            "loss: 0.396500 [    0/ 1601]\n",
            "Test Error: \n",
            " Accuracy: 94.2%, Avg loss: 0.409144 \n",
            "\n",
            "Epoch 8\n",
            "------------------\n",
            "loss: 0.446180 [    0/ 1601]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-17a1e7b0e294>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-29-d463463185eb>\u001b[0m in \u001b[0;36mtraining\u001b[0;34m(model, train_dataloader, test_dataloader, device, epochs, lr, wd, momentum)\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {epoch+1}\\n------------------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mtrain_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mtest_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-35-6bdf5aa08a68>\u001b[0m in \u001b[0;36mtrain_loop\u001b[0;34m(dataloader, model, loss_fn, optimizer, device)\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    528\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    469\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 471\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    472\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    230\u001b[0m         \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m             \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    347\u001b[0m             \u001b[0mPIL\u001b[0m \u001b[0mImage\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRescaled\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m         \"\"\"\n\u001b[0;32m--> 349\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mantialias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation, max_size, antialias)\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Anti-alias option is always applied for PIL Image input. Argument antialias is ignored.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m         \u001b[0mpil_interpolation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpil_modes_mapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 436\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF_pil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpil_interpolation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mF_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mantialias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mantialias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional_pil.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation, max_size)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[0mnew_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_h\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnew_short\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_long\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mh\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnew_long\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_short\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_h\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmax_size\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(self, size, resample, box, reducing_gap)\u001b[0m\n\u001b[1;32m   1903\u001b[0m                 )\n\u001b[1;32m   1904\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1905\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbox\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1906\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1907\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfactor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbox\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TODO\n",
        "\n",
        "### TODO: Dataset unzip Google Drive, Copy to folder\n",
        "\n",
        "TODO: Batch progress number error\n",
        "\n",
        "Otherwise Continue UDA"
      ],
      "metadata": {
        "id": "kTUMJfmMw-tQ"
      },
      "id": "kTUMJfmMw-tQ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part-3: UDA \n",
        "\n",
        "Here we use Contrastive Domain Adaptation method proposed [here]().\n",
        "We train the previous network and run the test on both Source Domain and Target Domain. "
      ],
      "metadata": {
        "id": "7q7jwnwwhRz2"
      },
      "id": "7q7jwnwwhRz2"
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "psgAJw-YhZGJ"
      },
      "id": "psgAJw-YhZGJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "zKscrnYohp7k"
      },
      "id": "zKscrnYohp7k",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "hGZqRFWfhpx2"
      },
      "id": "hGZqRFWfhpx2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part-4: Comparison & Discussion\n",
        "Here we compare the test result from the direct method and the UDA method. "
      ],
      "metadata": {
        "id": "CKsMcxgihquu"
      },
      "id": "CKsMcxgihquu"
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "6C0fg_GZh4ou"
      },
      "id": "6C0fg_GZh4ou",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "lcDs1PTfh9G9"
      },
      "id": "lcDs1PTfh9G9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part-5: Conclusion"
      ],
      "metadata": {
        "id": "u3VyXbyth9sO"
      },
      "id": "u3VyXbyth9sO"
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "xgSBA7eMh-Nk"
      },
      "id": "xgSBA7eMh-Nk",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.9"
    },
    "colab": {
      "name": "Unsupervised_Domain_Adaptation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "661e5d4535e747478fc50ca9ab0da715": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8eb136596e6144f3a310cb72e0a94daf",
              "IPY_MODEL_03bcfbcad85c4a93905d9f4197afef8f",
              "IPY_MODEL_0ff76416bef640bb8ef6a72da882b6c7"
            ],
            "layout": "IPY_MODEL_df79359fe04742ba862115b4a832a46e"
          }
        },
        "8eb136596e6144f3a310cb72e0a94daf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6ad659c6752146deb1405e75f65c9bae",
            "placeholder": "​",
            "style": "IPY_MODEL_7c260fc013c1439da442cf4a43986173",
            "value": "100%"
          }
        },
        "03bcfbcad85c4a93905d9f4197afef8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3a92998020ab42c0b2a1f4c25a2078da",
            "max": 553433881,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8aa6b78d7cd04b8eaa9cf1e0fdb17fca",
            "value": 553433881
          }
        },
        "0ff76416bef640bb8ef6a72da882b6c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c8d2108be39c4ac7b9932e99ae5f04d6",
            "placeholder": "​",
            "style": "IPY_MODEL_82023da03dd74c03bce87792d31d73d0",
            "value": " 528M/528M [00:36&lt;00:00, 16.9MB/s]"
          }
        },
        "df79359fe04742ba862115b4a832a46e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ad659c6752146deb1405e75f65c9bae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7c260fc013c1439da442cf4a43986173": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3a92998020ab42c0b2a1f4c25a2078da": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8aa6b78d7cd04b8eaa9cf1e0fdb17fca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c8d2108be39c4ac7b9932e99ae5f04d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "82023da03dd74c03bce87792d31d73d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}